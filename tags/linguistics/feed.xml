<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linguistics on Yongfu's Blog</title><link>https://yongfu.name/tags/linguistics/</link><description>Recent content in Linguistics on Yongfu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 23 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/linguistics/feed.xml" rel="self" type="application/rss+xml"/><item><title>Searching Interlinear Glosses Written in Word Documents</title><link>https://yongfu.name/2020/04/23/gloss-search/</link><pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/04/23/gloss-search/</guid><description>I am taking the course Linguistic Fieldwork this semester. Each week, we record and transcribe Budai Rukai, an Austronesian language spoken by Rukai people (魯凱族). The resulting data (interlinear glosses) are written in a Word document (.docx) as required by the course instructor. Things get worse as the number of documents accumulates each week, since it becomes harder to locate specific linguistic patterns in the corpus of texts, as they are spread across multiple documents.</description></item><item><title>以 Python 實作 Concordancer</title><link>https://yongfu.name/2020/03/20/building-concordancer/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/03/20/building-concordancer/</guid><description>每次接近學期末的時候，寫程式癮就會開始發作 (可能是不想面對無趣的期末報告)，這時候腦袋會蹦出許多很有趣的想法，然後就會迫不及待地想將這些想法實作出來。這次(2019 年末) 的程式癮刺激來源是實驗室的雲端硬碟裡的某個 (版權封閉) 中文語料庫，雖然該語料庫已有很好的搜尋界面，但我就是想 reinvent the wheel，自己手刻出一個 concordancer。不為了什麼，就只是因為這件事本身就很有樂趣。
初步嘗試：for loop&amp;hellip; forever 我本來並沒有太大的雄心壯志，就只想快速弄出個程式界面方便我查找 concordance，想說使用 NLTK concordance 應該很快就可以弄出我想的東西。但 NLTK concordance 只能使用 word form (或 pattern) 去搜尋 concordance，我的需求卻是要能使用 word form 或 PoS tag 搜尋語料庫 (類似 Corpus Query Langauge1，但不用這麼複雜)。但要自己用 Python 實作這個功能也頗簡單，於是我就自己手刻了這個功能。然而事實證明我太過天真了。語料庫的大小約 1000 萬個 token，而每次搜尋時，我的程式使用 for 迴圈跑過整個語料，因此要花非常非常非常久的時間才能完成搜尋。對於非資訊背景出生的我，第一次體驗 $O(n)$ 是件不可忽視的問題以及 Database 存在的必要性。
重新規劃： Database + Python + Vue 為了解決上述問題我暫時擱置了這個專案 (寒假開始到春節期間) 去學習必備的一些知識2，最後比較有系統地重新規劃了這個 concordancer 的架構：
這個新的架構分成前3、後端，前端不是本文的重點 (原始碼在此)，就不細談。這邊直接舉一個實例說明這個 concordancer 如何運作：
首先，使用者在前端輸入一個搜尋的字串 (keyword)，這個字串需符特定的格式：[token 1][token 2][token 3]。每對中括號代表一個 token，中括號內則是描述此 token 的特徵，如 word form 與 PoS tag，例如 [word=&amp;quot;打&amp;quot; pos=&amp;quot;V.</description></item><item><title>Recreating Leipizig.js with Vue for Interlinear Glossing</title><link>https://yongfu.name/2020/02/22/leipzigvue/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/02/22/leipzigvue/</guid><description>I noticed Leipzig.js from George Moroz&amp;rsquo;s GitHub activity (he starred bdchauvette/leipzig.js a few months ago). This JS library is fantastic, and at the moment I saw it, I came up with an idea of building a web app facilitating interlinear glossing. During Chinese New Year, I finally started on the project. I thought it would be easy since I had some experience with Vue.js before1, but it turned out that leipzig.</description></item><item><title>閱讀筆記：Origins of Human Communication</title><link>https://yongfu.name/2019/03/04/originsofhumancommunication/</link><pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate><guid>https://yongfu.name/2019/03/04/originsofhumancommunication/</guid><description>去年秋天在準備研究所甄試時，發現市面上的語言學概論課本對於筆試幾乎沒有任何幫助，於是去找認知語言學相關的專著和文章來閱讀。我找的第一本書是 George Lakoff 的 Metaphors We Live By。閱讀此書時，發覺自己又面臨一個陌生、龐大、擁有一堆術語的領域，而且認知語言學中的「認知」似乎和我所熟悉的「認知」心理學不太一樣，很難從自己熟悉的角度去切入作者的想法。於是我換了一本書 – Geeraerts (2006)。這本書由一篇篇獨立的文章組成，每篇文章由一或兩位從事認知語言學相關研究的學者撰寫。在 Geeraerts (2006) 中，我發現一個熟悉的名字 – Michael Tomasello。當時我對 Tomasello 並不熟悉，僅知道他是個有名的心理學家，研究興趣與這些關鍵字有關：Language acquisition, Joint attention, Primate cognition。閱讀 Tomasello 的文章後，我終於開始抓到一點認知語言學的想法。果然，要了解一個陌生的領域最好還是從自己熟悉的思維下手。
語言到底是什麼？ 這個出現在語概課本第一章的問題，通常會說明人類語言和動物的溝通有什麼本質上的不同，並且指出人類語言具有結構、可組合的特性等等。但這些說明只是描述人類語言的特性，但對於語言是什麼，或更精確地說 – 語言如何產生 (包含認知處理上、生理發展上、演化歷史上)，並未提供有洞見的解釋。
我想認知語言學的主要貢獻在於，其看見語言以外的世界，強調認知系統對於形塑語言的影響。對於語言的分析，因此需跨出「語言」，擴展到人類的認知系統。這真的是充滿雄心壯志的抱負，但我並不覺得這帶來的影響是全然正面的，因為面對人類心智運作的問題時，事情會變得非常非常非常複雜。特別是語言，從人類語言的相關研究可以發現語言真的很「雜亂 (noisy)」 (相較於視覺系統)。這使將戰線拉得如此廣泛的認知語言學可能變得十分混亂，在沒有透徹理解一個現象中比較基礎的因素前，就直接研究更複雜的整體現象1。這也是為何我對於 語言是什麼 這個問題感到困惑。我一直找不到一個清晰、穩固的基礎，作為評斷各種對語言的看法與理論的基準。我們似乎根本不了解語言是什麼，卻常在很抽象的層次上試圖說明語言是什麼。
於是，我決定先從 Tomasello 的專著下手，因為他的研究興趣是語言 (甚至是人類) 的演化，而就我所學過的東西而言，演化論應是僅次於數學，最可靠的一種想法。
看見更基礎的元素 看完 Origins of Human Communication (Tomasello, 2008) 後，不得不佩服 Tomasello 這位學者。剛了解他研究對象是嬰兒與靈長類動物、研究主題是心理學和語言學時，不太能理解為什麼，只以為他是興趣廣泛。讀完書後才了解到，這些內容都與 Tomasello 想回答的問題密切相關 – 語言的起源。
Tomasello 厲害的地方在於其能夠跳脫「語言」的框架，嘗試找出語言要能運作所需的更基礎的元素。而要找出這些基礎的元素，需要跳脫過去語言相關研究關注的主體 – 成年人類的語言使用，將目光聚焦在「尚未」發展出語言的族群上 – 黑猩猩與人類嬰兒，藉此找出哪些因素造成成年人類和這兩個族群有如此不同的語言表現。
例如，「口語」在語言學中可說是最受關注的焦點 (相較於「文字」與「手勢」)，所以許多對於語言的研究與分析都是以口語作為基礎。但 Tomasello 強調了「手勢」的重要性。要了解「語言 (language)」，我們應該先了解「溝通 (communication)」，因為語言只是人類溝通的一種形式，而「指向 (pointing)」與「比手畫腳 (pantomiming)」則是人類溝通的另外兩種重要方式。「手勢」( 指向 與 比手畫腳 ) 的重要性在於，由於其所能攜帶的資訊量遠少於語言，其特別容易突顯人類溝通時，溝通者需具備的社會認知結構。這裡以書中提到的例子說明：</description></item><item><title>Visualizing Language Loss in Taiwan</title><link>https://yongfu.name/2019/02/17/visualize-language-loss/</link><pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate><guid>https://yongfu.name/2019/02/17/visualize-language-loss/</guid><description>Taiwan Language Survey is a small project I worked on during May to June in 2018. The idea was to create a survey that continuously collects data and a web page that visualizes the collected data. The web page is updated weekly using Travis-CI.
The main purpose of this survey is to raise public awareness of language loss in Taiwan. Hence, the survey is designed to collect data that can provide valuable information about language loss, for example, some questions were asked to gain insight about the change of linguistic competence acoss generations in a family (i.</description></item><item><title>Easy Linguistics Document Writing with R Markdown</title><link>https://yongfu.name/2018/09/09/linguistics-down/</link><pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/09/09/linguistics-down/</guid><description>I’ve written a post about rendering IPA symbols properly regardless of the output format of the R Markdown document. I implemented the ideas into an R package, linguisticsdown. linguisticsdown provides a Shiny interface to facilitate inserting IPA symbols in R Markdown. See a quick demo of the current feature of linguisticsdown in the gif at the end of the post.
A live demo is hosted on shinyapps.io. For more details, visit linguisticsdown.</description></item><item><title>Rendering IPA Symbols in R Markdown</title><link>https://yongfu.name/2018/09/06/ipa-symbols/</link><pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/09/06/ipa-symbols/</guid><description>I was thinking about promoting reproducible research in Linguistics, or more precisely, how to attract people with no programming skills to have incentives to learn at least a bit programming, so that they have the ability to make their research more reproducible. I arrived at the solution: start by adopting R Markdown to write articles (see the last section for details), but making R Markdown more friendly to novices in a particular field of academia is crucial to enhance their incentives to learn programming.</description></item></channel></rss>