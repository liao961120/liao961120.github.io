<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>R on Yongfu's Blog</title><link>https://yongfu.name/tags/r/</link><description>Recent content in R on Yongfu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 26 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/r/feed.xml" rel="self" type="application/rss+xml"/><item><title>Demystifying Item Response Theory (4/4)</title><link>https://yongfu.name/2023/04/26/irt4/</link><pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/04/26/irt4/</guid><description>&lt;p>Rating scales require special treatments during data analyses. It is
dangerous to treat the choices in a rating scale as simple numerical
values. Nor is it satisfactory to treat them as discrete categories in
which the internal ordering is thrown away. A rating scale is
&lt;strong>ordinal&lt;/strong> in nature, meaning that there is an inherent order among the
choices within. This ordering is different from the ordering in
numerical values such as counts and heights. In such cases, the
differences between numerical values are directly comparable. For
instance, a count of 5 differs from a count of 3 by a count of 2, and so
is the difference between a count of 8 and 6. &lt;strong>Ordinal variables&lt;/strong> are
different. Take for example the subjective rating of happiness. It is
probably easier to move from a rating of 2 to 3 than from a rating of 4
to 5 on a five-point Likert scale, as many people prefer to reserve the
boundary ratings (1 and 5) for extreme cases. Ratings like this are
ubiquitous in the social sciences and particularly in psychology, where
rating scales are deployed to measure unobserved latent psychological
constructs.&lt;/p>
&lt;p>In this post, the final one in the &lt;em>demystifying IRT&lt;/em> series, I will
walk you through the statistical machinery that deals with the rating
scale. Things get a bit complicated in rating scales since the
dimensionality increases, and it is always more challenging to think in
higher dimensions. However, after peeling off the complexity introduced
by the high dimensions, the underlying concept is quite straightforward.
It is again a GLM, just with fancier machinery to map continuous latent
quantities to a vector of probabilities. So don’t be scared off by the
high dimensions. We just have to take one step at a time. Don’t worry if
you run out of working memory. Shift the burden of holding everything in
your brain to a piece of paper. Sketch what you need and proceed slowly.
You will finally get there.&lt;/p>
&lt;h2 id="wine-quality">Wine Quality&lt;/h2>
&lt;p>Before moving on to the details of the statistical machinery behind the
rating scale, let me first provide some context.&lt;/p>
&lt;p>The examples presented in previous posts are classical situations where
IRT is applied and known for—a testing context. In such contexts, there
are testees, test items, and possibly raters, but IRT is much more
general than that. It is well applicable beyond the testing situation.
Let us look at one such example, the &lt;em>rating of wine quality&lt;/em>.&lt;/p>
&lt;p>There are wines, fine wines, premium wines, and judges in a wine
competition. It is a simple twist of the item-testing scenario in which
IRT is often applied. Again, two factors co-determine the rating scores
of the wines here. First, it is the “inherent” property associated with
each wine, the &lt;em>wine quality&lt;/em>. High-quality wines should receive high
ratings for the ratings to make sense at all. The second factor is the
&lt;em>leniency&lt;/em> of a judge in giving out the scores. A lenient judge tends to
give higher ratings to the same wines as compared to stricter judges.
These assumptions are illustrated in the DAG below. Here, $W$ and $J$
represent the latent &lt;strong>wine quality&lt;/strong> and &lt;strong>judge leniency&lt;/strong>,
respectively. $R$ stands for the rating scores. If you will, you could
draw the analogy to the previous IRT context, where $W$ can be thought
of as corresponding to the person ability and $J$ to the item easiness.
The analogy isn’t exact though. It’s equally sensible to think in the
other direction. There’s nothing wrong to think of $W$ as corresponding
to item easiness and $J$ to person ability.&lt;/p>
&lt;div class="goat svg-container ">
&lt;svg
xmlns="http://www.w3.org/2000/svg"
font-family="Menlo,Lucida Console,monospace"
viewBox="0 0 152 121"
>
&lt;g transform='translate(8,16)'>
&lt;path d='M 32,64 L 48,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,32 L 96,64' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='60.000000,32.000000 48.000000,26.400000 48.000000,37.599998' fill='currentColor' transform='rotate(300.000000, 48.000000, 32.000000)'>&lt;/polygon>
&lt;polygon points='92.000000,32.000000 80.000000,26.400000 80.000000,37.599998' fill='currentColor' transform='rotate(240.000000, 80.000000, 32.000000)'>&lt;/polygon>
&lt;path d='M 24,64 A 16,16 0 0,0 8,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,64 A 16,16 0 0,1 40,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,64 A 16,16 0 0,0 88,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,64 A 16,16 0 0,1 120,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 8,80 A 16,16 0 0,0 24,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 40,80 A 16,16 0 0,1 24,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 88,80 A 16,16 0 0,0 104,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 120,80 A 16,16 0 0,1 104,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;text text-anchor='middle' x='24' y='84' fill='currentColor' style='font-size:1em'>W&lt;/text>
&lt;text text-anchor='middle' x='64' y='4' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;text text-anchor='middle' x='104' y='84' fill='currentColor' style='font-size:1em'>J&lt;/text>
&lt;/g>
&lt;/svg>
&lt;/div>
&lt;p>The only thing new is that instead of a binary response, $R$ can take more than
two values. We need new machinery to map the aggregated influence from the two
factors ($W$ and $J$), which is a latent score in the real space, to the outcome
ordinal scale. Lower latent scores should give rise to lower ratings, and higher
latent scores to higher ratings, in general. How is this achieved? Let’s dive
into the intricacy of this machinery.&lt;/p>
&lt;h2 id="from-latent-to-rating">From Latent to Rating&lt;/h2>
&lt;p>$$
L ~~ \rightarrow ~~ P_{cum.}
~~ \rightarrow ~~ \begin{bmatrix} P_1 \\ P_2 \\ P_3 \\ P_4 \end{bmatrix}
~~ \rightarrow ~~ R \sim Categorical( \begin{bmatrix} P_1 \\ P_2 \\ P_3 \\ P_4 \end{bmatrix} )
\tag{1}
$$&lt;/p>
&lt;p>The path along the mapping of the latent scores onto the rating-scale
(ordinal) space is sketched above. The leftmost term $L$ stands for the
latent score, which we have learned to deduce from the simulations in
previous posts. It is also the starting point of this machinery of
converting real-valued scores to ordinal ratings. Things get a bit
complicated in the intermediate steps on the path. Therefore, indulge me
with explaining the path in reverse. I will start with the rightmost
term, which, monstrous as it may seem, is probably the least challenging
concept to be grasped here.&lt;/p>
&lt;h3 id="random-category-generator">Random Category Generator&lt;/h3>
&lt;p>The seemingly monstrous term represents the generation of a rating score
($R$) from a &lt;strong>categorical distribution&lt;/strong>. A categorical distribution
takes &lt;strong>a vector of $k$ probabilities&lt;/strong> as parameters. Each probability
specifies the chance that a particular category (one of the $k$
categories) gets drawn. In essence, a categorical distribution is simply
a bar chart in disguise. Each bar specifies the probability that the
category is sampled. In the example here, I set the number of categories
to $k = 4$, hence the four probability terms $P_1,~P_2, P_3,~P_4$.&lt;/p>
&lt;p>The code below plots a categorical distribution (bar chart) with 4
categories. The first line of the code specifies the relative odds of
producing the 4 categories: Category 2 and 3 are three times more likely
to be drawn than Category 1 and 4. Since the probabilities of all
categories must sum to one in a distribution, the second line of code
normalizes this vector to the correct probability scale.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Category&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">.5&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i,i), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-1-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Now, to sample from this distribution,
$Categorical( \begin{bmatrix} .125 \\ .375 \\ .375 \\ .125 \end{bmatrix} )$,
we simply use the &lt;code>sample()&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Sample one category from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 1
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Repeatedly sample from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e5&lt;/span>, replace&lt;span style="color:#f92672">=&lt;/span>T, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( P2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">table&lt;/span>(s) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">length&lt;/span>(s) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>s
1 2 3 4
0.12399 0.37616 0.37439 0.12546
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Empirical frequency distibution obtained through sampling&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Category&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">.5&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i,i), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P2[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-3-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>After drawing a large sample from this distribution, we can see that the
frequency distribution of the samples approaches the original
distribution.&lt;/p>
&lt;p>Back to the wine rating scenario. The categories in this context are the
available rating scores. Since I adopted the example of four categories,
in the rating-scale context, it would correspond to a 4-point Likert
scale in which &lt;code>1&lt;/code>, &lt;code>2&lt;/code>, &lt;code>3&lt;/code>, and &lt;code>4&lt;/code> are the four categories. One
crucial part is missing though. The categorical distribution is
order-agnostic: it knows nothing about the order of the categories it
generates. What it does is faithfully produce categories according to
the given probabilities. So, where does the order come from? It’s from
the relationship between rating probabilities and the latent scores.&lt;/p>
&lt;h3 id="enforcing-order-to-categories">Enforcing Order to Categories&lt;/h3>
&lt;p>When a higher latent score tends to give rise to a higher rating, an
order is automatically enforced on the categorical ratings (&lt;code>1&lt;/code>, &lt;code>2&lt;/code>,
&lt;code>3&lt;/code>, and &lt;code>4&lt;/code>). But how is this done? Recall the analogous situation of
the binary regression in the previous posts. Back then, the link between
the responses (&lt;code>0&lt;/code>/&lt;code>1&lt;/code>) and the latent scores is established through the
&lt;strong>probability&lt;/strong>: a higher latent score results in a higher probability
of generating &lt;code>1&lt;/code>. Thus, in general, higher latent scores tend to
produce &lt;code>1&lt;/code>s. A similar strategy can be deployed here: we bridge the
responses and the latent scores through probabilities. The crucial
difference is that we now get multiple, instead of one, probabilities to
deal with. Statisticians came up with a clever solution to this. Instead
of dealing with a vector of fluctuating probabilities, which breaks the
desired monotonically increasing relationship between the probabilities
and the ratings, the probabilities are transformed into a vector of
&lt;strong>cumulative probabilities&lt;/strong>. The nice thing about this vector of
cumulative probabilities is that the probabilities are now &lt;strong>ordered&lt;/strong>,
naturally. Larger cumulative probabilities now correspond to higher
rating scores. Sounds confusing? Let me re-describe these more vividly
with some code and plots. I’ll continue to use the four-point rating
example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P) ) &lt;span style="color:#75715e"># Probabilities for R = 1, 2, 3, 4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P) ) &lt;span style="color:#75715e"># Cumulative Probabilities for R = 1, 2, 3, 4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.500 0.875 1.000
&lt;/code>&lt;/pre>
&lt;p>The code above computes the cumulative probabilities (&lt;code>Pc&lt;/code>) from the
vector of rating probabilities (&lt;code>P&lt;/code>) through the function &lt;code>cumsum()&lt;/code>
(cumulative sum). Note that both vectors contain the same information.
The original vector can well be reconstructed from the cumulative
version. In math terms, their relationship is as follows:&lt;/p>
&lt;p>$$
\begin{aligned}
Pr(R=1) = Pr(R \leq 1)&amp;amp; \\
Pr(R=2) = Pr(R \leq 2)&amp;amp; - Pr(R \leq 1) \\
Pr(R=3) = Pr(R \leq 3)&amp;amp; - Pr(R \leq 2) \\
Pr(R=4) = Pr(R \leq 4)&amp;amp; - Pr(R \leq 3) \\
= \phantom{PPaa} 1 \phantom{aaa}&amp;amp; - Pr(R \leq 3)
\end{aligned}
\tag{2}
$$&lt;/p>
&lt;p>and in code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>Ps[2&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[1&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>] &lt;span style="color:#75715e"># or more generally, Ps[-1] - Ps[-length(Ps)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;p>The two vectors are visualized as distributions below. The red bars are the
probability distribution we have met in the previous section. The blue bars plot
the cumulative version of it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Rating&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i&lt;span style="color:#ae81ff">-.05&lt;/span>,i&lt;span style="color:#ae81ff">-.05&lt;/span>), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i&lt;span style="color:#ae81ff">+.05&lt;/span>,i&lt;span style="color:#ae81ff">+.05&lt;/span>), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,Pc[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-6-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Once we have an ordered sequence of probabilities, or more precisely,
probabilities with a monotonically increasing relationship to the rating
scores, we’ll be able to introduce latent scores through the &lt;strong>logit
link&lt;/strong>, as we have done in the binary case. We simply pass the
cumulative probabilities to the logit function to map them onto the real
space. To save space, I pack some commonly used functions into my
package &lt;a href="https://yongfu.name/stom/reference">&lt;code>stom&lt;/code>&lt;/a>, which can be
installed through the first two lines of commented code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># install.packages(&amp;#34;remotes&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># remotes::install_github(&amp;#34;liao961120/stom&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(stom)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">logit&lt;/span>(Pc) &lt;span style="color:#75715e"># convert cumulative probabilities to reals&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
&lt;/code>&lt;/pre>
&lt;p>The statistical machinery behind rating scales likely remains elusive
after my wordy explanation. Indeed, since we are only halfway through
the machinery, it would hardly make sense only by looking at part of the
picture. What I have presented so far is part of the machinery that
monotonically aligns the latent scores with the ratings, through the use
of cumulative probabilities. The second part of the machinery is to
allow for the shifting of the entire vector of latent scores (and thus
the probabilities of ratings, through the first part of the machinery)
by a common term, which enables the modeling of extraneous influences on
the ratings (thus the “regression”). Let’s now look at how this shifting
is achieved.&lt;/p>
&lt;h3 id="shifting-latent-scores">Shifting Latent Scores&lt;/h3>
&lt;p>The code below summarizes the first part of the rating-scale machinery:
establishing the link between latent scores and the probabilities of
rating scores.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#75715e"># vector of rating probs (starting point)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P) ) &lt;span style="color:#75715e"># vector of rating probs (cumulative)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( L &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logit&lt;/span>(Pc) ) &lt;span style="color:#75715e"># vector of latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
[1] 0.125 0.500 0.875 1.000
[1] -1.94591 0.00000 1.94591 Inf
&lt;/code>&lt;/pre>
&lt;p>Since all of the above mappings are one-to-one, we can as well express
the same machinery in reverse:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>L &lt;span style="color:#75715e"># vector of latent scores (starting point)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L) ) &lt;span style="color:#75715e"># vector of rating probs (cumulative)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)] ) &lt;span style="color:#75715e"># vector of rating probs &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P ) &lt;span style="color:#75715e"># draw one rating score from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
[1] 0.125 0.500 0.875 1.000
[1] 0.125 0.375 0.375 0.125
[1] 3
&lt;/code>&lt;/pre>
&lt;p>This second expression aligns well with the simulation perspective and
precisely lays out the data-generating process of the rating scores. It
also makes it clear that a &lt;em>predetermined&lt;/em> set of latent scores (or
probabilities of ratings) is required for generating the ratings. In a
simulation, these latent scores are determined by us. For a model, they
are a subset of parameters that the model tries to estimate from data.
These latent scores can be thought of as &lt;strong>baselines&lt;/strong> during rating.
That is, the latent scores, or more visually, the shape of the rating
distribution &lt;strong>before any factor has exerted an effect on the ratings&lt;/strong>.&lt;/p>
&lt;p>To model the extraneous influences on the ratings, we utilize an
independent term $\phi$ in the latent score space. The trick is to
&lt;strong>subtract&lt;/strong> this $\phi$ from the vector of the &lt;em>baseline latent
scores&lt;/em>. For instance, if a wine has a better-than-average quality that
raises its quality (latent score) by $1.9$ above the baseline but is
rated by a harsh judge that lowers the quantity by $1.1$, $\phi$ will be
$.8$. Subtracting $\phi=.8$ from the baseline latent scores gives the
shifted latent scores, from which the rating probabilities could then be
derived:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>latent_to_prob &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(L) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#a6e22e">return&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>phi &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.9&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1.1&lt;/span> &lt;span style="color:#75715e"># wine (1.9) &amp;amp; judge (-1.1) influence on ratings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>L &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>( Ls &lt;span style="color:#f92672">=&lt;/span> L &lt;span style="color:#f92672">-&lt;/span> phi ) &lt;span style="color:#75715e"># latent scores after influences of wine &amp;amp; judge&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#a6e22e">latent_to_prob&lt;/span>(Ls) &lt;span style="color:#75715e"># rating probs after influences of wine &amp;amp; judge&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
[1] -2.74591 -0.80000 1.14591 Inf
[1] 0.06031805 0.24970747 0.44873758 0.24123690
&lt;/code>&lt;/pre>
&lt;p>The bar chart below overlays the rating score distribution after
considering $\phi$ (blue bars) on the baseline distribution (red bars).
It can be seen that subtracting $\phi=.8$ from the baseline latent
scores pushes the probability mass toward the right, raising the
expected rating score.&lt;/p>
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>It might seem unintuitive that &lt;em>subtracting&lt;/em> a positive value from the
latent scores &lt;em>raises&lt;/em> the expected rating scores. But it’s simply the
effect of the cumulative probabilities. When the vector of the latent
scores gets shifted, note that the last term doesn’t move since it is
infinity ($logit(1) = \infty$). Thus, the difference between the last
and the second-to-last term, on the cumulative probability scale,
becomes larger after the shift. This difference is essentially the
probability of the largest rating ($P_4$ in our example). Therefore, the
effect of subtracting a positive value from the baseline latent scores
shifts the probability mass toward the larger ratings. For the remaining
ratings, the directions of changes in probability depend on the amount
of shift and the shape of the baseline distribution. It is thus hard to
conceive how these probability bars react to the shift in the latent
scores and how their shifts contribute to the increasing or decreasing
of the expected rating.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="ordlogit.png" alt="Interactive visualization of the rating probability
distribution">
&lt;figcaption>Interactive visualization of the rating probability
distribution&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>I’ve built an &lt;a href="https://yongfu.name/ordlogit">interactive
visualization&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> just to help with that.
As shown in the figure above, there are two places where users can
change to see how the shape of the rating distribution gets influenced.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The four vertical sliders are there to adjust the baseline
probabilities of the ratings, $Pr(R=1)$, $Pr(R=2)$, $Pr(R=3)$, and
$Pr(R=4)$ (abbreviated as $P_1$ ~ $P_4$ respectively). The
numerical value on top of each bar indicates the probability of that
rating. Note that it is the relative positions between the vertical
sliders that matter, and the four probabilities automatically adjust
to always sum to one.&lt;/p>
&lt;p>The three values, $\kappa_1$, $\kappa_2$, and $\kappa_3$, shown on
top of the four probabilities are the &lt;strong>cumulative logits&lt;/strong>, which
are basically the vector of the cumulative probabilities,
transformed to the logit scale. They are the &lt;strong>baseline latent
scores&lt;/strong> mentioned previously. The last term, $\kappa_4$ is dropped
since it is always infinite.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The horizontal slider above the vertical sliders controls the value
of $\phi$, which gets subtracted from each of the baseline latent
scores to derive the final distribution.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="wheres-the-regression">Where’s the Regression?&lt;/h2>
&lt;p>The previous section demonstrates how the baseline rating distribution
shifts according to an aggregated influence of $\phi$, which is the hard
part of the statistical machinery behind the rating scale IRT model.
Regression is the easy part. Now we have a nice and neat $\phi$ sitting
on the real space&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> to work with. If we zoom in on $\phi$, it’s simply
the summed effect of the predictor variables in a linear regression,
which is similar to $\mu$ in logistic regressions. The only difference
here is that we need a different linking distribution to map the effect
onto the response scale (i.e., discrete ratings). In math terms,
resuming our wine rating example, the distribution is shown in
&lt;a href="#math-eq3">(3)&lt;/a>:&lt;/p>
&lt;p>$$
\begin{aligned}
R_i &amp;amp; \sim OrderedLogit(\phi_i, ~ \bm{\kappa} = \begin{bmatrix} \kappa_1 \\ \kappa_2 \\ \kappa_3 \end{bmatrix} ) \\
\phi_i &amp;amp; = W_{Wid[i]} + J_{Jid[i]} \\
\tag{3}
\end{aligned}
$$&lt;/p>
&lt;p>The $OrderedLogit$ expression hides all the details from the reader. But
you’ve already seen the details at work in code form in previous
sections, albeit in a quite scattered manner. Later, I will collect them
into a single function. If you prefer clarity now, the monstrous
expressions in &lt;a href="#math-eq4">(4)&lt;/a> should be satisfying.&lt;/p>
&lt;p>$$
\begin{aligned}
R_i \sim Categorical &amp;amp; (
\begin{bmatrix}
Pr(R_i = 1) = Pr(R_i \le 1) \phantom{- Pr(R_i \le 1)} \\
Pr(R_i = 2) = Pr(R_i \le 2) - Pr(R_i \le 1) \\
Pr(R_i = 3) = Pr(R_i \le 3) - Pr(R_i \le 2) \\
Pr(R_i = 4) = Pr(R_i \le 4) - Pr(R_i \le 3) \\
\end{bmatrix}
) \\
logit[ Pr(R_i \le 1) ] &amp;amp;= logit[ Pr(R_i = 1) ] = \kappa_1 - \phi_i \\
logit[ Pr(R_i \le 2) ] &amp;amp;= \kappa_2 - \phi_i \\
logit[ Pr(R_i \le 3) ] &amp;amp;= \kappa_3 - \phi_i \\
logit[ Pr(R_i \le 4) ] &amp;amp;= logit(1) = \infty \\
\phi_i &amp;amp;= W_{Wid[i]} + J_{Jid[i]}
\tag{4}
\end{aligned}
$$&lt;/p>
&lt;p>Don’t worry if you cannot understand the equations in &lt;a href="#math-eq4">(4)&lt;/a>
right now. After you get accustomed to the logic of the ordered logit,
through coding, the expressions become straightforward. So now let’s
wrap up what we have done so far, in code. I will write down the code
form of the $OrderedLogit$ distribution in the function &lt;code>rOrdLogit()&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>rOrdLogit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(phi, kappa) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( kappa, &lt;span style="color:#66d9ef">Inf&lt;/span> ) &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> L &lt;span style="color:#f92672">=&lt;/span> kappa &lt;span style="color:#f92672">-&lt;/span> phi &lt;span style="color:#75715e"># latent scores, after shifting&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L) &lt;span style="color:#75715e"># map latent scores to cumulative probs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#75715e"># Compute probs for each rating from Pc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)] &lt;span style="color:#75715e"># probs of each rating&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> &lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(P), size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e">## Replicate previous example ##&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>( kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logit&lt;/span>( Pc )[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Pc)] ) &lt;span style="color:#75715e"># Set up baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#75715e"># 10,000 draws from OrdLogit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>draws &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( &lt;span style="color:#ae81ff">1e4&lt;/span>, &lt;span style="color:#a6e22e">rOrdLogit&lt;/span>(phi&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, kappa&lt;span style="color:#f92672">=&lt;/span>kappa) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e"># should approach P = c(.125, .375, .375, .125)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#a6e22e">table&lt;/span>(draws) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">length&lt;/span>(draws)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591
draws
1 2 3 4
0.1277 0.3741 0.3755 0.1227
&lt;/code>&lt;/pre>
&lt;h2 id="simulating-and-fitting-wine-ratings">Simulating and Fitting Wine Ratings&lt;/h2>
&lt;p>Having all concepts in place, let’s start synthesizing data for our
later model fitting. We will simulate data from the Ordered Logit
distribution. One minor limitation with &lt;code>rOrdLogit()&lt;/code> defined previously
is that it can only take a single value &lt;code>phi&lt;/code>, but it is more desirable
for &lt;code>phi&lt;/code> to be a vector of values. A vectorized version of
&lt;code>rOrdLogit()&lt;/code> is available in the &lt;code>stom&lt;/code> package as &lt;code>rordlogit()&lt;/code>. We
will be using this function for our data simulation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(stom)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">1025&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>Nj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">12&lt;/span> &lt;span style="color:#75715e"># number of judges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>Nw &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of wines&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>J &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>(Nj) &lt;span style="color:#75715e"># judge leniency&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>W &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>(Nw) &lt;span style="color:#75715e"># wine quality&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>J &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">standardize&lt;/span>(J) &lt;span style="color:#75715e"># scale to mean = 0, sd = 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>W &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">standardize&lt;/span>(W) &lt;span style="color:#75715e"># scale to mean = 0, sd = 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">-1.7&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1.7&lt;/span> ) &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># Create long-form data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Jid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Nj, Wid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Nw, KEEP.OUT.ATTRS&lt;span style="color:#f92672">=&lt;/span>F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>J &lt;span style="color:#f92672">=&lt;/span> J[d&lt;span style="color:#f92672">$&lt;/span>Jid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>W &lt;span style="color:#f92672">=&lt;/span> W[d&lt;span style="color:#f92672">$&lt;/span>Wid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>phi &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">nrow&lt;/span>(d), &lt;span style="color:#a6e22e">function&lt;/span>(i) d&lt;span style="color:#f92672">$&lt;/span>J[i] &lt;span style="color:#f92672">+&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>W[i] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rordlogit&lt;/span>( d&lt;span style="color:#f92672">$&lt;/span>phi, kappa ) &lt;span style="color:#75715e"># simulated rating responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>B &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>phi) ) &lt;span style="color:#75715e"># simulated binary responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>C &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( &lt;span style="color:#a6e22e">nrow&lt;/span>(d), d&lt;span style="color:#f92672">$&lt;/span>phi ) &lt;span style="color:#75715e"># simulated continuous responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span>&lt;span style="color:#75715e"># Conversion of data types to match model-fitting function&amp;#39;s requirements&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( v in &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Jid&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Wid&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span> d[[v]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d[[v]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ordered&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>R)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 360 obs. of 8 variables:
$ Jid: Factor w/ 12 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
$ Wid: Factor w/ 30 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
$ J : num 0.0187 -0.1794 -1.4372 1.3915 -0.1134 ...
$ W : num 0.236 0.236 0.236 0.236 0.236 ...
$ phi: num 0.2544 0.0564 -1.2014 1.6273 0.1223 ...
$ R : Ord.factor w/ 4 levels &amp;quot;1&amp;quot;&amp;lt;&amp;quot;2&amp;quot;&amp;lt;&amp;quot;3&amp;quot;&amp;lt;&amp;quot;4&amp;quot;: 4 2 2 2 1 1 1 4 2 3 ...
$ B : int 1 0 0 1 1 1 1 1 0 0 ...
$ C : num 0.6142 -1.5915 -0.2754 1.2755 0.0792 ...
&lt;/code>&lt;/pre>
&lt;p>Running the above code will get our data prepared. Two things might be
worth noting. The first is the &lt;code>standardize()&lt;/code> function, which centers
the input vector to zero mean and a standard deviation of one. &lt;code>J&lt;/code> and
&lt;code>W&lt;/code> are centered here to make the parameters later estimated by the
model comparable to the scale of the true values. In our later model, we
will partial-pool both the judges and the wines and hence assume a
zero-meaned distribution for both of them. Since the sample size of our
data isn’t large (12 judges and 30 wines), which will likely cause the
means of the raw &lt;code>J&lt;/code> and &lt;code>W&lt;/code> to have non-minor deviations from zero,
standardization is needed.&lt;/p>
&lt;p>Second, in addition to &lt;code>R&lt;/code>, the rating responses, I also simulate binary
responses &lt;code>B&lt;/code> (&lt;code>0&lt;/code>/&lt;code>1&lt;/code>) from &lt;code>phi&lt;/code>. Indeed, if a model is fitted with
&lt;code>B&lt;/code> as the dependent variable, it will be identical to the logistic
regression models fitted in previous posts. The binary responses are
simulated to demonstrate the parallels between the binary&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> model and
the rating scale model. The two models are highly similar: the linear
effects are aggregated in the same ways (in $\mu$/$\phi$). The only
difference is how these effects are projected onto the response scale:
the binary model does so through the Bernoulli distribution, and the
rating scale model through the Ordered Logit distribution.&lt;/p>
&lt;p>Another reason for simulating binary responses along the rating
responses is for the preparation of model debugging. As we start fitting
more and more complex models, we are bound to find ourselves lost in
situations where we have no idea why the model fails to give the
expected results. In such cases, it helps a lot to check the results
from simpler models, which might hint at where the complex model went
wrong. This is also the reason why I simulate the continuous responses
&lt;code>C&lt;/code>—to prepare data for fitting an even simpler model. By eliminating
the influences arising from nonlinear links in the GLMs, the normal
response model becomes more transparent and hence much easier to debug.&lt;/p>
&lt;p>For our wine rating example here, I’ve deliberately made the
data-generating process simple enough that our rating scale model can
smoothly fit and give us the expected results. To fit ordered logit
regressions with partial pooling structures, we need the &lt;code>clmm()&lt;/code>
function from the package
&lt;a href="https://cran.r-project.org/web/packages/ordinal">&lt;code>ordinal&lt;/code>&lt;/a>. The model
syntax in &lt;code>clmm()&lt;/code> is basically identical to the syntax we used in
&lt;code>lme4::glmer()&lt;/code> back then. As shown in the code below, we model the
rating scores (&lt;code>R&lt;/code>) to be influenced by both the wines and the judges.
By partial pooling wines and judges, the wine effects and the judge
effects are respectively assumed to come from a zero-meaned normal
distribution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(ordinal)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">clmm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Jid) &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Wid), data &lt;span style="color:#f92672">=&lt;/span> d )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">summary&lt;/span>(m)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Cumulative Link Mixed Model fitted with the Laplace approximation
formula: R ~ (1 | Jid) + (1 | Wid)
data: d
link threshold nobs logLik AIC niter max.grad cond.H
logit flexible 360 -455.07 920.14 182(726) 6.91e-06 4.9e+01
Random effects:
Groups Name Variance Std.Dev.
Wid (Intercept) 1.220 1.1046
Jid (Intercept) 0.853 0.9236
Number of groups: Wid 30, Jid 12
No Coefficients
Threshold coefficients:
Estimate Std. Error z value
1|2 -1.42044 0.36490 -3.893
2|3 0.05442 0.35607 0.153
3|4 1.56532 0.36674 4.268
&lt;/code>&lt;/pre>
&lt;p>&lt;code>summary(m)&lt;/code> prints out the model summary along with the estimated
baseline latent scores, which are labeled as &lt;code>Threshold coefficients&lt;/code>
above. You can see that these coefficients (-1.42, 0.054, and 1.565)
align pretty well with the &lt;code>kappa&lt;/code> set in the simulation (-1.7, 0, and
1.7).&lt;/p>
&lt;p>To examine the estimated wine and judge effects, we similarly utilize
the &lt;code>ranef()&lt;/code> function as demonstrated in the previous post:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>est_wine &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m)&lt;span style="color:#f92672">$&lt;/span>Wid[[1]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>est_judge &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m)&lt;span style="color:#f92672">$&lt;/span>Jid[[1]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( est_wine, W ); &lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( est_judge, J ); &lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="two-column">
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-15-1.svg"
data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-15-2.svg"
data-fig-align="center" />&lt;/p>
&lt;/div>
&lt;h2 id="item-response-theory-and-beyond">Item Response Theory and Beyond&lt;/h2>
&lt;p>We have come a long way, from the simplest binary item response model to models
with delicate machinery such as the rating scale model with partial-pooling
structures. The posts in this &lt;em>demystifying&lt;/em> series are sufficient, I suppose,
in providing a solid understanding of and practical skills for working with item
response theory. There are certainly even more complex IRT models, but I won’t
go further in that direction. No matter how many new and complex models are
added to the toolkit, we are certain to find our tools in shortage when facing
real-world problems.&lt;/p>
&lt;p>Item response models, general as they might seem, quickly run out of
supply. Although binary and rating scale models allow us to deal with
most response types found in the field (such as tests in educational
settings, scales measuring psychological constructs, and various surveys
used in the social sciences), even the slightest complication renders
these models useless. Just consider a mixed-format test consisting of,
for example, multiple-choice items (binary scored) and items of
open-ended questions (rated). Which IRT model can we apply to this
mixed-format test? Not a single one. Instead, we need two separate
models, each independently running on a subset of the test for a
particular item format. A special technique is then required to map the
independently estimated person/item parameters onto a common scale.&lt;/p>
&lt;p>The method works, but it wastes a lot of information. When separately
estimated, information cannot be shared across different item formats to
improve parameter estimation. Item estimates might be fine, as long as
there are many subjects. Person estimates suffer greatly though since,
in practice, the test length is limited and is now further divided up by
two independent models. This is equivalent to estimating person
parameters with fewer items.&lt;/p>
&lt;p>It is always better to incorporate &lt;em>everything&lt;/em> into a &lt;em>&lt;strong>single
comprehensive model&lt;/strong>&lt;/em> instead of separately modeling a subset of
variables in multiple small models. It is better because information
flows smoothly through the variables in a comprehensive model, but the
flow breaks down when the model gets torn apart into several pieces.
However, such comprehensive models are rarely, if not never, available
in the literature. We have to tailor a model ourselves according to what
the current situation demands. Therefore, a &lt;em>&lt;strong>framework&lt;/strong>&lt;/em> is required
to guide us through building up such a model.&lt;/p>
&lt;p>This post marks the end of the &lt;em>demystifying&lt;/em> series. When the thick cloud of
mystery begins to dissolve, we finally get to start solving real and exciting
problems rather than wrangling with mad statistical models. In my next post, I
will move on to Bayesian statistics, a &lt;em>&lt;strong>unified framework&lt;/strong>&lt;/em> that allows
flexibly extending a model to match the demanded conditions. Bayesian framework
is ideal for empirical research because it is &lt;em>practical&lt;/em>. We do not need to
wait for a statistician to come up with a model for every new situation. In
Bayesian inference, we simply describe the &lt;em>data-generating process&lt;/em> and the
&lt;em>priors&lt;/em>, and the rest is handled by probability theory and an estimation
algorithm. Therefore, we can focus on the scientific problems at hand instead of
fussing around with fancy models and their names. We will see how item response
models can be embedded into a larger network of causes and effects that
represents the assumed interactions underlying the current problem. Item
response models, which are essentially &lt;strong>methods for handling measurement
errors&lt;/strong>, help deal with the latent constructs measured indirectly through
surveys in this network of interacting variables.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The source code for building the interactive visualization of the
ordered logit distribution can be found on
&lt;a href="https://github.com/liao961120/ordlogit">GitHub&lt;/a>. It is built upon
&lt;a href="https://github.com/probstats/probstats.github.io">this&lt;/a> nice
project for visualizing various probability distributions.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Recall that $\phi$ works in the latent score space by increasing
or decreasing the baseline latent scores.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>In the testing context, a binary dependent variable is often used
for modeling correct/incorrect responses. In the current wine rating
context, a binary dependent variable could also be used for modeling
ratings. In such cases, there must only be two possible ratings,
such as mediocre/premium, on the wines.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (3/4)</title><link>https://yongfu.name/2023/03/29/irt3/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/03/29/irt3/</guid><description>&lt;h2 id="fixed-random-and-mixed">Fixed, Random and Mixed&lt;/h2>
&lt;p>Statistics is confusing enough through its massive terminology.
Psychology, which is largely experiment-oriented, further confuses
people by adding in its own flavor. A peek at the definitions of
&lt;a href="https://dictionary.apa.org/fixed-effect">fixed&lt;/a>,
&lt;a href="https://dictionary.apa.org/random-effect">random&lt;/a>, and
&lt;a href="https://dictionary.apa.org/mixed-effects-model">mixed&lt;/a> effects in the
&lt;a href="https://dictionary.apa.org">APA Dictionary of Psychology&lt;/a> exemplifies
this:&lt;/p>
&lt;blockquote>
&lt;p>[A mixed-effect model is] any statistical procedure or experimental
design that uses one or more independent variables whose levels are
&lt;strong>specifically selected by the researcher&lt;/strong> (fixed effects; e.g.,
gender) and one or more additional independent variables whose levels
are &lt;strong>chosen randomly&lt;/strong> from a wide range of possible values (random
effects; e.g., age).&lt;br>
&lt;b>&lt;/b>&lt;br>
—Definition of “mixed-effect model” in the APA Dictionary of
Psychology&lt;/p>
&lt;/blockquote>
&lt;p>The definitions for random and fixed effects above are not only
confusing but also misleading. In principle, whether a categorical
variable is “fixed” or “random” has nothing to do with the nature of the
variable (e.g., &lt;em>gender&lt;/em> doesn’t have to be fixed) or how the levels
within a variable are selected (randomly drawn or chosen by
researchers). Whether a variable is modeled as fixed or random is a
decision to be made by the modeler. And the modeler should &lt;strong>always
model variables as random&lt;/strong> if there are no justifiable prohibitive
reasons. Let me explain.&lt;/p>
&lt;h2 id="multilevel-instead-of-mixed">Multilevel Instead of Mixed&lt;/h2>
&lt;p>A better way to understand fixed and random effects is to think
&lt;strong>hierarchically&lt;/strong>. The levels of a random-effect variable are treated
as &lt;strong>related&lt;/strong> by the model, meaning that the effect of each level is
estimated by also considering information from other levels in the
variable. This is known as &lt;strong>partial pooling&lt;/strong>, and it has several
desirable properties. On the other hand, the levels within a
fixed-effect variable are treated as independent: during parameter
estimation, the model considers only information within each level. This
is the &lt;strong>no-pooling&lt;/strong> case. So how does the model incorporate
information from the other levels during estimation in the
partial-pooling case? To explain this, let me start with the no-pooling
case.&lt;/p>
&lt;p>As an example, suppose we have a categorical variable with $n$ levels.
Our goal is to obtain an estimate for each of these levels (and the
variability in the estimates), labeled as
$\alpha_1, \alpha_2, &amp;hellip;, \alpha_n$. To provide some context, we can
think of the categorical variable here as &lt;em>nationality&lt;/em>, and for each
nation (a level), we want to estimate the average height (the parameter)
of its citizens. When we are &lt;strong>not pooling information across the
levels&lt;/strong>, the structure of the data-generating process assumed by the
statistical model is shown in the figure below. Here, the model assumes
that there is a parameter associated with each level that generates the
observations. However, the parameters here are assumed to be
independent. Therefore, the model utilizes only the observations under
each level to estimate its parameter. What has been learned about a
nation is uninformative about another nation for the &lt;strong>no-pooling&lt;/strong>
model.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="tree0.svg" alt="Levels within a categorical variable estimated
independently.">
&lt;figcaption>Levels within a categorical variable estimated
independently.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>But there actually is some information, right? Take the perspective of
an alien, for instance. Knowing the average height of the Americans, say
5′9″, provides quite much information about the heights of the
Japanese—they are unlikely to be 60 feet or 6 inches, agree? This is why
we prefer to partial pool. Partial pooling allows the sharing of
information across different levels, which, as elucidated below, leads
to several desirable properties.&lt;/p>
&lt;p>When we want to incorporate—or partial pool—information from other
levels during estimation, we can utilize models that assume a
hierarchical structure on the levels within a variable. Such a
hierarchical structure is exemplified in the figure below. This
hierarchical structure assumes that all levels, or more precisely all
parameters underlying the levels, come from a common distribution. Here,
this common distribution is assumed to be a normal distribution with
mean $\mu$ and variance $\sigma^2$. The mean and the variance are to be
estimated from the data. When this structure is imposed, the
observations under different levels will be naturally linked since now,
the observations under every level all provide information for
estimating the common distribution.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="tree1.svg" alt="Levels within a categorical variable estimated by incorporating
information from all levels. This is achieved through assuming all
levels to come from a common distribution.">
&lt;figcaption>Levels within a categorical variable estimated by incorporating
information from all levels. This is achieved through assuming all
levels to come from a common distribution.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>What can be gained from partial pooling information across levels? As
mentioned above, when we partial-pool, the information of the
observations across all levels is shared. This means that the model
considers &lt;strong>more information&lt;/strong> for estimating the parameter of each
level. As a direct result of this, the model uses the data
(observations) more efficiently by squeezing out more information.
Secondly, it reduces overfitting and thus provides better
(out-of-sample) estimation. The model is less likely to overfit because
it is more “objective” by considering information across different
levels. Overfitting occurs when data are scarce, which is the case in
the no-pooling case since only data within each level are considered. In
such cases, the model bases the estimations on fewer observations and
hence tends to be overly sensitive to idiosyncratic patterns in the
local data. Another great thing about partial pooling is that it
automatically adjusts according to the sample sizes under each level.
For levels with fewer observations (e.g, North Korea, in which the alien
managed to collect only heights from three of its citizens), the model
places more weight on the overall information provided by other levels,
resulting in larger adjustments of the levels’ estimates. For levels
with abundant data, their estimates are only slightly affected by the
observations from other levels.&lt;/p>
&lt;p>Partial pooling essentially arises from the hierarchical structure
assumed in the models. Therefore, these models are known as
&lt;strong>hierarchical&lt;/strong> or &lt;strong>multilevel&lt;/strong> models. &lt;strong>Mixed (effect)&lt;/strong> models are
another common label for these models, though, as explained above, the
name is quite uninformative. To understand how these models work, it is
better to start with their hierarchical structuring. I will use the term
&lt;strong>multilevel models&lt;/strong> from now on to save ourselves from confusion. This
name is also nice in that it coincides in abbreviation with the mixed
model—both of them are abbreviated as GLMM for Generalized Linear
Multilevel/Mixed Models.&lt;/p>
&lt;h2 id="back-to-irt">Back to IRT&lt;/h2>
&lt;p>Now, we are acquainted with the concept of partial pooling and
multilevel models, let’s apply them to the IRT context to improve our
previous model, which is fitted without partial pooling across levels.
To warm up, let me rephrase the structure of the simulated IRT dataset
in terms of the multilevel terminologies.&lt;/p>
&lt;p>There are two variables at work here—the item variable and the person
(or subject) variable. Within the item variable, there are several
items. In other words, each item acts as a level within the item
variable. Similarly, each person corresponds to a level within the
person variable. For each item, we want to estimate a parameter, the
item’s difficulty. Likewise, for each person, we also want to estimate a
parameter, the person’s ability. To improve our model in estimating the
item/person parameters, we can partial pool information across the
levels &lt;strong>within&lt;/strong> the item and/or the person variable.&lt;/p>
&lt;p>The chunk below copies the data simulation code from the previous post,
with two minor changes. The first is the renaming of the variables for
the item and subject ID as &lt;code>Iid&lt;/code> (originally &lt;code>I&lt;/code>) and &lt;code>Sid&lt;/code> (originally
&lt;code>S&lt;/code>). The second is that, instead of item difficulty (&lt;code>D&lt;/code> in the
previous post), we conceptualize the effect of items as &lt;strong>easiness&lt;/strong>
(&lt;code>E&lt;/code>) here. Item easiness is simply the negative of item difficulty.
This simple switch would allow us to skip the step of reversing the item
effects’ signs returned by the regression model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>logit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p ) &lt;span style="color:#a6e22e">log&lt;/span>( p&lt;span style="color:#f92672">/&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">-&lt;/span>p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) ) &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">12&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>n_item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">60&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>n_resp &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">*&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>n_param &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">+&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( n_subj ) &lt;span style="color:#75715e"># Person ability&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>E &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1.6&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, length&lt;span style="color:#f92672">=&lt;/span>n_item ) &lt;span style="color:#75715e"># Item easiness&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># The data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Sid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj, Iid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_item, KEEP.OUT.ATTRS &lt;span style="color:#f92672">=&lt;/span> F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>mu &lt;span style="color:#f92672">=&lt;/span> A[d&lt;span style="color:#f92672">$&lt;/span>Sid] &lt;span style="color:#f92672">+&lt;/span> E[d&lt;span style="color:#f92672">$&lt;/span>Iid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>( d&lt;span style="color:#f92672">$&lt;/span>mu ) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>Sid &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>Sid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>Iid &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>Iid)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="unpooled-model">Unpooled Model&lt;/h3>
&lt;p>With the data prepared, let’s refit model
&lt;a href="https://yongfu.name/2023/03/06/irt2/#coding-models-the-easy-route">&lt;code>m1.2&lt;/code>&lt;/a> from the
previous post. Later, I will fit another model that partial pools the
subject variable (&lt;code>m2&lt;/code>) and compare it to the unpooled model here
(&lt;code>m1&lt;/code>).&lt;/p>
&lt;p>The code below for fitting &lt;code>m1&lt;/code> is identical to those in the previous
post, except that I adopt another method (starting from line 5) to
reconstruct the dropped estimate (forced by the sum-to-zero constraint).
This change is necessary, as it also allows us to reconstruct the
standard errors of the dropped estimate. We will need the standard
errors later to quantify the &lt;em>uncertainty&lt;/em> in the estimates, which are
used for comparing the unpooled and partial-pooled models. In addition,
the method adopted here is more principled and general, which further
consolidates our understanding of contrasts and dummy coding. However,
it takes up some space for the explanation since a little matrix algebra
is involved. I thus leave the details in the &lt;a href="#matrix-algebra">box&lt;/a> at
the end of the post.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>d1 &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>(d1&lt;span style="color:#f92672">$&lt;/span>Sid) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> Sid, data&lt;span style="color:#f92672">=&lt;/span>d1, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> (Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1))[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct std. error of the estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>(&lt;span style="color:#a6e22e">diag&lt;/span>(Vmat))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="partial-pooled-model">Partial-pooled Model&lt;/h3>
&lt;p>To fit the partial-pooled model, &lt;code>glmer()&lt;/code> from the &lt;code>lme4&lt;/code> package is
used.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(lme4)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glmer&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Sid), data&lt;span style="color:#f92672">=&lt;/span>d, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;logit&amp;#39;&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>lme4&lt;/code> provides a syntax for expressing multilevel models of different
structures. For our model here, which is one of the simplest multilevel
models (known as the varying intercept models), we express the partial
pooling of persons with the syntax &lt;code>(1|Sid)&lt;/code>, as shown in the last term
of the model formula. When such a partial pooling structure is
specified, &lt;code>glmer()&lt;/code> automatically imposes a constraint of &lt;strong>zero-meaned
normal distribution&lt;/strong> on the partial-pooled variable. In the case here,
this means that the ability of each person is modeled as being drawn
from a normal distribution with a mean of zero and an unknown standard
deviation to be estimated from the data. This constraint on the
distribution of the person ability naturally resolves the identification
issue of the IRT model. Hence, there is no need to impose an additional
sum-to-zero constraint as we did in &lt;code>m1&lt;/code>. We are only partial-pooling
the person variable here, so except for &lt;code>(1|Sid)&lt;/code>, everything else in
&lt;code>glmer()&lt;/code> is identical to those in &lt;code>m1&lt;/code>.&lt;/p>
&lt;p>After fitting the model, the estimates from &lt;code>m2&lt;/code> can be obtained with
the code below. Unpooled and partial-pooled estimates are extracted
differently in &lt;code>lme4&lt;/code>. To extract the unpooled estimates, one uses the
&lt;code>fixef()&lt;/code> function. These unpooled estimates, along with their standard
errors and other information, are also found in the model summary table
returned by &lt;code>summary()&lt;/code>. The partial-pooled estimates, however, are not
found in the table. To extract them, we need the &lt;code>ranef()&lt;/code> function as
shown below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2_eff.item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">fixef&lt;/span>(m2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2_eff.subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In addition to the estimates, we would also like to retrieve their
standard errors. Similar to the estimates, the standard errors of the
estimates are extracted differently according to whether they are
unpooled (fixed) or partial-pooled (random). We can utilize &lt;code>se.fixef()&lt;/code>
and &lt;code>se.ranef()&lt;/code> from the &lt;code>arm&lt;/code> package to extract these standard
errors:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2_se.item &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.fixef&lt;/span>(m2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2_se.subj &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.ranef&lt;/span>(m2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, to compare the estimates of &lt;code>m1&lt;/code> and &lt;code>m2&lt;/code>, I plot them together
in the same figure. I also plot the uncertainty—calculated as
$\pm 2 \times Standard~error$—around each estimate. Estimates and
uncertainties from &lt;code>m1&lt;/code> are plotted as blue, whereas those from &lt;code>m2&lt;/code> are
plotted as pink. The true effects for generating the simulated data are
plotted as solid black points.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Concatenate item &amp;amp; subj effect/std to match m1_eff/m1_se&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>m2_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2_eff.item, m2_eff.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m2_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2_se.item, m2_se.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e">#&amp;#39; Function stolen from `rethinking::col.alpha()`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>col.alpha &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function &lt;/span>(acol, alpha &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.5&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> acol &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">col2rgb&lt;/span>(acol)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> acol &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rgb&lt;/span>(acol[1]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, acol[2]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, acol[3]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, alpha)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> acol
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># Plot for comparing `m1` &amp;amp; `m2`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, ylim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-4.8&lt;/span>, &lt;span style="color:#ae81ff">4.8&lt;/span>), xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, n_subj&lt;span style="color:#f92672">+&lt;/span>n_item &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Effect&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Item Index&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>n_item &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">segments&lt;/span>( &lt;span style="color:#ae81ff">-5&lt;/span>, &lt;span style="color:#a6e22e">mean&lt;/span>(m2_eff.item), n_item&lt;span style="color:#ae81ff">+.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">segments&lt;/span>( n_item&lt;span style="color:#ae81ff">+.5&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1000&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(E, A), pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e"># Uncertainty bars&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#a6e22e">seq_along&lt;/span>(m2_se)) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i,i), m1_eff[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>m1_se[i], col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">4&lt;/span>), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">6&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i,i), m2_eff[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>m2_se[i], col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">.7&lt;/span>), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( m1_eff, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( m2_eff, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part3_files/figure-commonmark/unnamed-chunk-6-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="shrinkage">Shrinkage&lt;/h3>
&lt;p>Some of the benefits of partial pooling &lt;a href="#multilevel-instead-of-mixed">discussed
earlier&lt;/a> are visualized in the comparison
plot above. The most drastic changes from the unpooled to the
partial-pooled model are seen in the ability estimates, which are
exactly the levels that get partially pooled. Two things to notice here.
First, there is less uncertainty in the partial-pooled estimates than in
the unpooled ones (pink bars tend to be shorter than their blue
counterparts). This follows naturally because, through partial pooling,
the model has access to more information (hence less uncertainty) for
each level. Secondly, the partial-pooled estimates tend to get “pulled”
towards the center (i.e., the grand mean of the subject estimates). In
addition, more extreme estimates are further pulled toward the center.
Essentially, this means that the model behaves in a way that is robust
against observations that result in extreme estimates. This is known as
&lt;strong>shrinkage&lt;/strong> and is also a feature that naturally arises from partial
pooling.&lt;/p>
&lt;p>From the figure, we can see that partial pooling improves the estimation
of the person abilities, as most pink circles are found to be much
closer to the solid black dots (true effects) than the blue ones. For
item easiness, which are not partial-pooled, the estimates also improve
slightly. This results from the improvement in estimating abilities.
Since ability and easiness are jointly estimated by the model, the
improvement from ability estimation carries on to easiness estimation.
Given the large improvement in ability estimates, one might consider
also pooling the items. Indeed, there is no reason to not pool.
&lt;strong>Partial pooling should be the default&lt;/strong>.&lt;/p>
&lt;h3 id="partial-pool-items-and-subjects">Partial Pool Items and Subjects&lt;/h3>
&lt;p>To specify the partial pooling of items in the model, we again utilize
the “bar” syntax: &lt;code>(1|Iid)&lt;/code>. This allows &lt;code>glmer()&lt;/code> to also model the
items as coming from a normal distribution with zero mean and unknown
variance. Now, since both the items and the subjects are centered at
zeros, an additional step is needed to reconstruct the zero-centered
item estimates back to their original locations&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This is why the
model formula in &lt;code>m2.2&lt;/code> uses &lt;code>1&lt;/code> instead of &lt;code>-1&lt;/code>. By specifying &lt;code>1&lt;/code>,
&lt;code>glmer()&lt;/code> estimates an independent global intercept. In the case here,
this intercept is identical to the amount subtracted from the item
effects for centering. Hence, to reconstruct the original non-centered
item estimates, we add the global intercept back to the item estimates,
as shown in line 4 in the code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glmer&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Iid) &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Sid), data&lt;span style="color:#f92672">=&lt;/span>d, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;logit&amp;#39;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>m2.2_eff.subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2.2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>m2.2_eff.item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2.2)&lt;span style="color:#f92672">$&lt;/span>Iid[, &lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">fixef&lt;/span>(m2.2)[[&lt;span style="color:#e6db74">&amp;#34;(Intercept)&amp;#34;&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>m2.2_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2.2_eff.item, m2.2_eff.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>m2.2_se &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.ranef&lt;/span>( m2.2 )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>m2.2_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2.2_se&lt;span style="color:#f92672">$&lt;/span>Iid, m2.2_se&lt;span style="color:#f92672">$&lt;/span>Sid )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can compare &lt;code>m2.2&lt;/code> to &lt;code>m2&lt;/code> by plotting their estimates with the
plotting code previously shown and see that &lt;code>m2.2&lt;/code> further improves the
estimation (though not large). In the psychometric/measurement
literature, partial pooling both item and person is uncommon. But as
seen in our simulate-and-fit approach, partial pooling results in better
estimation. This approach also refutes the unjustified belief that
“&lt;em>fixed&lt;/em> effects should be used when the levels are &lt;strong>specifically
selected by the researcher&lt;/strong>”. In our simulation, values of the item
easiness are specifically “picked out”. They are deliberately set to be
equally-spaced values. And still, we saw that modeling the item effects
as &lt;em>random&lt;/em> is not only benign but even improves estimation. This is
true in general, and you can change the values of the item easiness in
the simulation to see that partial pooling mostly, if not always, gives
better estimates.&lt;/p>
&lt;h2 id="whats-next">What’s next&lt;/h2>
&lt;p>So far, we have been dealing with item response models with dichotomous
item responses. That is, a response can only either be correct (&lt;code>1&lt;/code>) or
wrong (&lt;code>0&lt;/code>). In &lt;a href="https://yongfu.name/irt4">Part 4&lt;/a>, we move on to item response models for
rating responses. These models are extremely useful since rating scales
are common in the social sciences. The models also allow us to model the
so-called “rater effect”, which quantifies the leniency of the raters.
By incorporating such rater effects, the model corrects for potential
biases introduced by subjective ratings, thereby giving more accurate
person and item estimates.&lt;/p>
&lt;!-- ####################################################################### -->
&lt;div id="matrix-algebra" class="Box"
title="Reconstructing dropped levels with the contrast matrix">
&lt;p>Don’t be intimidated by matrix algebra. It’s simply arithmetics in a
fancy manner, and it looks scary only because it does many things at
once. With some patience, you will be able to break down and understand
the steps involved.&lt;/p>
&lt;h4 id="reconstructing-dropped-estimate">Reconstructing Dropped Estimate&lt;/h4>
&lt;p>Let’s first see how the contrast matrix reconstructs the dropped
estimate from the sum-to-zero constrained model. I’ll start with a toy
example with only three subjects, $S_1, S_2, S_3$. The contrast matrix
for imposing a sum-to-zero constraint on the subjects is shown below.
Recall that the sum-to-zero constraint is coded through the dropping of
the last subject, $S_3$ (hence two columns left in the contrast matrix),
and implicit coding of $S_3$’s information into $S_1$ and $S_2$ by the
&lt;code>-1&lt;/code>s on the third row. Given this coding, the effect of $S_3$ can be
reconstructed from the effects of $S_1$ and $S_2$ by taking the negative
of their sum. This can be done through the code
&lt;code>c(subj_eff.m1.2, -sum(subj_eff.m1.2) )&lt;/code> from the previous post.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">$S_1$&lt;/th>
&lt;th style="text-align:center">$S_2$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">$S_1$&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$S_2$&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$S_3$&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The same thing can be done through &lt;strong>matrix multiplication&lt;/strong>. Simply
take the above 3-by-2 contrast matrix and multiply the 2-by-1 column
vector of the estimated effects for $S_1$ and $S_2$, which I abbreviate
as $E_1$ and $E_2$ here. The last entry of the resulting column vector
would then give what we want.&lt;/p>
&lt;p>$$
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1 \\
-1 &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
E_1 \\
E_2
\end{bmatrix} =
\begin{bmatrix}
E_1 \\
E_2 \\
-E_1 - E_2
\end{bmatrix}
$$&lt;/p>
&lt;p>Here’s the R code version of the above matrix multiplication:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>) &lt;span style="color:#75715e"># Contrast matrix coding sum-to-zero constraint&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( E1&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.5&lt;/span>, E2&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.7&lt;/span> ) &lt;span style="color:#75715e"># Made-up effect of S1 and S2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">%*%&lt;/span> eff
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [,1]
1 1.5
2 1.7
3 -3.2
&lt;/code>&lt;/pre>
&lt;h4 id="reconstructing-standard-error-of-dropped-estimate">Reconstructing Standard Error of Dropped Estimate&lt;/h4>
&lt;p>The contrast matrix can similarly be applied to reconstruct the variance
( hence the standard error) of the dropped subject’s estimate. The
reconstruction is based on the &lt;strong>variance sum law&lt;/strong>,
$Var(X+Y) = Var(X) + Var(Y) + 2~Cov(X,Y)$, which has a natural
generalization through matrix notations. Hence, given the variance of
the estimates for $S_1$ and $S_2$ and their covariance, we will be able
to reconstruct the variance of $E_3$ as&lt;/p>
&lt;p>$$
\begin{equation}
Var(E_3) = Var(E_1) + Var(E_2) + 2~Cov(E_1,E_2) \tag{1}
\end{equation}
$$&lt;/p>
&lt;p>The variances and covariances of the estimates are given by the
(variance-)covariance matrix of the fitted model. The formula below
shows the matrix generalization to the variance sum law. Note that
through the matrix generalization, we also get the reconstructed
covariances, as shown in the off-diagonal entries in the reconstructed
covariance matrix (the right-most matrix). The variance of $E_1$, $E_2$,
and $E_3$ are found on the diagonal. The standard errors of the
estimates are then obtained by taking the square roots of these diagonal
entries.&lt;/p>
&lt;img src="Cov.svg" style="width:95.0%" />
&lt;p>In R, the same calculation is done with the code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>) &lt;span style="color:#75715e"># Contrast matrix for coding sum-to-zero constraint&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># Made-up variances-covariances matrix of the estimates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">matrix&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0.3&lt;/span>, &lt;span style="color:#ae81ff">-0.01&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#ae81ff">0.0&lt;/span>, &lt;span style="color:#ae81ff">0.4&lt;/span> ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> byrow&lt;span style="color:#f92672">=&lt;/span>T, nrow&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">%*%&lt;/span> Vmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat) &lt;span style="color:#75715e"># Reconstructed variance-covariance matrix&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> 1 2 3
1 0.3 -0.01 -0.29
2 0.0 0.40 -0.40
3 -0.3 -0.39 0.69
&lt;/code>&lt;/pre>
&lt;h4 id="back-to-the-code">Back to the Code&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>d1 &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>(d1&lt;span style="color:#f92672">$&lt;/span>Sid) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> Sid, data&lt;span style="color:#f92672">=&lt;/span>d1, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> (Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1))[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct std. error of the estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>(&lt;span style="color:#a6e22e">diag&lt;/span>(Vmat))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once familiar with the matrix algebra discussed, the above code for
reconstructing the dropped subject’s effect should become
self-explaining. The only complication here is that instead of using the
contrast matrix of the subjects, a larger matrix encompassing the coding
of &lt;strong>all levels of both the item and the subject variables&lt;/strong> is used to
match the covariance matrix given by the model (which also contains all
levels from all variables). This large contrast matrix can be thought of
as the concatenation of two contrast matrices along the diagonal, with
the remaining off-diagonal entries filled in with zeros. To better
explain this, let me go back to our previous example with three
subjects.&lt;/p>
&lt;p>To keep things simple, let’s assume additionally that there are only
three items. Since in the model, the sum-to-zero constraint is only
imposed on the subjects, the contrast matrix for the coding of items
would be a 3-by-3 identity matrix. Concatenating the item and subject
contrast matrices in the way mentioned above results in the matrix:&lt;/p>
&lt;p>&lt;code>$$ \begin{bmatrix} \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}_{3 \times 3} &amp;amp; 0~~~~~ \\ 0 &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}_{3 \times 2} \end{bmatrix}_{6 \times 5} $$&lt;/code>&lt;/p>
&lt;p>In general, with $n_I$ items and $n_S$ subjects, this concatenated
contrast matrix has the form:&lt;/p>
&lt;p>&lt;code>$$ \begin{bmatrix} ~~\mathrm{I}_{n_I \times n_I}~~~ &amp;amp; 0 ~~~~~~~~~~~~~~ \\ \\ 0 &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; &amp;amp; 0 \\ \vdots &amp;amp; &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\ -1 &amp;amp; -1 &amp;amp; \cdots &amp;amp; -1 \end{bmatrix}_{n_S \times (n_S - 1)} \end{bmatrix}_{ (n_I + n_S) \times (n_I + n_S - 1) } $$&lt;/code>&lt;/p>
&lt;p>This is what the second part of the above code (reproduced below) is
doing. It first sets up the correct shape of this large contrast matrix
according to the number of items and subjects. The trick here is to use
the &lt;code>diag()&lt;/code> function to initialize a square matrix of zeros and drop
one of the columns to match the correct number of dimensions. Then, line
3 of the code sets the upper-left portion of this matrix (the item
sub-matrix) as an identity matrix by filling in the diagonal with ones.
Finally, the lower-right portion of the matrix (the subject sub-matrix)
is replaced with the subject contrast matrix constructed by the
&lt;code>contr.sum()&lt;/code> function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the contrast matrix &lt;code>Cmat&lt;/code> prepared, we can construct what we need
through matrix algebra. The estimates for all levels, including the
dropped one, are reconstructed by multiplying &lt;code>Cmat&lt;/code> with the estimates
returned by the model. This is illustrated in the line below. The
estimates are given by &lt;code>coef(m1)&lt;/code>, and the &lt;code>[, 1]&lt;/code> at the end of the
line forces the resulting one-column matrix into vector form.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> ( Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1) )[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The full covariance matrix is similarly reconstructed through &lt;code>Cmat&lt;/code> and
the covariance matrix extracted from the model (&lt;code>vcov(m1)&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Since the final products we need are the standard errors, we extract the
diagonal entries of &lt;code>Vmat&lt;/code> and take the square root:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>( &lt;span style="color:#a6e22e">diag&lt;/span>(Vmat) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/div>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>We don’t touch the subject estimates, though, since we assume them
to be centered at zero in the simulation, remember?&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (2/4)</title><link>https://yongfu.name/2023/03/06/irt2/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/03/06/irt2/</guid><description>&lt;p>In &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>, we went through the simplest item response model,
the 1PL model, from the perspective of simulations. Starting with item
difficulty and testee ability, we &lt;strong>worked forward&lt;/strong> to simulate item
responses that mimic real-world data. Back then, we were precisely
laying out the &lt;strong>data generating process&lt;/strong> that is assumed by the item
response theory. In this post, we &lt;strong>work backward&lt;/strong>. We will start with
the item responses and work back toward the unobserved difficulties and
abilities, with the help of statistical models. But first, let’s
simulate the data we will be using!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) ) &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">12&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>n_item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>n_subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">60&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_resp &lt;span style="color:#f92672">=&lt;/span> n_subj &lt;span style="color:#f92672">*&lt;/span> n_item &lt;span style="color:#75715e"># number of responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n_subj, mean&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, sd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> ) &lt;span style="color:#75715e"># Subjects&amp;#39; ability&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1.6&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, length&lt;span style="color:#f92672">=&lt;/span>n_item ) &lt;span style="color:#75715e"># Items&amp;#39; difficulty&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># The data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( S&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj, I&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_item, KEEP.OUT.ATTRS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">FALSE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>(A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I]) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>S &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>S)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>I)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 1800 obs. of 3 variables:
$ S: Factor w/ 60 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
$ I: Factor w/ 30 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
$ R: int 0 1 0 0 1 1 1 1 1 1 ...
&lt;/code>&lt;/pre>
&lt;p>In the code above, the first two lines are the definitions for the
logistic and the Bernoulli functions used previously. The second chunk
of code sets the shape of our data. This time, the simulated data is
much larger, with 30 items and 60 testees, or &lt;strong>subjects&lt;/strong> (I will use
the more general term “subject” hereafter). Since we assume here that
each subject responds to every item, this gives us 1800 responses in the
data.&lt;/p>
&lt;p>The subject abilities come from a normal distribution with a zero mean
and a standard deviation of one (the standard normal). The item
difficulties are equally-spaced values that range from -1.6 to 1. A
notable change from the previous post is that &lt;strong>number indices&lt;/strong> are
used here for labeling items (&lt;code>I&lt;/code>) and subjects (&lt;code>S&lt;/code>). For simple
illustrations, letter indices are clearer. But for larger data sets,
number indices are easier to manipulate with code. Now, since &lt;code>S&lt;/code> and
&lt;code>I&lt;/code> are coded as integers, we need to explicitly convert them into
factors. Otherwise, the model will treat the number indices as values in
a continuous variable.&lt;/p>
&lt;h2 id="dags-revisited">DAGs Revisited&lt;/h2>
&lt;p>Before we move on to the statistical model, let me lay out the DAGs
again. The DAG on the right below is identical to the one in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a> (the left DAG here), but with a slight modification that
emphasizes the perspective from the statistical model. Here, the
observed $S$ and $I$ take place of the unobserved $A$ and $D$,
respectively. So why the difference?&lt;/p>
&lt;div class="goat svg-container ">
&lt;svg
xmlns="http://www.w3.org/2000/svg"
font-family="Menlo,Lucida Console,monospace"
viewBox="0 0 352 137"
>
&lt;g transform='translate(8,16)'>
&lt;path d='M 88,16 L 120,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 280,16 L 312,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 168,16 L 168,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 32,80 L 48,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 224,80 L 240,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,48 L 96,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,48 L 288,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 48,48 L 56,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='66.000000,48.000000 54.000000,42.400002 54.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 48.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 72,32 L 80,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='98.000000,48.000000 86.000000,42.400002 86.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 80.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='128.000000,16.000000 116.000000,10.400000 116.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 120.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 240,48 L 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='258.000000,48.000000 246.000000,42.400002 246.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 240.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 264,32 L 272,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='290.000000,48.000000 278.000000,42.400002 278.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 272.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='320.000000,16.000000 308.000000,10.400000 308.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 312.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 64,0 A 16,16 0 0,0 48,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 64,0 A 16,16 0 0,1 80,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 256,0 A 16,16 0 0,0 240,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 256,0 A 16,16 0 0,1 272,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 48,16 A 16,16 0 0,0 64,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,16 A 16,16 0 0,1 64,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 240,16 A 16,16 0 0,0 256,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,16 A 16,16 0 0,1 256,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 A 16,16 0 0,0 8,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 A 16,16 0 0,1 40,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,80 A 16,16 0 0,0 88,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,80 A 16,16 0 0,1 120,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 8,96 A 16,16 0 0,0 24,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 40,96 A 16,16 0 0,1 24,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 88,96 A 16,16 0 0,0 104,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 120,96 A 16,16 0 0,1 104,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;text text-anchor='middle' x='24' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='64' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='104' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='144' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;text text-anchor='middle' x='216' y='100' fill='currentColor' style='font-size:1em'>S&lt;/text>
&lt;text text-anchor='middle' x='256' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='296' y='100' fill='currentColor' style='font-size:1em'>I&lt;/text>
&lt;text text-anchor='middle' x='336' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;/g>
&lt;/svg>
&lt;/div>
&lt;p>Recall that the nodes $A$ and $D$ represent the joint influences of a
subject’s ability and an item’s difficulty on the probability of success
on that item. However, the statistical model cannot notice $A$ and $D$
since they are &lt;strong>theoretical concepts&lt;/strong> proposed by the IRT. What the
model “sees” is more similar to the DAG on the right. This DAG is
theoretically neutral. All it says is that the probability of success is
influenced by the particular subject and item present in an observation.
It does not further comment on the factors underlying each subject/item
that lead to the results.&lt;/p>
&lt;p>Given the data and the right DAG, the statistical model estimates the
so-called &lt;strong>subject effects&lt;/strong> and &lt;strong>item effects&lt;/strong>. These effects will
be estimates of subject ability and item difficulty &lt;strong>if the IRT
assumptions are met&lt;/strong>: when a subject and an item influence the result
&lt;strong>only through subject ability and item difficulty&lt;/strong>. With the concepts
of &lt;strong>subject/item effects&lt;/strong> in place, we can move on to the formulas of
the statistical model.&lt;/p>
&lt;h2 id="equation-index-and-annoying-things">Equation, Index and Annoying Things&lt;/h2>
&lt;p>The equations in (1) are the formulation of our model. This model is
known as the &lt;a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
regression&lt;/a>, or in
GLM terms, the Generalized Linear Model of the binomial family with the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit link&lt;/a> (more on this later).
Lots of things are going on here. Don’t panic, I’ll walk you through
slowly.&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim Bernoulli( P_i ) \\
&amp;amp; P_i = logistic( \mu_i ) \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{1}
$$&lt;/p>
&lt;p>First, note the common subscript $_i$ to the variables above. The
presence of this common $_i$ indicates that &lt;strong>the equations are read at
the &lt;em>observational&lt;/em> level&lt;/strong>. The observational level is easier to think
of with help of the &lt;a href="https://en.wikipedia.org/wiki/Wide_and_narrow_data">long data
format&lt;/a>. In this
long form of data, each row records an observation and is indexed by the
subscript $_i$. So you can think of the set of three equations as
describing the links among the variables for each observation. Note that
the long data format is also the format we have been using for the data
frames.&lt;/p>
&lt;p>The last equation in (1), also related to the reading of the subscript
$_i$, deserves some elaboration, as some might feel confused about the
square brackets after $\alpha$ and $\delta$. Actually, we have already
met these brackets in &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>. The brackets here serve a similar
function to R’s subset function &lt;code>[]&lt;/code> that we have used for linking
particular ability/difficulty levels of a subject/item to the rows
(observations) of the data frame. So what the square brackets after
$\alpha$ and $\delta$ do exactly, is to “look up” the index of the
subject and item for the $_i$th observation such that the $\alpha$
corresponding to the subject and the $\delta$ corresponding to the item
could be correctly retrieved. The model can thus “know” which $\alpha$
and $\delta$ to update when it encounters an observation. For instance,
suppose we are on the 3rd row (observation) of the data, in which
$S_3 = 5$ and $I_3 = 8$. This tells the model that the observation gives
information about $\alpha_5$ and $\delta_8$. The model thus learns
something about them and updates accordingly.&lt;/p>
&lt;p>I haven’t written about $\alpha$ and $\delta$ yet, but based on the
previous paragraph, you might already know what they are: $\alpha$s are
the subject effects and $\delta$s the item effects to be estimated by
the model.&lt;/p>
&lt;p>Now, let me walk you through the equations from bottom to top:&lt;/p>
&lt;ul>
&lt;li>$\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$&lt;br>
No surprise here. This equation simply illustrates how the model
computes a new variable $\mu$ from $\alpha$ and $\delta$.&lt;/li>
&lt;li>$P_i = logistic( \mu_i )$&lt;br>
The equation should look familiar. It indicates how the model maps
$\mu$, which can range from $-\infty$ to $\infty$, to probability,
$P$, through the logistic function.&lt;/li>
&lt;li>$R_i \sim Bernoulli( P_i )$&lt;br>
This equation describes that each observed response is generated from
a Bernoulli distribution with probability $P_i$. Or even simpler,
$R_i$ would be $1$ with probability $P_i$ and $0$ with probability
$1 - P_i$.&lt;/li>
&lt;/ul>
&lt;p>These equations all look familiar because they are essentially
mathematical representations of the simulation we have done. Here, the
model formulation is simply simulation in reverse.&lt;/p>
&lt;h3 id="the-logit-link">The Logit Link&lt;/h3>
&lt;p>The GLM formulation of (1) is often seen in an alternative form in (2).
The only difference between (2) and (1) lies in the second equation.
Instead of the logistic function, the second equation in (2) uses the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit&lt;/a> function. What is the
logit?&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim Bernoulli( P_i ) \\
&amp;amp; logit(P_i) = \mu_i \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{2}
$$&lt;/p>
&lt;p>The logit function is simply the &lt;strong>mirror of the logistic&lt;/strong>. They do the
same mapping but in &lt;strong>reverse directions&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>the logistic function maps real numbers to probabilities&lt;/li>
&lt;li>the logit function maps probabilities to real numbers&lt;/li>
&lt;/ul>
&lt;p>The logistic and the logit are &lt;strong>inverse functions&lt;/strong> to each other. So
if a real number gets converted to the probability by the logistic, the
logit can convert it back to the original real, and vice versa.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>logit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p ) &lt;span style="color:#a6e22e">log&lt;/span>( p&lt;span style="color:#f92672">/&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">-&lt;/span>p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(x) ) &lt;span style="color:#75715e"># Transformed x on probability space&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] 0.2689414 0.2890505 0.3100255 0.3318122 0.3543437 0.3775407 0.4013123
[8] 0.4255575 0.4501660 0.4750208 0.5000000 0.5249792 0.5498340 0.5744425
[15] 0.5986877 0.6224593 0.6456563 0.6681878 0.6899745 0.7109495 0.7310586
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Logit gives x back&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">logit&lt;/span>(p)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4
[16] 0.5 0.6 0.7 0.8 0.9 1.0
&lt;/code>&lt;/pre>
&lt;p>Some arithmetics would get us from the logistic to the logit:&lt;/p>
&lt;p>$$
\begin{aligned}
logistic(x) &amp;amp;= \frac{1}{1 + e^{-x}} = p \\
&amp;amp; \Rightarrow ~~ e^{-x} = \frac{1-p}{p} \\
&amp;amp; \Rightarrow ~ -x = log(\frac{1-p}{p}) \\
&amp;amp; \Rightarrow ~~ x = -log(\frac{1-p}{p}) \\
&amp;amp; \phantom{\Rightarrow ~~ x } = log(\frac{p}{1-p}) = logit(p)
\end{aligned}
$$&lt;/p>
&lt;p>There is really nothing special about the logit function. We have
learned all the important things through the logistic back in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a>. I mention the logit here simply because the term is
frequently used. When people talk about GLMs, they prefer to use the
&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">link
function&lt;/a>
to characterize the model. The link function, in the case of the
logistic regression here, is the logit function. It transforms the
outcome probabilities into real numbers that are modeled linearly. It’s
just the logistic, but works in the reverse direction.&lt;/p>
&lt;h2 id="fitting-glm">Fitting GLM&lt;/h2>
&lt;p>Now, we are packed with the statistical muscles to carry out the
analysis. Let’s fit the model on the data we’ve simulated. In R, this is
done through the function &lt;code>glm()&lt;/code>. The first argument of &lt;code>glm()&lt;/code> is the
formula, in which we specify our linear model with R’s model syntax.
There are in principle two ways, one succinct and the other tedious, to
express the formula &lt;strong>when there are &lt;em>categorical predictors&lt;/em> in the
model&lt;/strong>. I will first demonstrate the tedious one, as it exposes all the
details hidden by the succinct form. Though tedious, it saves us from
confusion.&lt;/p>
&lt;h3 id="dummy-coding">Dummy Coding&lt;/h3>
&lt;p>The formulas we specify in &lt;code>glm()&lt;/code> (and other model fitting functions in
general) correspond pretty well to their mathematical counterparts. So
let me first present the math before we move on to the code. Lots of
things to explain here.&lt;/p>
&lt;p>Equation (3.2) is rewritten from the last two equations,
$logit(P_i) = \mu_i$ and $\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$ in
(2), which I reproduce here in Equation (3.1) by combining the two
equations.&lt;/p>
&lt;p>Earlier I mentioned that the square brackets after $\alpha$ and $\delta$
serve as a “look up” function to locate the relevant $\alpha$ and
$\delta$ of each subject and item in an observation. There is an
equivalent way to express the same formula without the use of these
“look up” functions, which is shown in equation (3.2). For the sake of
simplicity, let’s assume here that we have only two items (A, B) and
three subjects (J, K, L). For real data, equation (3.2) would be
extremely long.&lt;/p>
&lt;p>$$
\begin{align}
\tag{3.1} logit(\mu_i) &amp;amp;= \alpha_{[S_i]} + \delta_{[I_i]} \\
\tag{3.2} logit(\mu_i) &amp;amp;= J_i \alpha_J + K_i \alpha_K + L_i \alpha_L + A_i \delta_A + B_i \delta_B
\end{align}
$$&lt;/p>
&lt;p>The variables ($J_i, K_i, L_i, A_i, B_i$) in front of the $\alpha$s and
$\delta$s have a value of either 0 or 1. Here, they serve as a “switch”
that turns on the relevant $\alpha$ and $\delta$ and turns off the
others in each observation. This is easier to see with the help of the
tables below. Table 3.1 corresponds to Equation (3.1), and Table 3.2
corresponds to Equation (3.2). So, for instance, in row 2 of Table 3.2,
$K$ and $A$ are 1 while the others are 0. This turns on, or picks out,
$\alpha_K$ and $\delta_A$. As such, they would be updated by the model
when it reaches this observation. In row 2 of Table 3.1, $\alpha_K$ and
$\delta_A$ are picked out too, but not by the switches. They are
directly picked out through the &lt;em>K&lt;/em> and &lt;em>A&lt;/em> present in the row.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.1
&lt;/th>
&lt;th>
Table 3.2
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$S$&lt;/th>
&lt;th style="text-align:center">$I$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">J&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">K&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">L&lt;/td>
&lt;td style="text-align:center">B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>The re-expression of Table 3.1 as Table 3.2 by coding the categories
into zeros and ones is known as &lt;strong>dummy coding&lt;/strong>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Why do we need
dummy coding? In short, this is because regression programs do not
“understand” the difference between categorical and continuous
variables. They read only numbers. Dummy coding is essentially
representing categorical variables as continuous ones so that the
program would know how to deal with them. Most programs dummy code for
the users (such as &lt;code>glm()&lt;/code>) if you give them categories. But there are
various ways to dummy code the categories and each of which results in a
different output. The interpretation of the output coefficients depends
on how the categories were coded. This confuses the novice as too much
is happening under the hood.&lt;/p>
&lt;h3 id="coding-models-the-long-route">Coding models: the long route&lt;/h3>
&lt;p>With the concepts of dummy coding in place, let’s code the model. I use
&lt;code>dummy_cols()&lt;/code> from the &lt;code>fastDummies&lt;/code> package to help me with dummy
coding. In the code below, I recode the item and subject variables into
zeros and ones. The result is identical to Table 3.2, except that it now
expands to 30 items and 60 subjects (90 columns in total). I won’t print
out the full dummy-coded data frame to save space. But be sure to take a
look at &lt;code>d_dummy&lt;/code> to see how it corresponds to Table 3.2.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(fastDummies)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>d_dummy &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">dummy_cols&lt;/span>( d, &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>), remove_selected_columns&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">TRUE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">dim&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 1800 91
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">colnames&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] &amp;quot;R&amp;quot; &amp;quot;I_1&amp;quot; &amp;quot;I_2&amp;quot; &amp;quot;I_3&amp;quot; &amp;quot;I_4&amp;quot; &amp;quot;I_5&amp;quot; &amp;quot;I_6&amp;quot; &amp;quot;I_7&amp;quot; &amp;quot;I_8&amp;quot; &amp;quot;I_9&amp;quot;
[11] &amp;quot;I_10&amp;quot; &amp;quot;I_11&amp;quot; &amp;quot;I_12&amp;quot; &amp;quot;I_13&amp;quot; &amp;quot;I_14&amp;quot; &amp;quot;I_15&amp;quot; &amp;quot;I_16&amp;quot; &amp;quot;I_17&amp;quot; &amp;quot;I_18&amp;quot; &amp;quot;I_19&amp;quot;
[21] &amp;quot;I_20&amp;quot; &amp;quot;I_21&amp;quot; &amp;quot;I_22&amp;quot; &amp;quot;I_23&amp;quot; &amp;quot;I_24&amp;quot; &amp;quot;I_25&amp;quot; &amp;quot;I_26&amp;quot; &amp;quot;I_27&amp;quot; &amp;quot;I_28&amp;quot; &amp;quot;I_29&amp;quot;
[31] &amp;quot;I_30&amp;quot; &amp;quot;S_1&amp;quot; &amp;quot;S_2&amp;quot; &amp;quot;S_3&amp;quot; &amp;quot;S_4&amp;quot; &amp;quot;S_5&amp;quot; &amp;quot;S_6&amp;quot; &amp;quot;S_7&amp;quot; &amp;quot;S_8&amp;quot; &amp;quot;S_9&amp;quot;
[41] &amp;quot;S_10&amp;quot; &amp;quot;S_11&amp;quot; &amp;quot;S_12&amp;quot; &amp;quot;S_13&amp;quot; &amp;quot;S_14&amp;quot; &amp;quot;S_15&amp;quot; &amp;quot;S_16&amp;quot; &amp;quot;S_17&amp;quot; &amp;quot;S_18&amp;quot; &amp;quot;S_19&amp;quot;
[51] &amp;quot;S_20&amp;quot; &amp;quot;S_21&amp;quot; &amp;quot;S_22&amp;quot; &amp;quot;S_23&amp;quot; &amp;quot;S_24&amp;quot; &amp;quot;S_25&amp;quot; &amp;quot;S_26&amp;quot; &amp;quot;S_27&amp;quot; &amp;quot;S_28&amp;quot; &amp;quot;S_29&amp;quot;
[61] &amp;quot;S_30&amp;quot; &amp;quot;S_31&amp;quot; &amp;quot;S_32&amp;quot; &amp;quot;S_33&amp;quot; &amp;quot;S_34&amp;quot; &amp;quot;S_35&amp;quot; &amp;quot;S_36&amp;quot; &amp;quot;S_37&amp;quot; &amp;quot;S_38&amp;quot; &amp;quot;S_39&amp;quot;
[71] &amp;quot;S_40&amp;quot; &amp;quot;S_41&amp;quot; &amp;quot;S_42&amp;quot; &amp;quot;S_43&amp;quot; &amp;quot;S_44&amp;quot; &amp;quot;S_45&amp;quot; &amp;quot;S_46&amp;quot; &amp;quot;S_47&amp;quot; &amp;quot;S_48&amp;quot; &amp;quot;S_49&amp;quot;
[81] &amp;quot;S_50&amp;quot; &amp;quot;S_51&amp;quot; &amp;quot;S_52&amp;quot; &amp;quot;S_53&amp;quot; &amp;quot;S_54&amp;quot; &amp;quot;S_55&amp;quot; &amp;quot;S_56&amp;quot; &amp;quot;S_57&amp;quot; &amp;quot;S_58&amp;quot; &amp;quot;S_59&amp;quot;
[91] &amp;quot;S_60&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now, we can fit the model with the dummy-coded data &lt;code>d_dummy&lt;/code>. The first
argument in &lt;code>glm()&lt;/code> specifies the formula in R’s model syntax. Based on
Equation (3.2), we include all the dummy variables in the table. In R,
this means typing out all the variables as
&lt;code>R ~ S_1 + S_2 + ... + S_80 + I_1 + I_2 + ... + I_20&lt;/code>. That’s a lot of
work!&lt;/p>
&lt;p>Luckily, R provides a handy syntax for this. Since we are including all
the variables except the outcome on the right-hand side of the formula,
we can simply type &lt;code>R ~ .&lt;/code>. Here, the dot serves as a placeholder for
all the remaining variables not specified in the formula. We also need a
&lt;code>-1&lt;/code> in front of the dot: &lt;code>R ~ -1 + .&lt;/code>. The &lt;code>-1&lt;/code> tells the model not to
estimate a global intercept, which is done by default&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. We don’t need
a global intercept here since we want all the effects to be presented in
the items and subjects. If a global intercept is estimated, it will
“suck out” what should have been part of the subject/item effects,
rendering the results hard to interpret.&lt;/p>
&lt;p>The last thing to note in &lt;code>glm()&lt;/code> is the &lt;code>family&lt;/code> argument, which
characterizes the type of GLM used. Since we are fitting the data with
logistic regression, we pass &lt;code>binomial(&amp;quot;logit&amp;quot;)&lt;/code> to &lt;code>family&lt;/code>. The GLM
will then adopt the binomial distribution&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> with the logit link to map
the right-hand linear terms to the outcome space.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">summary&lt;/span>(m1)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Call:
glm(formula = R ~ -1 + ., family = binomial(&amp;quot;logit&amp;quot;), data = d_dummy)
Deviance Residuals:
Min 1Q Median 3Q Max
-2.4291 -0.8850 0.2498 0.8978 2.7509
Coefficients: (1 not defined because of singularities)
Estimate Std. Error z value Pr(&amp;gt;|z|)
I_1 1.926e+00 5.422e-01 3.553 0.000381 ***
I_2 1.798e+00 5.339e-01 3.368 0.000758 ***
I_3 1.154e+00 5.049e-01 2.286 0.022267 *
I_4 1.154e+00 5.049e-01 2.286 0.022267 *
I_5 1.351e+00 5.118e-01 2.639 0.008304 **
I_6 9.686e-01 4.997e-01 1.939 0.052559 .
I_7 1.351e+00 5.118e-01 2.639 0.008304 **
I_8 1.060e+00 5.021e-01 2.111 0.034737 *
I_9 9.686e-01 4.997e-01 1.939 0.052559 .
I_10 1.060e+00 5.021e-01 2.111 0.034737 *
I_11 5.368e-01 4.920e-01 1.091 0.275173
I_12 3.715e-01 4.905e-01 0.757 0.448848
I_13 -3.712e-02 4.901e-01 -0.076 0.939634
I_14 -3.712e-02 4.901e-01 -0.076 0.939634
I_15 1.263e-01 4.897e-01 0.258 0.796423
I_16 2.079e-01 4.898e-01 0.424 0.671244
I_17 -2.857e-01 4.922e-01 -0.581 0.561549
I_18 1.263e-01 4.897e-01 0.258 0.796423
I_19 -3.712e-02 4.901e-01 -0.076 0.939634
I_20 4.473e-02 4.898e-01 0.091 0.927249
I_21 -7.217e-01 5.000e-01 -1.443 0.148885
I_22 -1.194e-01 4.906e-01 -0.243 0.807787
I_23 -6.313e-01 4.979e-01 -1.268 0.204796
I_24 -7.217e-01 5.000e-01 -1.443 0.148885
I_25 -7.217e-01 5.000e-01 -1.443 0.148885
I_26 -7.217e-01 5.000e-01 -1.443 0.148885
I_27 -1.007e+00 5.082e-01 -1.981 0.047587 *
I_28 -1.212e+00 5.159e-01 -2.350 0.018770 *
I_29 -1.212e+00 5.159e-01 -2.350 0.018770 *
I_30 -1.961e+00 5.585e-01 -3.511 0.000447 ***
S_1 -1.800e+00 6.322e-01 -2.847 0.004415 **
S_2 1.732e+00 6.580e-01 2.632 0.008491 **
S_3 -1.575e+00 6.143e-01 -2.564 0.010355 *
S_4 -8.194e-01 5.775e-01 -1.419 0.155962
S_5 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_6 3.276e-01 5.732e-01 0.572 0.567650
S_7 4.966e-01 5.774e-01 0.860 0.389744
S_8 -3.227e-01 5.688e-01 -0.567 0.570509
S_9 -4.853e-01 5.705e-01 -0.851 0.394979
S_10 4.966e-01 5.774e-01 0.860 0.389744
S_11 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_12 -1.369e+00 6.010e-01 -2.277 0.022758 *
S_13 -8.194e-01 5.775e-01 -1.419 0.155962
S_14 -1.613e-01 5.682e-01 -0.284 0.776469
S_15 -1.613e-01 5.682e-01 -0.284 0.776469
S_16 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_17 1.253e+00 6.146e-01 2.039 0.041452 *
S_18 3.276e-01 5.732e-01 0.572 0.567650
S_19 1.046e+00 6.010e-01 1.741 0.081706 .
S_20 -8.194e-01 5.775e-01 -1.419 0.155962
S_21 4.966e-01 5.774e-01 0.860 0.389744
S_22 2.856e+00 8.565e-01 3.334 0.000855 ***
S_23 1.479e+00 6.328e-01 2.337 0.019424 *
S_24 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_25 -8.194e-01 5.775e-01 -1.419 0.155962
S_26 1.625e-01 5.704e-01 0.285 0.775661
S_27 -4.853e-01 5.705e-01 -0.851 0.394979
S_28 -3.227e-01 5.688e-01 -0.567 0.570509
S_29 1.969e-15 5.687e-01 0.000 1.000000
S_30 6.712e-01 5.831e-01 1.151 0.249706
S_31 6.712e-01 5.831e-01 1.151 0.249706
S_32 3.618e+00 1.111e+00 3.256 0.001131 **
S_33 -1.613e-01 5.682e-01 -0.284 0.776469
S_34 -8.194e-01 5.775e-01 -1.419 0.155962
S_35 -4.853e-01 5.705e-01 -0.851 0.394979
S_36 -4.853e-01 5.705e-01 -0.851 0.394979
S_37 1.046e+00 6.010e-01 1.741 0.081706 .
S_38 -6.504e-01 5.734e-01 -1.134 0.256656
S_39 1.046e+00 6.010e-01 1.741 0.081706 .
S_40 -1.575e+00 6.143e-01 -2.564 0.010355 *
S_41 1.625e-01 5.704e-01 0.285 0.775661
S_42 -6.504e-01 5.734e-01 -1.134 0.256656
S_43 1.253e+00 6.146e-01 2.039 0.041452 *
S_44 -1.800e+00 6.322e-01 -2.847 0.004415 **
S_45 -3.227e-01 5.688e-01 -0.567 0.570509
S_46 4.966e-01 5.774e-01 0.860 0.389744
S_47 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_48 3.276e-01 5.732e-01 0.572 0.567650
S_49 8.536e-01 5.908e-01 1.445 0.148546
S_50 -3.227e-01 5.688e-01 -0.567 0.570509
S_51 -4.853e-01 5.705e-01 -0.851 0.394979
S_52 -1.613e-01 5.682e-01 -0.284 0.776469
S_53 6.712e-01 5.831e-01 1.151 0.249706
S_54 2.856e+00 8.565e-01 3.334 0.000855 ***
S_55 -6.504e-01 5.734e-01 -1.134 0.256656
S_56 6.712e-01 5.831e-01 1.151 0.249706
S_57 1.253e+00 6.146e-01 2.039 0.041452 *
S_58 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_59 -4.853e-01 5.705e-01 -0.851 0.394979
S_60 NA NA NA NA
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 2495.3 on 1800 degrees of freedom
Residual deviance: 1927.2 on 1711 degrees of freedom
AIC: 2105.2
Number of Fisher Scoring iterations: 6
&lt;/code>&lt;/pre>
&lt;p>Take a look at the output. Something’s strange. Since there are 30 items
and 60 subjects in the data, we expect the model to return 90
coefficients, one for each subject/item effect. However, the last
coefficient, &lt;code>S_60&lt;/code> in this case, is &lt;code>NA&lt;/code>. The model does not
estimate this coefficient. Why?&lt;/p>
&lt;h4 id="identifiability">Identifiability&lt;/h4>
&lt;p>The reason is that there are &lt;strong>infinite&lt;/strong> sets of parameter combinations
that generate the same probabilities underlying our data. Thus, the
model is unable to work in reverse to infer a unique set of coefficients
from the data. To deal with this issue, R silently sets a constraint on
the parameters: it simply drops one of the parameters and estimates the
rest. When this is done, the remaining parameters become
&lt;a href="https://en.wikipedia.org/wiki/Identifiability">identifiable&lt;/a>, and the
model would be able to estimate them.&lt;/p>
&lt;p>But still, where did the infinity come from? Didn’t we simulate the
data? We didn’t introduce infinity, did we?&lt;/p>
&lt;p>We &lt;em>&lt;strong>did&lt;/strong>&lt;/em> actually, in silence. Recall that the probability of
success on an item is determined by the difference between ability and
difficulty. Since it is the &lt;em>&lt;strong>difference&lt;/strong>&lt;/em> that matters, there could
be an infinite number of ability and difficulty pairs that yield the
same difference. By adding any common value to a pair, we get a new pair
of ability and difficulty that yields the same probability. The code
below demonstrates this. Here, I shift the ability and difficulty levels
by a common value &lt;code>s&lt;/code>. The resulting probabilities should be identical
before and after the shift (except for a tiny floating point
imprecision). You can play with the code below by changing the value of
&lt;code>s&lt;/code>. Identical results should always be yielded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Shift A/D by a common factor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>A2 &lt;span style="color:#f92672">=&lt;/span> A &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>D2 &lt;span style="color:#f92672">=&lt;/span> D &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>p1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities before shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A2[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D2[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities after shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sum&lt;/span>( &lt;span style="color:#a6e22e">abs&lt;/span>(p1 &lt;span style="color:#f92672">-&lt;/span> p2) ) &lt;span style="color:#75715e"># Should be extremely close to zero&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 9.456325e-14
&lt;/code>&lt;/pre>
&lt;p>The way R deals with this issue of identifiability is not preferable
since we want to recover the parameters in our simulation (i.e., the set
of parameters without the shift). To get around R’s default treatment,
we have to impose constraints on the parameters ourselves.&lt;/p>
&lt;p>Recall that subject abilities are generated according to a standard
normal distribution in the simulation. Since the standard normal has a
mean of zero, the &lt;strong>expectation of the sum of subject abilities is
&lt;em>zero&lt;/em>&lt;/strong>. We can use this expectation as a constraint to the parameters
by constraining the subject effects to &lt;strong>sum to zero&lt;/strong>. This constraint,
however, &lt;strong>does not&lt;/strong> scale the model’s estimates to perfectly match the
true parameters since the true subject abilities never exactly sum to
zero in a single run of the simulation. However, the relative scale
would be close enough for the simulated parameters to be comparable to
those recovered by the model. Later in the next post, when the
&lt;strong>generalized linear mixed model&lt;/strong> is introduced, you will see that
there is no need to impose such a constraint. The constraint is
naturally included through the model’s assumptions. The estimated
subject effects then, do not need to sum to zero. Subject effects would
be assumed to result from a normal distribution with a mean of zero.&lt;/p>
&lt;p>Through dummy coding, we can impose the sum-to-zero constraint on the
subject effects. I illustrate this with the example previously presented
in Table 3.2, where there are only 3 subjects and 2 items. Table 3.3 is
re-coded from Table 3.2, in which the sum-to-zero constraint is imposed.&lt;/p>
&lt;p>The sum-to-zero constraint is imposed by dropping one of the subjects
and coding the remaining as &lt;code>-1&lt;/code> for all the observations where the
dropped subject is originally coded as &lt;code>1&lt;/code>. This is shown in Table 3.3,
where I drop subject $L$ (hence the &lt;code>-&lt;/code> in the column) and code the 3rd
row of $J$ and $K$ as &lt;code>-1&lt;/code>.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.2
&lt;/th>
&lt;th>
Table 3.3
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>With the coding scheme in Table 3.3, the estimated effect of subject $L$
can be reconstructed from the remaining subject effects returned by the
model. Since the sum of all subject effects is zero, the effect of
subject $L$ will be the negative of the others’ sum. This might seem a
bit confusing. But notice that the sum-to-zero constraint
&lt;strong>simultaneously applies to all effects in the variable&lt;/strong>. Once the
effect of the dropped category is reconstructed, each effect will also
be the negative sum of the remaining effects.&lt;/p>
&lt;p>Let’s impose this constraint on the data with code. Here, I will drop
the first subject &lt;code>S_1&lt;/code>. You can choose any subject you like to drop,
and the result will be identical. The code from line 3 to 5 below pick
outs the rows where &lt;code>S_1&lt;/code> is coded as &lt;code>1&lt;/code> and recode them as &lt;code>-1&lt;/code> on all
the subject columns. After the re-coding, the final line of code then
drops &lt;code>S_1&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>d_dummy2 &lt;span style="color:#f92672">=&lt;/span> d_dummy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>toDrop &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;S_1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>allCategories &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">startsWith&lt;/span>( &lt;span style="color:#a6e22e">names&lt;/span>(d_dummy2), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>idx_recode &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">which&lt;/span>( d_dummy2[[toDrop]] &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>d_dummy2[idx_recode, allCategories] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>d_dummy2[[toDrop]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NULL&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, let’s refit the model with this constraint-coded data. I simply
replace &lt;code>d_dummy&lt;/code> with &lt;code>d_dummy2&lt;/code> in the &lt;code>data&lt;/code> argument. Everything
else is the same.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy2, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># summary(m1.1)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you print out the coefficients of the fitted model, you will see that
the result is what we expected. The model returns 89 coefficients, which
match the number of the predictor variables we passed in. No coefficient
is dropped. We already dropped it for the model. And since we dropped
the predictor in a principled way, we know how to reconstruct it. The
effect of the dropped &lt;code>S_1&lt;/code> will be the negative sum of the remaining.
This is shown in the code below, which reconstructs all the item/subject
effects from the model’s coefficients.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>item_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct S_1 from the remaining subject effects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff), subj_eff )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now plot the estimated effects against the true parameter values
from the simulation. The figures below plot the estimated effects on the
x-axis and the true parameters of the y-axis. The dashed line has a
slope of 1 without an intercept. This line indicates perfect matches
between the truth and the estimates. Notice that for the figure on the
right, I reverse the signs of the item effects to match the scale of
item difficulty. This is necessary since $D$ is subtracted from $A$ in
the simulation. In other words, the effect of difficulty assumed by the
1PL model is &lt;strong>negative&lt;/strong>: the larger the difficulty, the less
probability of success on the item. However, &lt;code>glm()&lt;/code> allows only
additive effects. The effects in the model are summed together to yield
predictions. Hence, the item effects estimated by &lt;code>glm()&lt;/code> will be the
negative of those assumed by the 1PL model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( subj_eff, A, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>item_eff, D, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="two-column">
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-1.svg"
data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-2.svg"
data-fig-align="center" />&lt;/p>
&lt;/div>
&lt;p>As seen in both figures, the dots scatter around the lines quite
randomly, which indicates that the model does recover the parameters. To
have a clearer view of the estimates’ accuracy, let me plot some more.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set figure margins&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1.6&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>true_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(D, A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>est_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>item_eff, subj_eff)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_param &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">+&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>cols &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, n_item), &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">4&lt;/span>, n_subj) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>y_lim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">max&lt;/span>( &lt;span style="color:#a6e22e">abs&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>(true_eff, est_eff)) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>y_lim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>y_lim&lt;span style="color:#ae81ff">-.1&lt;/span>, y_lim&lt;span style="color:#ae81ff">+.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, ylim&lt;span style="color:#f92672">=&lt;/span>y_lim, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, n_param&lt;span style="color:#ae81ff">+1&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Effect&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#ae81ff">+0.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( true_eff, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_eff, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_param)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i, i), &lt;span style="color:#a6e22e">c&lt;/span>(true_eff[i], est_eff[i]), col&lt;span style="color:#f92672">=&lt;/span>cols[i] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mtext&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Items&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Subjects&amp;#34;&lt;/span>), at&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">61&lt;/span>), padj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/eff-est-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>In the plot above, I overlay the estimated effects onto the true
parameters. The dots are the true parameters and the circles are the
model’s estimates. The vertical lines connecting the dots and the
circles show the distances between the truth and the estimates. It is
obvious from the plot that, compared to item difficulties, subject
abilities are harder to estimate, as the distances to the truth are in
general larger for subject estimates. This is apparent in hindsight, as
each item is taken by 60 subjects whereas each subject only takes 30
items. Hence, the estimation for the items is more accurate, compared to
the subjects, as there are more data to estimate each.&lt;/p>
&lt;p>The effect of manipulating the number of subjects and items is revealed
in the plot below. Here, I refit the model with data that have the
subject and the item numbers flipped. The subject abilities are now
estimated more accurately than the item difficulties. You can experiment
with this to see how the estimation accuracy changes with different
combinations of subject/item numbers. The functions &lt;code>sim_data()&lt;/code> and
&lt;code>plot_estimate()&lt;/code> in
&lt;a href="https://github.com/liao961120/stom/blob/main/inst/blog/irt/estimate-acc.R">&lt;code>estimate-acc.R&lt;/code>&lt;/a>
can help you with this.&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="coding-models-the-easy-route">Coding models: the easy route&lt;/h3>
&lt;p>We have gone through a long route, along which we have learned a lot.
Now, you are qualified to take the easy route: to use R’s handy function
for dummy coding. Note that this route won’t be easy at all if you never
went through the longer one. Rather, confusion is all you will get, and
you will have no confidence in the model you coded. Simple code is
simple only for those who are well-trained. So now, let’s fit the model
again. This time, we take the highway.&lt;/p>
&lt;p>The trick for controlling how the model functions dummy code the
categorical variables is to use the &lt;code>contrasts()&lt;/code> function to set up the
preferred coding scheme. In the code below, I pass the number of the
categories in $S$ (i.e., &lt;code>n_subj&lt;/code>) to &lt;code>contr.sum()&lt;/code>, which is a helper
function that codes the subjects in the exact same way as we did in
Table 3.3 (execute &lt;code>contr.sum(3)&lt;/code> and you will see a table that
corresponds exactly to Table 3.3).&lt;/p>
&lt;p>After the coding scheme is set, we can express categorical predictors in
the model formula directly. Everything else is the same except the last
line. Previously, I demonstrated dummy coding by dropping the first
subject in $S$. Here, &lt;code>contr.sum()&lt;/code> drops the last subject by default.
Thus, the code for constructing the dropped subject is slightly
different here.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># Drop the last S and impose sum-to-zero constraint on S&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>S ) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> I &lt;span style="color:#f92672">+&lt;/span> S, data&lt;span style="color:#f92672">=&lt;/span>dat, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>item_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( subj_eff.m1.2, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff.m1.2) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we have the estimated effects from the model, let’s check the
results. The figure below plots the current estimates (&lt;code>m1.2&lt;/code>) against
previous ones (&lt;code>m1.1&lt;/code>). The estimates from the two models agree, which
confirms that the second model is correctly coded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>est_m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff, subj_eff )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>est_m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff.m1.2, subj_eff.m1.2 )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.2&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.1&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_m1.2, est_m1.1, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-13-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>This post is lengthy, but not because it is hard. Rather, the concepts
presented are fairly simple. The post is lengthy because we got used to
texts that hide details from the readers. The text here does the
opposite: it presents all the &lt;strong>necessary details&lt;/strong> to get the
statistical model working, without confusion. People often assume that
hidden details are trivial. But more often, it is just because writers
are too lazy to present the details. Statistics is hard partly because
it is loaded with details that are hidden and ignored. When details get
ignored long enough, they accumulate to become entangled and
uncrackable. Coding, again, is here to help. It dissolves the fuzziness
that otherwise accumulates and hinders understanding.&lt;/p>
&lt;p>In &lt;a href="https://yongfu.name/irt3">Part 3&lt;/a>, we move on to &lt;strong>Generalized Linear Mixed Models&lt;/strong>,
which are extensions to GLMs that improve estimation and efficiency by
harnessing the information from common group memberships in the data. We
will use the same data, and the text would be much shorter, I hope.
Seeya!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I use &lt;em>dummy coding&lt;/em> as a general umbrella term to cover all
systems for coding categorical variables. In R, what is known as
“treatment coding” (&lt;code>contr.treatment&lt;/code>) is sometimes called “dummy
coding” by others. Here, I follow R’s convention. When “dummy
coding” is used, I always refer to the general sense of re-coding
categories as numbers (not necessarily zeros and ones).&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This default behavior of estimating a global intercept makes sense
in the context of continuous predictors, such as the simple linear
model shown below. In this case, we can succinctly express the
formula as &lt;code>y ~ x&lt;/code> in R’s model syntax. The estimate of the global
intercept $\alpha$ would be given as the intercept coefficient in
the model output.&lt;/p>
&lt;p>$$
\begin{aligned}
y_i &amp;amp;\sim Normal(\mu_i, \sigma) \\
\mu_i &amp;amp;= \alpha + \beta x_i
\end{aligned}
$$&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>I haven’t mentioned the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial
distribution&lt;/a>
before. The binomial distribution is the extension of the Bernoulli
distribution to $n$ independent trials. So if you repeat the
Bernoulli process $n$ times and sum the outcomes, say, you toss the
coin $n=10$ times and record the number of heads observed, the
distribution of outcomes would follow a binomial distribution with
parameters $n$ and $p$. So the Bernoulli distribution is simply a
special case of the binomial distribution where $n=1$.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (1/4)</title><link>https://yongfu.name/2023/02/25/irt1/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/25/irt1/</guid><description>&lt;figure>
&lt;img src="irt.jpg" style="width:70.0%"
alt="Ok, so these are the item characteristic curves. What then?" />
&lt;figcaption aria-hidden="true">&lt;em>Ok, so these are the &lt;a
href="https://www.researchgate.net/figure/Item-characteristic-curves-Item-Response-Theory-IRT-1PL-model_fig1_342560715">item
characteristic curves&lt;/a>. What then?&lt;/em>&lt;/figcaption>
&lt;/figure>
&lt;h2 id="mysterious-item-response-theory">Mysterious Item Response Theory&lt;/h2>
&lt;p>&lt;strong>Item response theory is &lt;em>mysterious&lt;/em> and intimidating to students.&lt;/strong>
It is mysterious in the way it is presented in textbooks, at least in
introductory ones. The text often starts with an ambitious conceptual
introduction to IRT, which most students would be able to follow, but
with some confusion. Curious students might bear with the confusion and
expect it to resolve in the following text, only to find themselves
disappointed. At the point where the underlying statistical model should
be further elaborated, the text abruptly stops and tries to convince the
readers to trust the results from black-box IRT software packages.&lt;/p>
&lt;p>It isn’t that I have trust issues with black-box software, and I also
agree that certain details of IRT model estimation should be hidden from
the readers. The problem is that there’s a huge gap here, between where
textbooks stopped explaining and where the confusing details of
statistics should be hidden. Hence, students would be tricked into
believing that they have a &lt;em>sufficient degree of understanding&lt;/em>, but in
reality, it’s just blind faith.&lt;/p>
&lt;p>A sufficient degree of understanding should allow the student to deploy
the learned skills to new situations. Therefore, a sufficient degree of
understanding of IRT models should allow students to extend and apply
the models to analyses of, for instance, &lt;a href="https://en.wikipedia.org/wiki/Differential_item_functioning">differential item functioning
(DIF)&lt;/a> or
&lt;strong>differential rater functioning (DRF)&lt;/strong>.&lt;/p>
&lt;p>I’m arguing here that there is a basic granularity of understanding,
somewhat similar to the concept of &lt;a href="https://en.wikipedia.org/wiki/Basic_category">basic-level
category&lt;/a>, that when
reached, allows a student to smoothly adapt the learned skills to a wide
variety of situations, modifying and extending the skills on demand. And
I believe that item response theory is &lt;em>&lt;strong>too hard&lt;/strong>&lt;/em> for a student to
learn and reach this basic level of understanding, given its
conventional presentation and historical development&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There is still hope however, thanks to the development of a very general
set of statistical models known as the &lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Models
(GLM)&lt;/a>. Item
response models could be understood in terms of the GLM and its
extensions
(&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_mixed_model">GLMM&lt;/a>
and non-linear form of the GLM/GLMM). To be too particular about the
details, the results from IRT software packages and the GLMs/GLMMs would
be very similar but not identical, since they utilize different
estimation methods. The strengths of the GLM, however, lie in its
conceptual simplicity and extensibility. Through GLM, IRT and other
models such as the T-test, ANOVA, and linear regression, are all placed
together into the same conceptual framework. Furthermore, software
packages implementing GLMs are widely available. Users can thus
experiment with them—simulate a set of data based on known parameters,
construct the model and feed it the data, and see if the fitted model
correctly recovers the parameters. This technique of learning statistics
is probably the only effective way for students to understand &lt;em>&lt;strong>a
mysterious statistical model&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>In this series of posts, I will walk you through the path of
understanding item response theory, with the help of simulations and
generalized linear models. No need to worry if you don’t know GLMs yet.
We have another ally—&lt;a href="https://www.r-project.org">R&lt;/a>, in which we will be
simulating artificial data and fitting statistical models along the way.
Although it might seem intimidating at first, coding simulations and
models in fact provides scaffolding for learning. When feeling unsure or
confused, you can always resort to these simulation-based experiments to
resolve the issues at hand. In this very first post, I will start by
teaching you &lt;em>&lt;strong>simulations&lt;/strong>&lt;/em>.&lt;/p>
&lt;h2 id="just-enough-theory-to-get-started">Just Enough Theory to Get Started&lt;/h2>
&lt;p>Jargons aside, the concept behind item response theory is fairly simple.
Consider the case where 80 testees are taking a 20-item English
proficiency test. Under this situation, what are the &lt;em>&lt;strong>factors&lt;/strong>&lt;/em> that
influence whether an item is correctly solved by a testee?
Straightforward right? If an item is easy and if a testee is proficient
in English, he/she would probably get the item correct. Here, &lt;em>&lt;strong>two
factors jointly influence the result&lt;/strong>&lt;/em>:&lt;/p>
&lt;ol>
&lt;li>how difficult (or easy) the item is&lt;/li>
&lt;li>the English ability of the testee&lt;/li>
&lt;/ol>
&lt;p>We can express these variables and the relations between them in the
graphs below. Let’s focus on the left one first. Here, $A$ represents
the &lt;strong>ability&lt;/strong> of the testee, $D$ represents the &lt;strong>difficulty&lt;/strong> of the
item, and $R$ represents the &lt;strong>item response&lt;/strong>, or &lt;strong>score&lt;/strong> on the
item. A response for an item is coded as &lt;code>1&lt;/code> ($R=1$) if it is solved
correctly. Otherwise, it is coded as &lt;code>0&lt;/code> ($R=0$). The arrows $A \rightarrow R$
and $D \rightarrow R$ indicate the direction of influence. The arrows enter $R$
since item difficulty and testee ability influence the score on the item
(not the other way around). $A$ and $D$ are drawn as a circled node to
indicate that they are &lt;strong>unobserved&lt;/strong> (or &lt;strong>latent&lt;/strong>, if you prefer a
fancier term), whereas uncircled nodes represent directly observable
variables (i.e., stuff that gets recorded during data collection). This
graphical representation of the variables and their relationships is
known as a &lt;a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed Acyclic Graph
(DAG)&lt;/a>.&lt;/p>
&lt;div class="goat svg-container ">
&lt;svg
xmlns="http://www.w3.org/2000/svg"
font-family="Menlo,Lucida Console,monospace"
viewBox="0 0 352 137"
>
&lt;g transform='translate(8,16)'>
&lt;path d='M 272,16 L 304,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 152,16 L 152,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 L 40,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 216,80 L 232,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 72,48 L 88,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 264,48 L 280,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='52.000000,48.000000 40.000000,42.400002 40.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 40.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='84.000000,48.000000 72.000000,42.400002 72.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 72.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 232,48 L 240,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='250.000000,48.000000 238.000000,42.400002 238.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 232.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 256,32 L 264,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='282.000000,48.000000 270.000000,42.400002 270.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 264.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='312.000000,16.000000 300.000000,10.400000 300.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 304.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 248,0 A 16,16 0 0,0 232,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 248,0 A 16,16 0 0,1 264,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 232,16 A 16,16 0 0,0 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 264,16 A 16,16 0 0,1 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 16,80 A 16,16 0 0,0 0,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 16,80 A 16,16 0 0,1 32,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 96,80 A 16,16 0 0,0 80,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 96,80 A 16,16 0 0,1 112,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 208,80 A 16,16 0 0,0 192,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 208,80 A 16,16 0 0,1 224,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 288,80 A 16,16 0 0,0 272,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 288,80 A 16,16 0 0,1 304,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 0,96 A 16,16 0 0,0 16,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 32,96 A 16,16 0 0,1 16,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,96 A 16,16 0 0,0 96,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 112,96 A 16,16 0 0,1 96,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 192,96 A 16,16 0 0,0 208,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 224,96 A 16,16 0 0,1 208,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,96 A 16,16 0 0,0 288,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 304,96 A 16,16 0 0,1 288,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;text text-anchor='middle' x='16' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='56' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;text text-anchor='middle' x='96' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='208' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='248' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='288' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='328' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;/g>
&lt;/svg>
&lt;/div>
&lt;p>The DAGs laid out here represent the concept behind the simplest kind of
item response models, known as the &lt;strong>1-parameter logistic (1PL) model&lt;/strong>
(or the Rasch Model). In more formal terms, this model posits that the
&lt;strong>probability&lt;/strong> of correctly answering an item is determined by the
&lt;strong>difference&lt;/strong> between testee ability and item difficulty. So a more
precise DAG representation for this model would be the one shown on the
right above. Here, $P$ is the probability of correctly answering the
item, which cannot be directly observed. However, $P$ directly
influences the item score $R$, hence the arrow $P \rightarrow R$.&lt;/p>
&lt;p>Believe it or not, the things we have learned so far could get us
started. So let’s simulate some data, based on what we’ve learned about
item response theory!&lt;/p>
&lt;h2 id="simulating-item-responses">Simulating Item Responses&lt;/h2>
&lt;figure>
&lt;img src="tenet2.gif" style="width:85.0%"
alt="Simulation is playing god in a small world. Similar to model fitting, but in reverse direction." />
&lt;figcaption aria-hidden="true">Simulation is &lt;em>playing god&lt;/em> in a
small world. Similar to model fitting, but in &lt;em>reverse&lt;/em>
direction.&lt;/figcaption>
&lt;/figure>
&lt;p>Consider the scenario where 3 students (Rob, Tom, and Joe) took a math
test with 2 items (A and B). Since we play gods during simulations, we
know the math ability of the students and the difficulty of the items.
These ability/difficulty levels can range from positive to negative
numbers, unbounded. Larger numbers indicate higher levels of
difficulty/ability. In addition, the levels of difficulty and ability
sit on a common scale and hence could be directly compared. Also, each
student responds to every item, so we get responses from all 6 (3x2)
combinations of students and items. Let’s code this in R below. The
function &lt;code>expand.grid()&lt;/code> would pair up the 6 combinations for us.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( A&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.4&lt;/span>, B&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> ) &lt;span style="color:#75715e"># Difficulty of item&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( R&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, T&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>, J&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">-0.4&lt;/span> ) &lt;span style="color:#75715e"># Ability of student (R:Rob, T:Tom, J:Joe)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B&amp;#34;&lt;/span> ), &lt;span style="color:#75715e"># Item index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span> T &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;R&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;T&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;J&amp;#34;&lt;/span> ) &lt;span style="color:#75715e"># Testee index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 2
I T
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;
1 A R
2 B R
3 A T
4 B T
5 A J
6 B J
&lt;/code>&lt;/pre>
&lt;p>After having all possible combinations of the students and the items, we
could collect the values of student ability and item difficulty into the
data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">=&lt;/span> A[ dat&lt;span style="color:#f92672">$&lt;/span>T ] &lt;span style="color:#75715e"># map ability to df by testee index T&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>D &lt;span style="color:#f92672">=&lt;/span> D[ dat&lt;span style="color:#f92672">$&lt;/span>I ] &lt;span style="color:#75715e"># map difficulty to df by item index I&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 4
I T A D
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4
2 B R 0.5 0.1
3 A T 0.1 0.4
4 B T 0.1 0.1
5 A J -0.4 0.4
6 B J -0.4 0.1
&lt;/code>&lt;/pre>
&lt;p>Now, we’ve got all the data needed for simulation, the only thing left
is to precisely lay out the &lt;strong>rules for generating the response data
$R$&lt;/strong>—the scores (zeros and ones) on the items solved by the students.
We are two steps away.&lt;/p>
&lt;h3 id="generating-probabilities">Generating Probabilities&lt;/h3>
&lt;p>When IRT is introduced in the previous section, I mention that the
probability of successfully solving an item is determined by the
&lt;strong>difference between testee ability and item difficulty&lt;/strong>. It is
straightforward to get this difference: simply subtract $D$ from $A$ in
the data. This would give us a new variable $\mu$. I save the values of
$\mu$ to column &lt;code>Mu&lt;/code> in the data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>Mu &lt;span style="color:#f92672">=&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">-&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>D
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 5
I T A D Mu
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4 0.1
2 B R 0.5 0.1 0.4
3 A T 0.1 0.4 -0.3
4 B T 0.1 0.1 0
5 A J -0.4 0.4 -0.8
6 B J -0.4 0.1 -0.5
&lt;/code>&lt;/pre>
&lt;p>From the way $\mu$ is calculated ($A$ - $D$), we can see that, for a
particular observation, if $\mu$ is positive and large, the testee’s
ability will be much greater than the item’s difficulty, and he would
probably succeed on this item. On the other hand, if $\mu$ is negative
and small, the item difficulty would be much greater in this case, and
the testee would likely fail on this item. Hence, $\mu$ should be
directly related to probability, in that $\mu$ of large values result in
high probabilities of success on the items, whereas $\mu$ of small
values result in low probabilities of success. But how exactly is $\mu$
linked to probability? How can we map $\mu$ to probability in a
principled manner? The solution is to take advantage of the &lt;a href="https://en.wikipedia.org/wiki/Logistic_function">logistic
function&lt;/a>.&lt;/p>
&lt;p>$$
logistic( x ) = \frac{ 1 }{ 1 + e^{-x} }
$$&lt;/p>
&lt;p>The &lt;em>&lt;strong>logistic&lt;/strong>&lt;/em> is a function that maps a real number $x$ to a
probability $p$. In other words, the logistic function transforms the
input $x$ and constrains it to a value between zero and one. Note that
the transformation is &lt;strong>monotonic increasing&lt;/strong>, meaning that a smaller
$x$ would be mapped onto a smaller $p$, and a larger $x$ would be mapped
onto a larger $p$. The ranks of the values before and after the
transformation stay the same. To have a feel of what the logistic
function does, let’s transform some values with the logistic.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set plot margins # (b, l, t, r)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># Outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> ( &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-5&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( x )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( x, p )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-4-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>As the plot shows, the logistic transformation results in an S-shaped
curve. Since the transformed values (p) are bounded by 0 and 1, extreme
values on the poles of the x-axis would be “squeezed” after the
transformation. Real numbers with absolute values greater than 4, after
transformations, would have probabilities very close to the boundaries.&lt;/p>
&lt;h4 id="less-math-less-confusion">Less Math, Less Confusion&lt;/h4>
&lt;p>For many students, the mathematical form of the logistic function leads
to confusion. Staring at the math symbols hardly enables one to arrive
at any insightful interpretation of the logistic. A suggestion here is
to let go of the search for such an interpretation. The logistic
function is introduced not because it is loaded with some crucial
mathematical or statistical meaning. Instead, it is used here solely for
a practical reason: to monotonically map real numbers to probabilities.
You may well use another function here to achieve the same purpose
(e.g., the &lt;strong>cumulative distribution function&lt;/strong> of the standard normal).&lt;/p>
&lt;h3 id="generating-responses">Generating Responses&lt;/h3>
&lt;p>We have gone all the way from ability/difficulty levels to the
probabilities of success on the items. Since we cannot directly observe
probabilities in the real world, the final step is to link these
probabilities to observable outcomes. In the case here, the outcomes are
simply item responses of zeros and ones. How do we map probabilities to
zeros and ones? Coin flips, or &lt;a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distributions&lt;/a>,
will get us there.&lt;/p>
&lt;p>Every time a coin is flipped, either a tail or a head is observed. The
Bernoulli distribution is just a fancy way of describing this process.
Assume that we record tails as &lt;code>0&lt;/code>s and heads as &lt;code>1&lt;/code>s, and suppose that
the probability $p$ of observing a head equals 0.75 (since the coin is
imbalanced in some way that the head is more likely observed and we know
it somehow). Then, the distribution of the outcomes (zero and one) will
be a Bernoulli distribution with parameter $P=0.75$. In graphical terms,
the distribution is just two bars.&lt;/p>
&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-5-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>We’ve got all we need by now. Let’s construct the remaining columns to
complete this simulation. In the code below, I compute the probabilities
(&lt;code>P&lt;/code>) from column &lt;code>Mu&lt;/code>. Column &lt;code>P&lt;/code> could then generate column &lt;code>R&lt;/code>, the
item responses, through the Bernoulli distribution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">13&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>Mu )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>P ) &lt;span style="color:#75715e"># Generate 0/1 from Bernoulli distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 7
I T A D Mu P R
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 A R 0.5 0.4 0.1 0.525 0
2 B R 0.5 0.1 0.4 0.599 1
3 A T 0.1 0.4 -0.3 0.426 0
4 B T 0.1 0.1 0 0.5 0
5 A J -0.4 0.4 -0.8 0.310 1
6 B J -0.4 0.1 -0.5 0.378 0
&lt;/code>&lt;/pre>
&lt;p>Now, we have a complete table of simulated item responses. A few things
to notice here. First, look at the fourth row of the data frame, where
the response of testee T (Tom) on item B is recorded. Column &lt;code>Mu&lt;/code> has a
value of zero since Tom’s ability level is identical to the difficulty
of item B. What does it mean to be “identical”? “Identical” implies that
Tom is neither more likely to succeed nor to fail on item B. Hence, you
can see that Tom has a 50% of getting item B correct in the $P$ column.
This is how the ability/difficulty levels and $\mu$ are interpreted.
They are on an abstract scale of real numbers. We need to convert them
to probabilities to make sense of them.&lt;/p>
&lt;p>The second thing to notice is column &lt;code>R&lt;/code>. This is the only column that
has &lt;em>&lt;strong>randomness&lt;/strong>&lt;/em> introduced. Every run of the simulation would
likely give different values of $R$ (unless a random seed is set, or the
&lt;code>P&lt;/code> column consists solely of zeros and ones). The outcomes are not
guaranteed, probability is at work.&lt;/p>
&lt;p>The presence of such randomness is the gist of simulations and
statistical models. We add uncertainty to the simulation, mimicking the
real world, to know that in the presence of such uncertainty, would it
still be possible to discover targets of interest with a statistical
model. Randomness, however, poses some challenges for coding. We need to
equip ourselves for those challenges.&lt;/p>
&lt;h2 id="coding-randomness">Coding Randomness&lt;/h2>
&lt;p>Randomness is inherent in simulations and statistical models, so it is
impossible to run away from it. It is everywhere. The problem with
randomness is that it introduces uncertainty in the outcome produced.
Thus, it would be hard to spot any errors just by &lt;strong>eyeballing the
results&lt;/strong>.&lt;/p>
&lt;p>Take &lt;code>rbern()&lt;/code> for instance. Given a parameter $P=0.5$, we can
repeatedly run &lt;code>rbern(0.5)&lt;/code> a couple of times to produce zeros and ones.
But these zeros and ones cannot tell us whether &lt;code>rbern(0.5)&lt;/code> is working
properly. &lt;code>rbern(0.5)&lt;/code> might be broken somehow, and instead generates
the ones with, say, $P=0.53$.&lt;/p>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large
numbers&lt;/a> can help us
here. Since &lt;code>rbern()&lt;/code> generates ones with probability $P$, if we run
&lt;code>rbern()&lt;/code> many times, the proportion of the ones in the outcomes should
converge to $P$. To achieve this, take a look at the second argument &lt;code>n&lt;/code>
in &lt;code>rbern()&lt;/code>, which is set here to repeatedly generate outcomes ten
thousand times. You can increase &lt;code>n&lt;/code> to see if the result comes even
closer to $0.5$.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> ) &lt;span style="color:#75715e"># Run rbern with P=0.5 10,000 times&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.4991
&lt;/code>&lt;/pre>
&lt;p>A more general way to rerun a chunk of code is through the for loop or
convenient wrappers such as the &lt;code>replicate()&lt;/code> function. I demonstrate
some of their uses below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{ &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span> ) } )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.5027
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># See if several runs of rbern( p=0.5, n=1e4 ) give results around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#75715e"># Plot Ps to see if they scatter around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">100&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0.47&lt;/span>, &lt;span style="color:#ae81ff">0.53&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;P&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( Ps, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-9-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="randomness-in-models">Randomness in Models&lt;/h3>
&lt;p>The method described above works only for simple cases. What about
complex statistical models? How do we test that they are working as they
claim to? Guess what? &lt;em>&lt;strong>Simulation&lt;/strong>&lt;/em> is the key.&lt;/p>
&lt;p>We simulate data based on the assumptions of the statistical model and
see if it indeed returns what it claims to estimate. The simulation can
be repeated several times, each set to different values of parameters.
If the parameters are always recovered by the statistical model, we can
then be confident that the model is properly constructed and correctly
coded. So &lt;strong>simulation is really &lt;em>not an option&lt;/em> when doing
statistics&lt;/strong>. It is the only safety that helps us guard against bugs in
our statistical models, both programmatical and theoretical ones.
Without first testing the statistical model on simulated data, any
inferences about the empirical data are in doubt.&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>In a real-world scenario of the example presented here, we would only
observe the score ($R$) of an item ($I$) taken by a testee ($T$). The
targets of interest are the unobserved item difficulty ($D$) and testee
ability ($A$). In &lt;a href="https://yongfu.name/irt2">Part 2&lt;/a>, we will work in reverse and fit
statistical models on simulated data. We will see how the models
discover the true $A$s and $D$s from the information of $R$, $I$, and
$T$. See you there!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I mean, what the hack is &lt;em>Joint and Conditional Maximum Likelihood
Estimation&lt;/em>? These are methods developed in the psychometric and
measurement literature and are hardly seen in other fields. Unless
obsessed with psychometrics, I don’t think one would be able to
understand these things.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category><category>reproducibility</category></item><item><title>A Minimalist Structure for Snakemake</title><link>https://yongfu.name/2023/02/15/snakemake/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/15/snakemake/</guid><description>&lt;p>I have &lt;a href="https://kbroman.org/minimal_make">heard of&lt;/a> the use of &lt;a href="https://www.gnu.org/software/make/">GNU Make&lt;/a> for enhancing
reproducibility for some time. I did not incorporate Make into my work however,
since a simple build script written in Bash was sufficient. Everything was well
in control, and I could structure the workflow to my will.&lt;/p>
&lt;p>It was not until I started working in a company setting that I found most things
out of my control. Decades of conventions have been accumulating and passing on,
and personal workflows have to fit into existing ones. In order to fit into my
company&amp;rsquo;s conventions of data analysis (which pretty much just ignore analysis
reproducibility), the number of scripts grew exponentially and quickly fell out
of my control (see figure below, which is automatically generated by Snakemake).
I needed a way to document and track my workflow in a consistent and scalable
manner. This was when I picked up Prof. Broman&amp;rsquo;s &lt;a href="https://kbroman.org/minimal_make">great introductory post&lt;/a>
on GNU Make again. Everything seemed hopeful, but I was soon defeated by the
omnipresent Windows. Since it is required to work on Windows machines in my
company, and since &lt;a href="http://gnuwin32.sourceforge.net/packages/make.htm">Make for Windows&lt;/a> has difficulties dealing with
Chinese file paths, I had to give up on Make. &lt;a href="https://snakemake.github.io">Snakemake&lt;/a> came as my
savior.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://img.yongfu.name/posts/dag.png" alt="Data analysis workflow graph generated by Snakemake">
&lt;figcaption>Data analysis workflow graph generated by Snakemake&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="meeting-snakemake">Meeting Snakemake&lt;/h2>
&lt;p>Snakemake was inspired by, but way more complicated than, GNU Make. Since it is
backed by Python, cross-platform issues such as character encodings are
automatically resolved. Snakemake is a thoughtful project that was originally
developed to facilitate computational research and reproducibility. Thus, it may
take some time to get started since there are many concepts to pick up. It&amp;rsquo;s
totally worth it, however. Dealing with complex tasks requires a complicated
framework. Often, these complications make sense (and are appreciated) only
after we face real-world complexities. Going through &lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html">Snakemake&amp;rsquo;s
tutorial&lt;/a> and experimenting with it on the computer would be sufficient
to get an average user started. It is not as complicated as it seems at first
glance.&lt;/p>
&lt;h2 id="snakemake-recommended-workflow">Snakemake Recommended Workflow&lt;/h2>
&lt;p>A great thing about Snakemake is that it is &lt;a href="https://stackoverflow.com/a/82064">opinionated&lt;/a>. This means
that certain conventions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> are proposed, and most users would benefit
from them since they spare the burden of planning and creating workflow
structures.&lt;/p>
&lt;p>For instance, Snakemake &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html#distribution-and-reproducibility">recommends&lt;/a> the directory structure listed
below for every Snakemake workflow. This structure is so simple that its genius
might not be obvious at first glance. There are four directories in the project
root&amp;mdash;&lt;code>workflow/&lt;/code>, &lt;code>config/&lt;/code>, &lt;code>results/&lt;/code>, and &lt;code>resources/&lt;/code>. &lt;code>workflow/&lt;/code> holds
the &lt;em>coding&lt;/em> stuff. Code for data analysis, computation, and reproducibility are
all found in this directory. &lt;code>config/&lt;/code> is for optional configuration and I would
skip it here (in my own project, I did not use config files since the
&lt;code>Snakefile&lt;/code> is sufficient for my purposes). &lt;code>results/&lt;/code> and &lt;code>resources/&lt;/code> are what
(I think) make this structure fantastic. &lt;code>resources/&lt;/code> holds all &lt;strong>raw data&lt;/strong>,
i.e., data that are not reproducible on your computer (e.g., manually annotated
data). All data resulting from the computation in the current project are
located in &lt;code>results/&lt;/code>. So ideally, you could delete &lt;code>results/&lt;/code> at any time
without worry. A single command &lt;code>snakmake -c&lt;/code> should generate all the results
from &lt;code>resources/&lt;/code>. The genius of this structure is that it eliminates the need
of worrying about where to place newly arrived data, as commonly encountered in
real-world situations (e.g., an analysis might require data that you did not
foresee).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>├── .gitignore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>├── README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>├── workflow
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>│ ├── rules
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>| │ ├── module1.smk
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>| │ └── module2.smk
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>│ ├── scripts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>| │ ├── script1.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>| │ └── script2.R
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>| └── Snakefile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>├── config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>│ └── config.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>├── results
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>└── resources
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="an-enhanced-snakemake-workflow">An Enhanced Snakemake Workflow&lt;/h2>
&lt;p>I adopted the workflow above in my work. It was great, but I still found &lt;strong>two
annoying drawbacks&lt;/strong>.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Long directory names&lt;/strong>&lt;/p>
&lt;p>Since in a &lt;code>Snakefile&lt;/code>, file paths of inputs and outputs are always
repeated, it soon becomes annoying to type in paths starting with
&lt;code>resources/...&lt;/code> and &lt;code>results/...&lt;/code>. In addition, &amp;ldquo;resources&amp;rdquo; and &amp;ldquo;results&amp;rdquo;
have a common prefix, which often confuses me. It would be better off if the
two terms are more readily distinguished visually.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Confusing relative paths&lt;/strong>&lt;/p>
&lt;p>According to the &lt;a href="https://snakemake.readthedocs.io/en/latest/project_info/faq.html#how-does-snakemake-interpret-relative-paths">documentation&lt;/a>, relative paths in different
directives are &lt;strong>interpreted differently&lt;/strong>. To be short, relative paths in
&lt;code>input:&lt;/code>, &lt;code>output:&lt;/code>, and &lt;code>shell:&lt;/code> are interpreted relative to the working
directory (i.e., where you invoke the command &lt;code>snakemake -c&lt;/code>), whereas in
directives such as &lt;code>script:&lt;/code>, they are interpreted as relative to the
&lt;code>Snakefile&lt;/code>. So it would be cognitively demanding to switch back and forth
between the reference points of relative paths while writing the
&lt;code>Snakefile&lt;/code>. Why not have all paths relative to the project root?&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>To deal with the aforementioned problems, I modified the recommended directory
structure and arrived at the structure below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>├── README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>├── Snakefile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>├── made
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>├── raw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>└── src
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>
&lt;p>&lt;strong>Simplified directory names&lt;/strong>&lt;/p>
&lt;p>&lt;code>resources/&lt;/code> is renamed as &lt;code>raw/&lt;/code>, and &lt;code>results/&lt;/code> is renamed as &lt;code>made/&lt;/code>. The
&lt;code>workflow/&lt;/code> directory is broken down into &lt;code>src/&lt;/code> (holding scripts) and the
&lt;code>Snakefile&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Consistent relative paths&lt;/strong>&lt;/p>
&lt;p>Since &lt;code>Snakefile&lt;/code> is now placed in the project root, the problem of
different relative paths for different directives is resolved, as long as
the user always invokes the command &lt;code>snakemake -c&lt;/code> in the project root.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The source code of this Snakemake workflow can be found
&lt;a href="https://github.com/liao961120/minimal-snake">here on GitHub&lt;/a>.&lt;/p>
&lt;h2 id="some-notes-for-using-git-bash-as-shell">Some Notes for Using Git-Bash as Shell&lt;/h2>
&lt;p>The experience of using Snakemake on Windows is great overall. I ran into a
few problems, but the problems were usually solvable. There is one particular
problem that took me a while to solve. On Windows, the default shell executable
used in Snakemake (and Python) is Cmd (or maybe Powershell). But since I am more
familiar with Bash and Unix tools, it is a real inconvenience. I had set up
Git-Bash on the company&amp;rsquo;s Windows machine but then spent a long time figuring
out how to set Git-Bash as the default shell in Snakemake. The information for
Snakemake users on Windows is scarce. I guess Snakemake is just unpopular among
Windows users. After reading the &lt;a href="https://snakemake.readthedocs.io/en/v6.15.2/_modules/snakemake/shell.html">source code&lt;/a>, the solution turned
out to be quite simple. Just put the code below at the top of the &lt;code>Snakefile&lt;/code>
and place the path to Git-Bash executable in &lt;code>shell.executable()&lt;/code>. This will
allow the identical &lt;code>Snakefile&lt;/code> to be used on both Windows and Unix-like
computers without additional configurations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Additional setup for running with git-bash on Windows&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>name &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;nt&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#f92672">from&lt;/span> snakemake.shell &lt;span style="color:#f92672">import&lt;/span> shell
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> shell&lt;span style="color:#f92672">.&lt;/span>executable(&lt;span style="color:#e6db74">r&lt;/span>&lt;span style="color:#e6db74">&amp;#39;C:\Users\rd\AppData\Local\Programs\Git\bin\bash.exe&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&amp;ldquo;Good&amp;rdquo; conventions here, as opposed to naturally-resulting conventions without the consideration of reproducibility.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>python</category><category>reproducibility</category></item><item><title>我的 R 開放課程</title><link>https://yongfu.name/2021/06/03/my-r-course/</link><pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate><guid>https://yongfu.name/2021/06/03/my-r-course/</guid><description>&lt;p>這學期 (109-2) 第二次擔任課程助教，給大學部的同學們上 R 語言。第二次教學，在熱情上減了一半，在教材難度上增加了一半。大概是因為不喜歡重複做一樣的事，這次課程刻意補了上一次 (&lt;a href="https://rlads2019.github.io/lab/">108-1&lt;/a>) 懶得教&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>、沒時間教&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>、沒自信可以教&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>以及沒有能力教&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>的內容。有了這些新的內容，課程準備起來就比較提得起勁，畢竟知道在去年已經教過的內容上，現在的我實在很難超越當時的自己&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>。不過，在準備教材的同時，也可以看到自己這一陣子的轉變：對某些概念的認識更加地完整、思緒變得更複雜迅速有條理。能發現這些真的蠻開心的，至少腦袋還是有長一些。擴增教材的另一個目的，是為了補齊上一次未能 (學會然後) 教的缺憾，大概是一種想把圓畫完整的感覺，當然圓不可能畫得完美&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>，不過整體來說，完整的感覺還是大過缺憾許多，也蠻值得開心的。&lt;/p>
&lt;p>這次的課程全部採取事前錄製影片的方式授課，表面上的目的是擔心遠距教學的情形再度出現 (結果真的出現了&amp;hellip;)，實際上的目的則是跟剛剛一樣：我不想要做同樣的事情 (實體授課)，而且這次實體授課大概也不會講得比上次好，所以不如換一種方式授課&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>。結果無心插柳，最後幾堂課因為突來的疫情改成遠距教學，對我反倒沒造成什麼影響，又是件值得開心的事情。&lt;/p>
&lt;p>在經歷 12 個頗為漫長的週末，終於完成了這個算是完整的 R 語言課程。在這 12 堂課中，或許值得慶幸的一件事情是課程內容主題都不是 state-of-the-art。在能夠選擇時，選擇了去講比較穩定不變的東西、比較不會經過兩三年後就變成歷史名詞的技術或概念。這麼做或許可以讓課程的保鮮期變得比較長，也或許可以讓更多人從這個課程中受惠。這 12 堂課程的完整內容 (影片/簡報/講義/作業/程式碼) 可以在這個&lt;a href="https://rlads2021.github.io/archives/">頁面&lt;/a>取得，歡迎讀者自行使用、分享或是修改並應用於教學之中。&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://img.yongfu.name/posts/rlads2021.png" alt="2021 課程影片">
&lt;figcaption>2021 課程影片&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>整個學期下來，雖然過得平淡、沒有第一次授課時那種短時間內大量成長的感覺，不過反倒是有種&lt;strong>比較完整地完成了一件事情&lt;/strong>的感覺。這種感覺不會讓人大喜大悲、異常緊張或是過度興奮，但會讓心裡變得厚實舒坦，讓內心安穩一些，然後微微的喜悅就會從心底緩慢而持續地湧出。&lt;/p>
&lt;p>最後，還要特別感謝 Andrea、Yulin、Mao-Chang 以及 Amber 在各個方面的協助，這門課的學生能有你們的照顧真的很幸運。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Lab01: &lt;a href="https://rlads2021.github.io/LabBook/ch01">路徑、終端機、R101&lt;/a> (絕對與相對路徑 &amp;amp; Terminal)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Lab12: &lt;a href="https://rlads2021.github.io/LabBook/ch12">專案成果展示&lt;/a> (Shiny)&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Lab09: &lt;a href="https://rlads2021.github.io/LabBook/ch09">文本與詞彙的向量表徵&lt;/a> (document-term matrix &amp;amp; latent semantic analysis)&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Lab06: &lt;a href="https://rlads2021.github.io/LabBook/ch06">Simulating Data with R&lt;/a> (Causal inference 101)&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>那時多有熱情和自信啊！反觀現在真的很難被激勵，不過往好處想或許這代表掌握情緒的能力有所提昇？&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>像是這次在教 Web API 時拿來示範用的 Public API 在我上傳教學影片之後就關閉服務了&amp;hellip;&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>這樣就不用和過去的自己硬碰硬比較，擔心自己退步了 (反正我有錄影片，觸及的人就是比較廣比較潮啦！108-1 的你輸了啦哈哈哈哈哈)。比較不見得會進步，但一定會帶來傷害，所以換個方式讓自己沒辦法去比較或許也不錯。&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>R</category><category>Course</category><category>statistics</category><category>linguistics</category><category>中文</category></item><item><title>Getting Tabular Data Through JavaScript in Compiled R Markdown Documents</title><link>https://yongfu.name/2020/09/09/getable/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/09/09/getable/</guid><description>&lt;p>Recently, I have learned more about JavaScript and created a few JS web apps. This gave me the idea that we can separate the &lt;em>content&lt;/em> and the &lt;em>data&lt;/em> in an HTML document to make it more &lt;strong>dynamic&lt;/strong>&amp;mdash;the content stays static while the data could be updated independently without rewriting or recompiling the HTML document. This could be done by utilizing JavaScript&amp;rsquo;s ability to asynchronously fetch data from the web and generate DOM elements based on these data.&lt;/p>
&lt;p>I implemented this idea in my new R package &lt;a href="https://github.com/liao961120/getable">&lt;code>getable&lt;/code>&lt;/a>. Basically, &lt;code>getable&lt;/code> lets the user insert &lt;em>dynamic&lt;/em> HTML tables in R Markdown (HTML output only) by providing the URLs to the tables&amp;rsquo; data. Every time when the compiled HTML document is opened, the data are fetched from the web and used to generate the HTML tables. This means that the user can update the data (e.g., hosted in a public GitHub repo) without recompiling the HTML from R Markdown.
In addition to hosting data in GitHub repos or on static sites, the user could use &lt;strong>Google Spreadsheets&lt;/strong> as the data store, as shown in the GIF below.&lt;/p>
&lt;p>
&lt;img src="https://img.yongfu.name/posts/getable.gif" alt="">
&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>&lt;code>getable&lt;/code> is now on CRAN, which can be installed with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">install.packages&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;getable&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or, install the latest version from GitHub:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>remotes&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">install_github&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;liao961120/getable&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>&lt;code>getable&lt;/code> comes with a template that you can import in RStudio by selecting: &lt;code>File &amp;gt; New File &amp;gt; R Markdown &amp;gt; From Template &amp;gt; HTML Tables with Dynamic Data {GETable}&lt;/code>.&lt;/p>
&lt;p>Or, you can simply run the command below in the R console:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>rmarkdown&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">draft&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;name_your_file.Rmd&amp;#34;&lt;/span>, template &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tablefromweb&amp;#34;&lt;/span>, package &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;getable&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The template contains several files, of which &lt;code>dfFromWeb.html&lt;/code>, &lt;code>dfFromWeb.js&lt;/code>, and &lt;code>dfFromWeb.css&lt;/code> are required for the compiled HTML to work properly (DO NOT change the RELATIVE PATHs between these files and the source Rmd). Note that you can style the appearance of the HTML tables with CSS in &lt;code>dfFromWeb.css&lt;/code>, and if you know a lot about JS, you can even modify the code in &lt;code>dfFromWeb.js&lt;/code> to use other JS libraries to generate the HTML tables. You can see a working example &lt;a href="https://yongfu.name/getable/demo/">here&lt;/a>.&lt;/p>
&lt;h3 id="inserting-tables">Inserting Tables&lt;/h3>
&lt;p>Simply use the function &lt;code>renderTable(&amp;quot;&amp;lt;URL&amp;gt;&amp;quot;)&lt;/code> in a code chunk to insert a dynamic HTML table. Remember to set the chunk option &lt;code>results='asis'&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-rmd" data-lang="rmd">```{r results=&amp;#39;asis&amp;#39;}
getable::renderTable(&amp;#34;https://yongfu.name/getable/demo/data/df.csv&amp;#34;)
```
&lt;/code>&lt;/pre></description><category>R</category><category>R Markdown</category><category>JavaScript</category><category>R-bloggers</category></item><item><title>ntuthesis</title><link>https://yongfu.name/2019/03/07/ntuthesis/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://yongfu.name/2019/03/07/ntuthesis/</guid><description>&lt;p>2023.2.27 更：下文套件已失修，若有需求，請參考 &lt;a href="https://github.com/liao961120/thesis">thesis&lt;/a>。&lt;/p>
&lt;hr>
&lt;p>去年十月份的研究所甄試，實在找不到合乎主題的文章報告可以附在審查資料。想想自己可以拿來說嘴的大概只剩 R Markdown，於是寫了&lt;a href="https://liao961120.github.io/ling-rmd">一篇 (硬是扯上語言學的) 文章&lt;/a>交了上去。為了怕被面試老師問：「你在文章中說的可信嗎？文章中哪裡可看出你的研究能力？」便在文章中引用自己撰寫的&lt;a href="https://liao961120.github.io/linguisticsdown">套件&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>。由於還是害怕文章太過空泛，又將文章中的部份想法 &amp;ndash; R Markdown 模板，實作出來以備不時之需，&lt;a href="https://liao961120.github.io/ntuthesis">&lt;code>ntuthesis&lt;/code>&lt;/a> 因而誕生了。結果 &amp;hellip; 面試時，老師們根本沒有問到有關 R Markdown 的問題。&lt;/p>
&lt;h2 id="緣起">緣起&lt;/h2>
&lt;p>這個用 R Markdown 寫論文的想法，其實在一年多前&lt;a href="https://yongfu.name/2018/01/31/RlearningPath.html">剛認識 R Markdown 時&lt;/a>時就有了。當時還不太確定是否可行，僅覺得如果可行的話一定會很有趣。現在，我百分百確定這是可行的 、九成九確定它能幫你省下論文排版的功夫。&lt;/p>
&lt;p>&lt;code>ntuthesis&lt;/code> 原本僅是個 &lt;a href="https://github.com/rstudio/bookdown">bookdown&lt;/a> (R Markdown 的一個擴充) 論文模板，但為了讓其便於使用，我將它作成套件。&lt;code>ntuthesis&lt;/code> 的目的只有一個 &amp;ndash; 讓作者能專注在&lt;strong>論文內容&lt;/strong>的寫作。其它論文寫作的麻煩事：排版、口試委員審定書、目錄、圖目錄、表目錄、文獻引用格式、浮水印，全部都能自動生成。論文模板的概念並不新穎，且現存許多 LaTeX 論文模板。但撰寫 LaTeX 的過程非常辛苦，因為作者必須時時將注意力放在&lt;strong>論文排版&lt;/strong>上。相對的，&lt;code>ntuthesis&lt;/code> 讓作者能用 &lt;a href="https://zh.wikipedia.org/wiki/Markdown#%E7%A4%BA%E4%BE%8B">Markdown&lt;/a> 撰寫論文，甚至可使用 R 語言直接在論文中動態產生結果 (如統計圖、數值與報表)。&lt;/p>
&lt;h2 id="總是需要跨出第一步">總是需要跨出第一步&lt;/h2>
&lt;p>現在的問題是，臺灣還沒有人用過 &lt;code>ntuthesis&lt;/code> 撰寫論文，而當「第一位」總是相當可怕的，更何況還要脫離自己習慣的寫作環境 (例如，MS Word)。作為 &lt;code>ntuthesis&lt;/code> 的作者，&lt;strong>我一定會用 &lt;code>ntuthesis&lt;/code> 撰寫論文&lt;/strong>，但不會是在短期內 (我才剛錄取研究所)。所以這篇文章的目的基本上有兩個：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>推銷 &lt;code>ntuthesis&lt;/code>：&lt;/p>
&lt;p>尋找願意使用 &lt;code>ntuthesis&lt;/code> 撰寫論文的研究生&lt;/p>
&lt;/li>
&lt;li>
&lt;p>為 &lt;code>ntuthesis&lt;/code> 擔保&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>：&lt;/p>
&lt;blockquote>
&lt;p>你專心寫論文，我負責處理論文格式。&lt;/p>
&lt;/blockquote>
&lt;p>如果使用 &lt;code>ntuthesis&lt;/code> 撰寫臺大&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>論文，你只要擔心論文的內容，任何套件相關的問題 (如排版設定不正確、bug、說明文件不清楚等) 我都會 (盡力) 協助解決 (不擺爛)。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="一點點的使用門檻">一點點的使用門檻&lt;/h2>
&lt;p>天下沒有白吃的午餐，使用新東西固然有點門檻：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>若已在用 R Markdown，那這門檻應該很低&lt;/p>
&lt;/li>
&lt;li>
&lt;p>若已經會用 bookdown，恭喜你沒有門檻問題&lt;/p>
&lt;/li>
&lt;li>
&lt;p>若不懂 R Markdown，甚至完全不懂 Markdown&lt;/p>
&lt;ol>
&lt;li>
&lt;p>請花 3 分鐘閱讀&lt;a href="https://zh.wikipedia.org/wiki/Markdown">維基百科&lt;/a>，花 5 分鐘&lt;a href="https://jbt.github.io/markdown-editor">用用看 Markdown&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>花一段悠閒的時光 (我是在臺中至臺北的客運上用平板滑完的)，輕鬆地閱讀僅 137 頁的 &lt;a href="https://bookdown.org/yihui/bookdown/">bookdown Book&lt;/a> (&lt;a href="https://bookdown.org/yihui/bookdown/bookdown.pdf">PDF&lt;/a>)。請不要很認真的細讀每頁，挑自己需要的看。請不要把它當成是負擔。當成是增廣見聞，欣賞世界上竟然有這種東西。看這本書真的很享受。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h2 id="問題">問題&lt;/h2>
&lt;p>若有任何關於 &lt;code>ntuthesis&lt;/code> 的問題，請儘管提出來。我最歡迎在 GitHub 提出 &lt;a href="https://github.com/liao961120/ntuthesis/issues">issue&lt;/a>，但若沒有 GitHub 帳號，也可以透過 &lt;a href="mailto:liao961120@gmail.com">Email&lt;/a> 與我聯絡。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;code>linguisticsdown&lt;/code> 當時僅有 3 個函數，而且是在兩天內寫完然後提交到 CRAN。雖然簡陋，但我覺得這套件是目前我所&lt;a href="https://yongfu.name/#projects">製造出來的一堆玩具&lt;/a>中最有用的。&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>由於我未來會使用 &lt;code>ntuthesis&lt;/code> 撰寫論文，所以請相信我有動機 (至少自此之後的兩年) 維護此套件。&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;code>ntuthesis&lt;/code> 其實亦可&lt;a href="https://liao961120.github.io/ntuthesis/articles/extend_template.html">撰寫他校論文&lt;/a>，但目前缺乏他校 (封面) 模板。&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>R Markdown</category><category>R</category><category>writing-tool</category><category>中文</category></item><item><title>Visualizing Language Loss in Taiwan</title><link>https://yongfu.name/2019/02/17/visualize-language-loss/</link><pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate><guid>https://yongfu.name/2019/02/17/visualize-language-loss/</guid><description>
&lt;p>&lt;a href="https://twlangsurvey.github.io">Taiwan Language Survey&lt;/a> is a small project I worked on during May to June in 2018. The idea was to create a survey that &lt;strong>continuously&lt;/strong> collects data and a web page that visualizes the collected data. The web page is updated weekly using &lt;a href="https://travis-ci.org/twLangSurvey/twLangSurvey.github.io">Travis-CI&lt;/a>.&lt;/p>
&lt;p>The main purpose of this survey is to raise public awareness of &lt;strong>language loss&lt;/strong> in Taiwan. Hence, the survey is designed to collect data that can provide valuable information about language loss, for example, some questions were asked to gain insight about the change of linguistic competence acoss generations in a family (i.e. across the subject’s parents, the subject, and the subject’s children). In addition to changes within family, information about the age of the subjects is also colleceted, meaning that we can see how linguistic competence changes among diffrent age groups of subjects in a community, i.e. is a language becoming more dominant or entering the dying process?&lt;/p>
&lt;p>Visualization is a powerful tool to capture how linguistic competences of different langauges are changing. But creating visualizations necessitates creativity – how can language loss be visualized? Below, I illustrate one of the methods I created for visualizing language loss – a visaulization inspired by the &lt;a href="https://en.wikipedia.org/wiki/Population_pyramid">age-sex pyramid&lt;/a>.&lt;/p>
&lt;div id="age-sex-pyramid-of-language" class="section level2">
&lt;h2>Age-Sex Pyramid of Language&lt;/h2>
&lt;p>The age-sex pyramid is used to visualize the population structure of a community. The vertical axis indicates the age and each horizontal bar represents an age group. The horizontal axis indicates the population size of male or female of a particular age group.&lt;/p>
&lt;p>The age-sex pyramid is a great tool to visualize the population structure since the ‘shape’ of the pyramid gives readers a lot information. For example, an ‘expansive pyramid’ has longer bars at the bottom of the pyramid, which indicates the population is young and growing. A ‘stationary pyramid’ looks like a rectangular bar, indicating the population sizes of different age groups are about the same. A ‘constructive’ pyramid indicates a shrinking population, which is narrowed at the bottom.&lt;/p>
&lt;p>Similarly, a &lt;strong>modified&lt;/strong> version of the age-sex pyramid, which I’ll call the ‘&lt;strong>age-sex pyramid of language&lt;/strong>’, can be used to visualize the population structure of a language and predicts the language’s &lt;strong>vitality&lt;/strong>. Instead of visualizing population size, the age-sex pyramid of language visualizes the &lt;strong>average fluency of a language&lt;/strong> on the horizontal axis.&lt;/p>
&lt;div class="figure">&lt;span id="fig:tw-pyramid">&lt;/span>
&lt;img src="https://twlangsurvey.github.io/out_graph/Tw_pyramid.png" alt="An age-sex pyramid of Taiwanese. The red bars on the left indicates females of different age groups and the blue bars on the right indicates males. The average fluency (values on the horizontal axis) is calculated from a six-point scale (0-5) on Taiwanese fluency." width="70%" />
&lt;p class="caption">
Figure 1: An age-sex pyramid of Taiwanese. The red bars on the left indicates females of different age groups and the blue bars on the right indicates males. The average fluency (values on the horizontal axis) is calculated from a six-point scale (0-5) on Taiwanese fluency.
&lt;/p>
&lt;/div>
&lt;p>As shown in Figure &lt;a href="#fig:tw-pyramid">1&lt;/a>, the shape of the age-sex pyramid of &lt;a href="https://en.wikipedia.org/wiki/Taiwanese_Hokkien">Taiwanese&lt;/a> in Taiwan&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> is an ‘&lt;strong>inverted triangle&lt;/strong>’, which is almost never seen in the conventional population pyramid. However, this inverted triangular shape is expected to appear quite often, since it indicates an ongoing language loss in a community.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th>&lt;strong>Vitality of Language&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Shape of Pyramid&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td>Shrinking and Dying&lt;/td>
&lt;td>Inverted Triangle&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td>Growing&lt;/td>
&lt;td>Triangle&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td>Stable&lt;/td>
&lt;td>Rectangular Bar&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;div id="drawing-age-sex-pyramid-with-ggplot2" class="section level2">
&lt;h2>Drawing Age-Sex Pyramid with ggplot2&lt;/h2>
&lt;p>As complex as it might seem, an age-sex pyramid created with ggplot2 is actually a (modified) bar chart. I learned this on &lt;a href="https://stackoverflow.com/a/36804394">stackoverflow&lt;/a>, and the trick is&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Use &lt;code>ifelse&lt;/code> to flip the value (here, population size) according to the gender of the age group&lt;/li>
&lt;li>Use &lt;code>geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code> to let the heights of the bars represent values in the data frame, i.e. the value given to &lt;code>y&lt;/code>&lt;/li>
&lt;li>Use &lt;code>coord_flip()&lt;/code> to make the bars horizontal&lt;/li>
&lt;/ol>
&lt;pre class="language-r">&lt;code class="language-r">&lt;span class="kw">set.seed&lt;/span>(&lt;span class="dv">1&lt;/span>)
df0 &amp;lt;-&lt;span class="st"> &lt;/span>tibble&lt;span class="op">::&lt;/span>&lt;span class="kw">tibble&lt;/span>(
&lt;span class="dt">Age =&lt;/span> &lt;span class="kw">rep&lt;/span>(&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;10-19&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;20-29&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;30-39&amp;#39;&lt;/span>), &lt;span class="dv">2&lt;/span>),
&lt;span class="dt">Gender =&lt;/span> &lt;span class="kw">rep&lt;/span>(&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;Female&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Male&amp;#39;&lt;/span>), &lt;span class="dt">each =&lt;/span> &lt;span class="dv">3&lt;/span>),
&lt;span class="dt">PopSize =&lt;/span> &lt;span class="kw">sample&lt;/span>(&lt;span class="dv">0&lt;/span>&lt;span class="op">:&lt;/span>&lt;span class="dv">100&lt;/span>, &lt;span class="dt">size =&lt;/span> &lt;span class="dv">6&lt;/span>, &lt;span class="dt">replace =&lt;/span> T)
)
df0&lt;/code>&lt;/pre>
&lt;div class="kable-table">
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">Age&lt;/th>
&lt;th align="left">Gender&lt;/th>
&lt;th align="right">PopSize&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">10-19&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="right">26&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">20-29&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="right">37&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">30-39&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="right">57&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">10-19&lt;/td>
&lt;td align="left">Male&lt;/td>
&lt;td align="right">91&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">20-29&lt;/td>
&lt;td align="left">Male&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">30-39&lt;/td>
&lt;td align="left">Male&lt;/td>
&lt;td align="right">90&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;pre class="language-r">&lt;code class="language-r">&lt;span class="kw">library&lt;/span>(ggplot2)
pl &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">ggplot&lt;/span>(df0, &lt;span class="kw">aes&lt;/span>(&lt;span class="dt">x =&lt;/span> Age,
&lt;span class="dt">y =&lt;/span> &lt;span class="kw">ifelse&lt;/span>(Gender &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Male&amp;#39;&lt;/span>, PopSize, &lt;span class="op">-&lt;/span>PopSize),
&lt;span class="dt">fill =&lt;/span> Gender)) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">geom_bar&lt;/span>(&lt;span class="dt">stat =&lt;/span> &lt;span class="st">&amp;#39;identity&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">coord_flip&lt;/span>()
pl&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://img.yongfu.name/assets/visualize-language-loss/unnamed-chunk-1-1.png" width="70%" />&lt;/p>
&lt;p>To center the plot (i.e. to make the point where population size is zero at the center of the plot), we have to scale the axis of ‘population size’ (&lt;code>y&lt;/code>) with &lt;code>scale_y_continuous&lt;/code>:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">pl &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>&lt;span class="kw">scale_y_continuous&lt;/span>(&lt;span class="dt">limits =&lt;/span> &lt;span class="kw">c&lt;/span>(&lt;span class="op">-&lt;/span>&lt;span class="dv">100&lt;/span>, &lt;span class="dv">100&lt;/span>),
&lt;span class="dt">breaks =&lt;/span> &lt;span class="kw">seq&lt;/span>(&lt;span class="op">-&lt;/span>&lt;span class="dv">100&lt;/span>, &lt;span class="dv">100&lt;/span>, &lt;span class="dv">25&lt;/span>),
&lt;span class="dt">labels =&lt;/span> abs) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">labs&lt;/span>(&lt;span class="dt">y =&lt;/span> &lt;span class="st">&amp;#39;Population Size&amp;#39;&lt;/span>)&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://img.yongfu.name/assets/visualize-language-loss/unnamed-chunk-2-1.png" width="70%" />&lt;/p>
&lt;p>where &lt;code>abs&lt;/code> in &lt;code>labels&lt;/code> is the function &lt;code>abs()&lt;/code>. By default, &lt;code>ggplot2&lt;/code> pass the value given in &lt;code>breaks&lt;/code> to the the function specified in &lt;code>labels&lt;/code>.&lt;/p>
&lt;/div>
&lt;div id="visualizing-language-loss" class="section level2">
&lt;h2>Visualizing Language Loss&lt;/h2>
&lt;p>To create an age-sex pyramid of language, the data structure needed is exactly the same as the one above, except the variable, &lt;code>PopSize&lt;/code>, is replaced by ‘average fluency’ of a language. But since most people in Taiwan can speak more than one language (e.g. Mandarin-Taiwanese, Mandarin-Taiwanese-Hakka, Mandarin-English, etc.), the real data from the survey is a bit more complex Basically, the data structure needed to draw an age-sex pyramid of language looks like:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th>&lt;strong>Gender&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Ethnicity&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Age Group&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Avg. Fluency&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td>female&lt;/td>
&lt;td>Mandarin&lt;/td>
&lt;td>20-24&lt;/td>
&lt;td>4.53&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td>male&lt;/td>
&lt;td>Taiwanese&lt;/td>
&lt;td>20-24&lt;/td>
&lt;td>2.78&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td>…&lt;/td>
&lt;td>…&lt;/td>
&lt;td>…&lt;/td>
&lt;td>…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td>female&lt;/td>
&lt;td>Hakka&lt;/td>
&lt;td>25-29&lt;/td>
&lt;td>2.23&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td>male&lt;/td>
&lt;td>Taiwanese&lt;/td>
&lt;td>35-39&lt;/td>
&lt;td>3.57&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;div id="preparation-of-data" class="section level3">
&lt;h3>Preparation of Data&lt;/h3>
&lt;p>The raw data of &lt;a href="https://twlangsurvey.github.io/">Taiwan Language Survey&lt;/a> can be retrieved &lt;a href="https://docs.google.com/spreadsheets/d/1NOJ9O_zBR6s-9e1fFGVcRaBjn5qQuVB9ZeSebhBctoM/edit?usp=sharing">here&lt;/a>. The survey and raw data is in traditional Chinese. I’ll skip the step of cleaning raw data (e.g., turn variable names to English) and used the cleaned data &lt;code>survey.rds&lt;/code> instead.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">temp &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">tempfile&lt;/span>()
&lt;span class="kw">download.file&lt;/span>(&lt;span class="st">&amp;#39;https://raw.githubusercontent.com/twLangSurvey/twLangSurvey.github.io/master/data/survey.rds&amp;#39;&lt;/span>, &lt;span class="dt">destfile =&lt;/span> temp)
data &amp;lt;-&lt;span class="st"> &lt;/span>readr&lt;span class="op">::&lt;/span>&lt;span class="kw">read_rds&lt;/span>(temp)
&lt;span class="kw">head&lt;/span>(data, &lt;span class="dv">3&lt;/span>)&lt;/code>&lt;/pre>
&lt;div class="kable-table">
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">date&lt;/th>
&lt;th align="left">curr_resid&lt;/th>
&lt;th align="right">curr_resid_since&lt;/th>
&lt;th align="left">settle_5yy&lt;/th>
&lt;th align="left">home_town&lt;/th>
&lt;th align="left">gender&lt;/th>
&lt;th align="right">age&lt;/th>
&lt;th align="right">kid_num&lt;/th>
&lt;th align="left">edu_level&lt;/th>
&lt;th align="left">work&lt;/th>
&lt;th align="left">income&lt;/th>
&lt;th align="left">work_hr&lt;/th>
&lt;th align="left">tribe&lt;/th>
&lt;th align="right">Mand_listen&lt;/th>
&lt;th align="right">Mand_speak&lt;/th>
&lt;th align="right">Tw_listen&lt;/th>
&lt;th align="right">Tw_speak&lt;/th>
&lt;th align="right">Hak_listen&lt;/th>
&lt;th align="right">Hak_speak&lt;/th>
&lt;th align="right">Ind_listen&lt;/th>
&lt;th align="right">Ind_speak&lt;/th>
&lt;th align="right">SEA_listen&lt;/th>
&lt;th align="right">SEA_speak&lt;/th>
&lt;th align="right">Eng_listen&lt;/th>
&lt;th align="right">Eng_speak&lt;/th>
&lt;th align="left">first_lang&lt;/th>
&lt;th align="left">when_Mand&lt;/th>
&lt;th align="left">when_Tw&lt;/th>
&lt;th align="left">when_Hak&lt;/th>
&lt;th align="left">when_Ind&lt;/th>
&lt;th align="left">when_SEA&lt;/th>
&lt;th align="left">when_Eng&lt;/th>
&lt;th align="left">m_guard_identity&lt;/th>
&lt;th align="left">f_guard_identity&lt;/th>
&lt;th align="right">dad_Mand_speak&lt;/th>
&lt;th align="right">dad_Tw_speak&lt;/th>
&lt;th align="right">dad_Hak_speak&lt;/th>
&lt;th align="right">dad_Eng_speak&lt;/th>
&lt;th align="right">dad_Ind_speak&lt;/th>
&lt;th align="right">dad_SEA_speak&lt;/th>
&lt;th align="right">mom_Mand_speak&lt;/th>
&lt;th align="right">mom_Tw_speak&lt;/th>
&lt;th align="right">mom_Hak_speak&lt;/th>
&lt;th align="right">mom_Eng_speak&lt;/th>
&lt;th align="right">mom_Ind_speak&lt;/th>
&lt;th align="right">mom_SEA_speak&lt;/th>
&lt;th align="left">dad_mom_Mand_fq&lt;/th>
&lt;th align="left">dad_mom_Tw_fq&lt;/th>
&lt;th align="left">dad_mom_Hak_fq&lt;/th>
&lt;th align="left">dad_mom_Ind_fq&lt;/th>
&lt;th align="left">dad_mom_SEA_fq&lt;/th>
&lt;th align="left">dad_mom_Other_fq&lt;/th>
&lt;th align="left">me_dad_Mand_fq&lt;/th>
&lt;th align="left">me_dad_Tw_fq&lt;/th>
&lt;th align="left">me_dad_Hak_fq&lt;/th>
&lt;th align="left">me_dad_Ind_fq&lt;/th>
&lt;th align="left">me_dad_SEA_fq&lt;/th>
&lt;th align="left">me_dad_Other_fq&lt;/th>
&lt;th align="left">me_mom_Mand_fq&lt;/th>
&lt;th align="left">me_mom_Tw_fq&lt;/th>
&lt;th align="left">me_mom_Hak_fq&lt;/th>
&lt;th align="left">me_mom_Ind_fq&lt;/th>
&lt;th align="left">me_mom_SEA_fq&lt;/th>
&lt;th align="left">me_mom_Other_fq&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">2018-06-12&lt;/td>
&lt;td align="left">106&lt;/td>
&lt;td align="right">1996&lt;/td>
&lt;td align="left">是&lt;/td>
&lt;td align="left">428&lt;/td>
&lt;td align="left">女&lt;/td>
&lt;td align="right">56&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="left">大學&lt;/td>
&lt;td align="left">金融業&lt;/td>
&lt;td align="left">10萬以上&lt;/td>
&lt;td align="left">45 - 50 小時&lt;/td>
&lt;td align="left">不具原住民身份&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="left">華語&lt;/td>
&lt;td align="left">7歲前&lt;/td>
&lt;td align="left">7歲前&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">12-15歲&lt;/td>
&lt;td align="left">父親&lt;/td>
&lt;td align="left">母親&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="left">幾乎全用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎全用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">約一半&lt;/td>
&lt;td align="left">多數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">2018-06-13&lt;/td>
&lt;td align="left">204&lt;/td>
&lt;td align="right">2004&lt;/td>
&lt;td align="left">是&lt;/td>
&lt;td align="left">204&lt;/td>
&lt;td align="left">女&lt;/td>
&lt;td align="right">57&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="left">碩士&lt;/td>
&lt;td align="left">製造業&lt;/td>
&lt;td align="left">10萬以上&lt;/td>
&lt;td align="left">50 - 55 小時&lt;/td>
&lt;td align="left">不具原住民身份&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="left">華語&lt;/td>
&lt;td align="left">7歲前&lt;/td>
&lt;td align="left">7歲前&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">12-15歲&lt;/td>
&lt;td align="left">父親&lt;/td>
&lt;td align="left">母親&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">多數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">多數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">多數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">2018-06-13&lt;/td>
&lt;td align="left">103&lt;/td>
&lt;td align="right">2013&lt;/td>
&lt;td align="left">是&lt;/td>
&lt;td align="left">100&lt;/td>
&lt;td align="left">男&lt;/td>
&lt;td align="right">37&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="left">大學&lt;/td>
&lt;td align="left">金融業&lt;/td>
&lt;td align="left">55,000 - 60,000&lt;/td>
&lt;td align="left">25 - 30 小時&lt;/td>
&lt;td align="left">不具原住民身份&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="left">華語&lt;/td>
&lt;td align="left">7歲前&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">未學會&lt;/td>
&lt;td align="left">7-12歲&lt;/td>
&lt;td align="left">父親&lt;/td>
&lt;td align="left">母親&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="left">幾乎全用&lt;/td>
&lt;td align="left">少數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">多數使用&lt;/td>
&lt;td align="left">少數使用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎全用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;td align="left">幾乎不用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;p>The survey data, &lt;code>data&lt;/code>, contains 22 variables and 267 observations (subjects). I only need these variables below to create the plot I want:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gender&lt;/code>: The gender of the subject&lt;/li>
&lt;li>&lt;code>age&lt;/code>: The age of the subject&lt;/li>
&lt;li>&lt;code>m_guard_identity&lt;/code>: Male guardian of the subject, should be ‘father’ in most cases&lt;/li>
&lt;li>&lt;code>f_guard_identity&lt;/code>: Female guardian of the subject, should be ‘mother’ in most cases&lt;/li>
&lt;li>&lt;code>&amp;lt;lang&amp;gt;_speak&lt;/code>: The subject’s fluency of a language. &lt;code>&amp;lt;lang&amp;gt;&lt;/code> is one of ‘Mand’ (Mandarin), ‘Tw’ (Taiwanese), ‘Hak’ (&lt;a href="https://en.wikipedia.org/wiki/Hakka_Chinese">Hakka&lt;/a>), ‘Ind’ (languages of the indigenous peoples, aka &lt;a href="https://en.wikipedia.org/wiki/Formosan_languages">Formosan languages&lt;/a>, belong to Austronesian languages), ‘SEA’ (languages from South Easth Asia), and ‘Eng’ (English)&lt;/li>
&lt;li>&lt;code>dad_&amp;lt;lang&amp;gt;_speak&lt;/code>: The subject’s male guardian’s fluency of a language. &lt;code>&amp;lt;lang&amp;gt;&lt;/code> is same as above.&lt;/li>
&lt;li>&lt;code>mom_&amp;lt;lang&amp;gt;_speak&lt;/code>: The subject’s female guardian’s fluency of a language. &lt;code>&amp;lt;lang&amp;gt;&lt;/code> is same as above.&lt;/li>
&lt;/ul>
&lt;pre class="language-r">&lt;code class="language-r">&lt;span class="kw">library&lt;/span>(dplyr)
lang &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;Mand&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Tw&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Hak&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Ind&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;SEA&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Eng&amp;#39;&lt;/span>)
cols &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;gender&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;age&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;m_guard_identity&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;f_guard_identity&amp;#39;&lt;/span>,
&lt;span class="kw">paste0&lt;/span>(lang, &lt;span class="st">&amp;#39;_speak&amp;#39;&lt;/span>),
&lt;span class="kw">paste0&lt;/span>(&lt;span class="st">&amp;#39;dad_&amp;#39;&lt;/span>, lang, &lt;span class="st">&amp;#39;_speak&amp;#39;&lt;/span>),
&lt;span class="kw">paste0&lt;/span>(&lt;span class="st">&amp;#39;mom_&amp;#39;&lt;/span>, lang, &lt;span class="st">&amp;#39;_speak&amp;#39;&lt;/span>)
)
data &amp;lt;-&lt;span class="st"> &lt;/span>data[, cols] &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(gender &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;男&amp;#39;&lt;/span> &lt;span class="op">|&lt;/span>&lt;span class="st"> &lt;/span>gender &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;女&amp;#39;&lt;/span>)&lt;/code>&lt;/pre>
&lt;p>Since some content of the survey data is in Chinese, the code below is used to translate it to English:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">ch2eng &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(x) {
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;男&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Male&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;女&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Female&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;母親&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Mother&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;父親&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Father&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;無&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;None&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;(外)祖父&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Grandpa&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;(外)祖母&amp;#39;&lt;/span>) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Grandma&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">%in%&lt;/span>&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;阿姨&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;嬸嬸&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;舅媽&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;姑姑&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;伯母&amp;#39;&lt;/span>)) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Aunt&amp;#39;&lt;/span>)
&lt;span class="cf">if&lt;/span> (x &lt;span class="op">%in%&lt;/span>&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;叔叔&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;伯伯&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;舅舅&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;姑丈&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;姨丈&amp;#39;&lt;/span>)) &lt;span class="kw">return&lt;/span>(&lt;span class="st">&amp;#39;Uncle&amp;#39;&lt;/span>)
&lt;span class="kw">message&lt;/span>(&lt;span class="st">&amp;#39;No translation found for `&amp;#39;&lt;/span>, x, &lt;span class="st">&amp;#39;`&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;&lt;/span>&lt;span class="ch">\n&lt;/span>&lt;span class="st">&amp;#39;&lt;/span>)
&lt;span class="kw">return&lt;/span>(x)
}
ch2eng_vec &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(vec) {
new_vec &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">vector&lt;/span>(&lt;span class="kw">typeof&lt;/span>(vec), &lt;span class="kw">length&lt;/span>(vec))
&lt;span class="cf">for&lt;/span> (i &lt;span class="cf">in&lt;/span> &lt;span class="kw">seq_along&lt;/span>(vec)) {
new_vec[[i]] &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">ch2eng&lt;/span>(vec[[i]])
}
&lt;span class="kw">return&lt;/span>(new_vec)
}
data &amp;lt;-&lt;span class="st"> &lt;/span>data &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">mutate&lt;/span>(&lt;span class="dt">gender =&lt;/span> &lt;span class="kw">ch2eng_vec&lt;/span>(gender),
&lt;span class="dt">m_guard_identity =&lt;/span> &lt;span class="kw">ch2eng_vec&lt;/span>(m_guard_identity),
&lt;span class="dt">f_guard_identity =&lt;/span> &lt;span class="kw">ch2eng_vec&lt;/span>(f_guard_identity))
&lt;span class="kw">head&lt;/span>(data)&lt;/code>&lt;/pre>
&lt;div class="kable-table">
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">gender&lt;/th>
&lt;th align="right">age&lt;/th>
&lt;th align="left">m_guard_identity&lt;/th>
&lt;th align="left">f_guard_identity&lt;/th>
&lt;th align="right">Mand_speak&lt;/th>
&lt;th align="right">Tw_speak&lt;/th>
&lt;th align="right">Hak_speak&lt;/th>
&lt;th align="right">Ind_speak&lt;/th>
&lt;th align="right">SEA_speak&lt;/th>
&lt;th align="right">Eng_speak&lt;/th>
&lt;th align="right">dad_Mand_speak&lt;/th>
&lt;th align="right">dad_Tw_speak&lt;/th>
&lt;th align="right">dad_Hak_speak&lt;/th>
&lt;th align="right">dad_Ind_speak&lt;/th>
&lt;th align="right">dad_SEA_speak&lt;/th>
&lt;th align="right">dad_Eng_speak&lt;/th>
&lt;th align="right">mom_Mand_speak&lt;/th>
&lt;th align="right">mom_Tw_speak&lt;/th>
&lt;th align="right">mom_Hak_speak&lt;/th>
&lt;th align="right">mom_Ind_speak&lt;/th>
&lt;th align="right">mom_SEA_speak&lt;/th>
&lt;th align="right">mom_Eng_speak&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">Female&lt;/td>
&lt;td align="right">56&lt;/td>
&lt;td align="left">Father&lt;/td>
&lt;td align="left">Mother&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="right">57&lt;/td>
&lt;td align="left">Father&lt;/td>
&lt;td align="left">Mother&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Male&lt;/td>
&lt;td align="right">37&lt;/td>
&lt;td align="left">Father&lt;/td>
&lt;td align="left">Mother&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">2&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">3&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="right">44&lt;/td>
&lt;td align="left">Father&lt;/td>
&lt;td align="left">Mother&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Male&lt;/td>
&lt;td align="right">62&lt;/td>
&lt;td align="left">None&lt;/td>
&lt;td align="left">None&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="right">56&lt;/td>
&lt;td align="left">Father&lt;/td>
&lt;td align="left">Mother&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">1&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;td align="right">0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;div id="defining-ethnicity" class="section level4">
&lt;h4>Defining Ethnicity&lt;/h4>
&lt;p>You might notice that there is no variable in the data which explicitly indicates the ethnicity of the subject. To serve the purpose of this survey – visualizing the vitality of languages in a community, ethnicity is defined solely by the linguistics competence of a subject’s guardians.
To give a specific example, let &lt;code>subject A&lt;/code> has a mother who &lt;strong>can speak&lt;/strong>&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Mandarin and Taiwanese and a father who speaks Mandarin and Hakka, then &lt;code>subject A&lt;/code> is categorized as a Mandarin, a Taiwanese, and a Hakka simultaneously.&lt;/p>
&lt;p>The function &lt;code>filter_ethnic()&lt;/code> is used to filter out subjects with specified ‘ethnicity’. This function is useful for drawing age-sex pyramid for each of the six languages.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">filter_ethnic &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(df, lang, &lt;span class="dt">lev =&lt;/span> &lt;span class="dv">3&lt;/span>) {
sp_lang &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">vector&lt;/span>(&lt;span class="st">&amp;quot;character&amp;quot;&lt;/span>, &lt;span class="dv">3&lt;/span>)
sp_lang[&lt;span class="dv">1&lt;/span>] &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">paste0&lt;/span>(&lt;span class="st">&amp;quot;dad_&amp;quot;&lt;/span>, lang, &lt;span class="st">&amp;quot;_speak&amp;quot;&lt;/span>)
sp_lang[&lt;span class="dv">2&lt;/span>] &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">paste0&lt;/span>(&lt;span class="st">&amp;quot;mom_&amp;quot;&lt;/span>, lang, &lt;span class="st">&amp;quot;_speak&amp;quot;&lt;/span>)
sp_lang[&lt;span class="dv">3&lt;/span>] &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">paste0&lt;/span>(lang, &lt;span class="st">&amp;quot;_speak&amp;quot;&lt;/span>)
df2 &amp;lt;-&lt;span class="st"> &lt;/span>df &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(m_guard_identity &lt;span class="op">!=&lt;/span>&lt;span class="st"> &amp;#39;None&amp;#39;&lt;/span> &lt;span class="op">|&lt;/span>&lt;span class="st"> &lt;/span>f_guard_identity &lt;span class="op">!=&lt;/span>&lt;span class="st"> &amp;#39;None&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(.data[[sp_lang[&lt;span class="dv">1&lt;/span>]]] &lt;span class="op">&amp;gt;=&lt;/span>&lt;span class="st"> &lt;/span>lev &lt;span class="op">|&lt;/span>&lt;span class="st"> &lt;/span>.data[[sp_lang[&lt;span class="dv">2&lt;/span>]]] &lt;span class="op">&amp;gt;=&lt;/span>&lt;span class="st"> &lt;/span>lev) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">select&lt;/span>(age, gender, sp_lang)
&lt;span class="kw">return&lt;/span>(df2)
}&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div id="assinging-age-group-to-subjects" class="section level3">
&lt;h3>Assinging Age Group to Subjects&lt;/h3>
&lt;p>Remember that the data structure needed for plotting age-sex pyramids requires &lt;strong>age group to be the basic unit&lt;/strong>. &lt;code>data&lt;/code> now consisits of single subjects, and we need information to group subjects together according to their ages. &lt;code>mutate_age_group()&lt;/code> creates a new variable &lt;code>age_group&lt;/code> by the subject’s age.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">mutate_age_group &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(df, &lt;span class="dt">range =&lt;/span> &lt;span class="dv">5&lt;/span>){
df&lt;span class="op">$&lt;/span>age_group &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">as.character&lt;/span>(
&lt;span class="kw">cut&lt;/span>(df&lt;span class="op">$&lt;/span>age, &lt;span class="dt">right =&lt;/span> F, &lt;span class="dt">breaks =&lt;/span> &lt;span class="kw">seq&lt;/span>(&lt;span class="dv">10&lt;/span>, &lt;span class="dv">95&lt;/span>, &lt;span class="dt">by =&lt;/span> range))
)
&lt;span class="kw">return&lt;/span>(df)
}&lt;/code>&lt;/pre>
&lt;p>&lt;code>cut()&lt;/code> takes a numeric vector as its first input, and codes the values of the vector into new values according to the interval they fall into. In &lt;code>mutate_age_group()&lt;/code>, &lt;code>cut()&lt;/code> codes the input vectors according to the intervals specified in the argument &lt;code>breaks&lt;/code>.&lt;/p>
&lt;p>Now we can use these functions to add more information to the data frame. The idea is to first create a separated data frame for each ethnicity&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a> (by &lt;code>filter_ethnic()&lt;/code>), then attact new variables to the data frame that indicate a subject’s ethnicity (&lt;code>ethn_group&lt;/code>) and age group (&lt;code>age_group&lt;/code>).&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">lang &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;#39;Mand&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Tw&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Hak&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Ind&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;SEA&amp;#39;&lt;/span>, &lt;span class="st">&amp;#39;Eng&amp;#39;&lt;/span>)
lev &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">c&lt;/span>(&lt;span class="dv">3&lt;/span>, &lt;span class="dv">3&lt;/span>, &lt;span class="dv">3&lt;/span>, &lt;span class="dv">3&lt;/span>, &lt;span class="dv">3&lt;/span>, &lt;span class="dv">0&lt;/span>)
ethn_list_df &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">vector&lt;/span>(&lt;span class="st">&amp;quot;list&amp;quot;&lt;/span>, &lt;span class="kw">length&lt;/span>(lang))
&lt;span class="cf">for&lt;/span> (i &lt;span class="cf">in&lt;/span> &lt;span class="kw">seq_along&lt;/span>(lang)){
ethn_list_df[[i]] &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">filter_ethnic&lt;/span>(data,
&lt;span class="dt">lang =&lt;/span> lang[i],
&lt;span class="dt">lev =&lt;/span> lev[i]) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">mutate&lt;/span>(&lt;span class="dt">ethn_group =&lt;/span> lang[i]) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">mutate_age_group&lt;/span>() &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">select&lt;/span>(age, gender, age_group, ethn_group,
&lt;span class="kw">paste0&lt;/span>(lang[i], &lt;span class="st">&amp;quot;_speak&amp;quot;&lt;/span>)) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">rename&lt;/span>(&lt;span class="dt">lang_fluency =&lt;/span> &lt;span class="kw">paste0&lt;/span>(lang[i], &lt;span class="st">&amp;quot;_speak&amp;quot;&lt;/span>))
}&lt;/code>&lt;/pre>
&lt;p>Then we can recombine these data frames back to a single one, and this new data frame now has information about a subject’s ethinicity and age group he/she belongs to. (Note that the new data frame is &lt;strong>expended&lt;/strong> since one subject can have several ethnicity, i.e. one subject can appear in different rows of the data frame with different ethnicity.)&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">&lt;span class="kw">bind_rows&lt;/span>(ethn_list_df) &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>&lt;span class="kw">head&lt;/span>()&lt;/code>&lt;/pre>
&lt;div class="kable-table">
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="right">age&lt;/th>
&lt;th align="left">gender&lt;/th>
&lt;th align="left">age_group&lt;/th>
&lt;th align="left">ethn_group&lt;/th>
&lt;th align="right">lang_fluency&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="right">56&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="left">[55,60)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">4&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="right">57&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="left">[55,60)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="right">37&lt;/td>
&lt;td align="left">Male&lt;/td>
&lt;td align="left">[35,40)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="right">44&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="left">[40,45)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="right">56&lt;/td>
&lt;td align="left">Female&lt;/td>
&lt;td align="left">[55,60)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="right">53&lt;/td>
&lt;td align="left">Male&lt;/td>
&lt;td align="left">[50,55)&lt;/td>
&lt;td align="left">Mand&lt;/td>
&lt;td align="right">5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;/div>
&lt;div id="data-for-plotting" class="section level3">
&lt;h3>Data for Plotting&lt;/h3>
&lt;p>Finally, we are ready to group the subjects together according to his/her &lt;code>gender&lt;/code>, &lt;code>ethnicity&lt;/code>, and &lt;code>age_group&lt;/code>. After grouping, we can use &lt;code>dplyr::summarise()&lt;/code> to calculate each group’s fluency of the language, which will be used as the variable on the horizontal axis of the age-sex pyramid.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">pl_data &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">bind_rows&lt;/span>(ethn_list_df) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">group_by&lt;/span>(gender, ethn_group, age_group) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">summarise&lt;/span>(&lt;span class="kw">mean&lt;/span>(lang_fluency)) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">rename&lt;/span>(&lt;span class="dt">avg_fluency =&lt;/span> &lt;span class="st">`&lt;/span>&lt;span class="dt">mean(lang_fluency)&lt;/span>&lt;span class="st">`&lt;/span>)
&lt;span class="kw">head&lt;/span>(pl_data)&lt;/code>&lt;/pre>
&lt;div class="kable-table">
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">gender&lt;/th>
&lt;th align="left">ethn_group&lt;/th>
&lt;th align="left">age_group&lt;/th>
&lt;th align="right">avg_fluency&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[15,20)&lt;/td>
&lt;td align="right">1.750&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[20,25)&lt;/td>
&lt;td align="right">2.950&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[25,30)&lt;/td>
&lt;td align="right">2.600&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[30,35)&lt;/td>
&lt;td align="right">3.000&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[35,40)&lt;/td>
&lt;td align="right">1.750&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">Female&lt;/td>
&lt;td align="left">Eng&lt;/td>
&lt;td align="left">[40,45)&lt;/td>
&lt;td align="right">2.125&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;/div>
&lt;div id="plotting-function" class="section level3">
&lt;h3>Plotting Function&lt;/h3>
&lt;p>We are going to draw 6 age-sex pyramids, one for each languages. So instead of writing &lt;code>ggplot()&lt;/code> six times, I wrote a plotting function:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">&lt;span class="kw">library&lt;/span>(ggplot2)
pl_pyramid &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(data, &lt;span class="dt">title =&lt;/span> &lt;span class="ot">NULL&lt;/span>) {
&lt;span class="kw">ggplot&lt;/span>(data,
&lt;span class="kw">aes&lt;/span>(&lt;span class="dt">x =&lt;/span> age_group,
&lt;span class="dt">y =&lt;/span> &lt;span class="kw">ifelse&lt;/span>(gender &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Male&amp;#39;&lt;/span>, avg_fluency, &lt;span class="op">-&lt;/span>avg_fluency),
&lt;span class="dt">fill =&lt;/span> gender)) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">geom_bar&lt;/span>(&lt;span class="dt">stat =&lt;/span> &lt;span class="st">&amp;quot;identity&amp;quot;&lt;/span>, &lt;span class="dt">width =&lt;/span> &lt;span class="fl">0.7&lt;/span>) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">scale_y_continuous&lt;/span>(&lt;span class="dt">limits =&lt;/span> &lt;span class="kw">c&lt;/span>(&lt;span class="op">-&lt;/span>&lt;span class="dv">5&lt;/span>, &lt;span class="dv">5&lt;/span>),
&lt;span class="dt">breaks =&lt;/span> &lt;span class="kw">seq&lt;/span>(&lt;span class="op">-&lt;/span>&lt;span class="dv">5&lt;/span>, &lt;span class="dv">5&lt;/span>, &lt;span class="dv">1&lt;/span>),
&lt;span class="dt">labels =&lt;/span> &lt;span class="kw">abs&lt;/span>(&lt;span class="kw">seq&lt;/span>(&lt;span class="op">-&lt;/span>&lt;span class="dv">5&lt;/span>, &lt;span class="dv">5&lt;/span>, &lt;span class="dv">1&lt;/span>))) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">coord_flip&lt;/span>() &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">scale_fill_manual&lt;/span>(&lt;span class="dt">values =&lt;/span> &lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;quot;#E41A1C&amp;quot;&lt;/span>, &lt;span class="st">&amp;quot;#377EB8&amp;quot;&lt;/span>),
&lt;span class="dt">breaks =&lt;/span> &lt;span class="kw">c&lt;/span>(&lt;span class="st">&amp;quot;Female&amp;quot;&lt;/span>, &lt;span class="st">&amp;quot;Male&amp;quot;&lt;/span>)) &lt;span class="op">+&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">labs&lt;/span>(&lt;span class="dt">x =&lt;/span> &lt;span class="st">&amp;quot;Age&amp;quot;&lt;/span>, &lt;span class="dt">y =&lt;/span> &lt;span class="st">&amp;quot;Fluency&amp;quot;&lt;/span>, &lt;span class="dt">fill =&lt;/span> &lt;span class="st">&amp;quot;&amp;quot;&lt;/span>, &lt;span class="dt">title =&lt;/span> title)
}&lt;/code>&lt;/pre>
&lt;p>Now we can start plotting. Let’s try &lt;code>Mand&lt;/code> (Mandarin) and &lt;code>Hak&lt;/code> (Hakka) first. We can use &lt;code>dplyr::filter()&lt;/code> to filter out people speaking these languages:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">tweak &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">theme_bw&lt;/span>() &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">theme&lt;/span>(&lt;span class="dt">axis.text =&lt;/span> &lt;span class="kw">element_text&lt;/span>(&lt;span class="dt">size =&lt;/span> &lt;span class="dv">15&lt;/span>),
&lt;span class="dt">legend.justification =&lt;/span> &lt;span class="st">&amp;quot;right&amp;quot;&lt;/span>,
&lt;span class="dt">legend.position =&lt;/span> &lt;span class="st">&amp;quot;bottom&amp;quot;&lt;/span>,
&lt;span class="dt">legend.box =&lt;/span> &lt;span class="st">&amp;quot;vertical&amp;quot;&lt;/span>)
pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Mand&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Mandarin&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak
pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Ind&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Formosan Languages&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://img.yongfu.name/assets/visualize-language-loss/unnamed-chunk-12-1.svg" width="49%" />&lt;img src="https://img.yongfu.name/assets/visualize-language-loss/unnamed-chunk-12-2.svg" width="49%" />&lt;/p>
&lt;p>Wait! It seems quite strange. The age-sex pyramid of ‘Formosan Languages’ doesn’t look like a pyramid at all!
This is because there were very few subjects defined as ‘indigenous people’ in the survey. To make plots like this (with only one or two age groups) comparable to others (such as that in ‘Mandarin’), we need one more function to &lt;strong>insert missing age groups&lt;/strong> to the data frame so that ggplot can draw empty bars for us.&lt;/p>
&lt;p>First, we need to find out &lt;strong>all age groups in the data&lt;/strong>:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">age_groups &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">unique&lt;/span>(pl_data&lt;span class="op">$&lt;/span>age_group)
age_groups&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight"> [1] &amp;quot;[15,20)&amp;quot; &amp;quot;[20,25)&amp;quot; &amp;quot;[25,30)&amp;quot; &amp;quot;[30,35)&amp;quot; &amp;quot;[35,40)&amp;quot; &amp;quot;[40,45)&amp;quot; &amp;quot;[45,50)&amp;quot;
[8] &amp;quot;[50,55)&amp;quot; &amp;quot;[55,60)&amp;quot; &amp;quot;[60,65)&amp;quot; &amp;quot;[65,70)&amp;quot;&lt;/code>&lt;/pre>
&lt;p>Then we can write a function &lt;code>fill_empty_age_group()&lt;/code>, which takes a data frame as its first argument and checks whether there are age groups missing in the data frame (using its second argument, &lt;code>age_group_all&lt;/code>, as comparison). If the age group is missing, &lt;code>fill_empty_age_group()&lt;/code> appends a new row, which has &lt;code>age_group&lt;/code> set to the missing age group and &lt;code>avg_fluency&lt;/code> set to &lt;code>0&lt;/code> (create an empty bar), to the input data frame.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">fill_empty_age_group &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="cf">function&lt;/span>(df, age_group_all) {
&lt;span class="cf">for&lt;/span> (i &lt;span class="cf">in&lt;/span> &lt;span class="kw">seq_along&lt;/span>(age_group_all)) {
&lt;span class="cf">if&lt;/span> (&lt;span class="op">!&lt;/span>(age_group_all[i] &lt;span class="op">%in%&lt;/span>&lt;span class="st"> &lt;/span>df&lt;span class="op">$&lt;/span>age_group)) {
df &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">rbind&lt;/span>(df, &lt;span class="kw">list&lt;/span>(&lt;span class="dt">gender =&lt;/span> &lt;span class="st">&amp;quot;Female&amp;quot;&lt;/span>,
&lt;span class="dt">ethn_group =&lt;/span> &lt;span class="st">&amp;quot;doesnt_matter&amp;quot;&lt;/span>,
&lt;span class="dt">age_group =&lt;/span> age_group_all[i],
&lt;span class="dt">avg_fluency =&lt;/span> &lt;span class="dv">0&lt;/span>))
}
}
&lt;span class="kw">return&lt;/span>(df)
}&lt;/code>&lt;/pre>
&lt;p>Now we’re ready to explore language loss in Taiwan. &lt;code>multiplot()&lt;/code> is used to put multiple plots together. The source code of &lt;code>multiplot()&lt;/code> is copied directly from Winston Chang’s &lt;a href="http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)">Cookbook for R&lt;/a>.&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">tweak2 &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">theme_bw&lt;/span>() &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">theme&lt;/span>(&lt;span class="dt">legend.justification =&lt;/span> &lt;span class="st">&amp;quot;right&amp;quot;&lt;/span>,
&lt;span class="dt">legend.position =&lt;/span> &lt;span class="st">&amp;quot;bottom&amp;quot;&lt;/span>,
&lt;span class="dt">legend.box =&lt;/span> &lt;span class="st">&amp;quot;vertical&amp;quot;&lt;/span>)
py1 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Mand&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>&lt;span class="st"> &lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Mandarin&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
py2 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Tw&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Taiwanese&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
py3 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Hak&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Hakka&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
py4 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Ind&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Formosan Languages&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
py5 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;SEA&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;Languages of South East Asia&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
py6 &amp;lt;-&lt;span class="st"> &lt;/span>pl_data &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">filter&lt;/span>(ethn_group &lt;span class="op">==&lt;/span>&lt;span class="st"> &amp;#39;Eng&amp;#39;&lt;/span>) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">fill_empty_age_group&lt;/span>(age_groups) &lt;span class="op">%&amp;gt;%&lt;/span>
&lt;span class="st"> &lt;/span>&lt;span class="kw">pl_pyramid&lt;/span>(&lt;span class="dt">title =&lt;/span> &lt;span class="st">&amp;#39;English&amp;#39;&lt;/span>) &lt;span class="op">+&lt;/span>&lt;span class="st"> &lt;/span>tweak2
&lt;span class="kw">multiplot&lt;/span>(py1, py2, py3, py4, py5, py6, &lt;span class="dt">cols =&lt;/span> &lt;span class="dv">2&lt;/span>)&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://img.yongfu.name/assets/visualize-language-loss/unnamed-chunk-16-1.svg" width="100%" />&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="language-loss-in-taiwan" class="section level2">
&lt;h2>Language Loss in Taiwan&lt;/h2>
&lt;p>As described in &lt;a href="#age-sex-pyramid-of-language">Age-Sex Pyramid of Language&lt;/a> above, we can learn about a language’s vitality in Taiwan from the age-sex pyramids drawn above.&lt;/p>
&lt;p>For Formosan and South East Asian languages, there are too few data, and we can’t learn much about these languages from the plots. For Mandarin, the shape of the pyramid is rectangular, indicating the linguistic competence of Mandarin is stable across people of all ages. This is expected as Mandarin, in Taiwan, is the most prevalent language, and most people use it as the primary language in workplace and home.&lt;/p>
&lt;p>Pyramids of Taiwanese and Hakka have inverted trianglular shapes, indicating lower linguistic competence among yonger people. Indeed, it’s not uncommmon to see the fathers and mothers talk to their children in Mandarin but talk to their parents in Taiwanese. Taiwanese is the second prevalent language in Taiwan, but it is shrinking, particularly among young people. Hakka ranks third in prevalence. It faces similar situation to Taiwanese but the situation is even worse, since the usage of Hakka is mostly restricted to Hakka people whereas usage of Taiwanese is not restricted to particular groups of people (many indigenous and Hakka peoples speak fluent Taiwanese).&lt;/p>
&lt;p>English has a special status in Taiwan. It is not a native tongue to people born and raised in Taiwan, but many people know at least a little English. This is because formal education and the (awareness of) globalization lead parents to place importance on English education of their children. This also gives the age-sex pyramid of English its appearance – a triangular shape with wider bottom than top. English is the only language that is growing in Taiwan and, arguably, the language with the strongest vitality. &lt;a href="#tab:tw-pyramids">Table 1&lt;/a> summarizes the discussion above.&lt;/p>
&lt;table>
&lt;caption>&lt;span id="tab:tw-pyramids">Table 1: &lt;/span> Situations of the four most prevalent languages in Taiwan.&lt;/caption>
&lt;thead>
&lt;tr class="header">
&lt;th>&lt;strong>Vitality of Language&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Shape of Pyramid&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Examples (Taiwan)&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td>Shrinking and Dying&lt;/td>
&lt;td>Inverted Triangle&lt;/td>
&lt;td>Taiwanese, Hakka&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td>Growing&lt;/td>
&lt;td>Triangle&lt;/td>
&lt;td>English&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td>Stable&lt;/td>
&lt;td>Rectangular Bar&lt;/td>
&lt;td>Mandarin&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>The samples are not representative though, and it might only reflect the situation in Taipei (see the geographical distribution of the samples below). But I think there are still reasons to believe that other locations in Taiwan have similar phenomena.&lt;/p>
&lt;p>&lt;img src="https://twlangsurvey.github.io/out_graph/sample_spatial_distr.gif" title="fig:" alt="geo-distribution of samples" style="width: 43%;display:block; margin-left:auto;margin-right:auto" />&lt;a href="#fnref1" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Defined by scoring 3 or above in a self-reported 6-point scale measuring the fluency of a language spoken by the subject’s parents.&lt;a href="#fnref2" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>For English, different from all other languages, the level used to determine ethnicity is &lt;code>0&lt;/code>, i.e. there is no filtering occuring, and all subjects are used. This is because, in Taiwan (and many other non-English speaking communities as well), English is not related to ethnicity and is strongly related to formal education and job requirements.&lt;a href="#fnref3" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2019-04-03
&lt;/p></description><category>R</category><category>Travis-CI</category><category>Linguistics</category><category>R-bloggers</category></item><item><title>Inserting “Edit on GitHub” Buttons in a Single R Markdown Document</title><link>https://yongfu.name/2019/02/10/rmd_edit_btn/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://yongfu.name/2019/02/10/rmd_edit_btn/</guid><description>
&lt;p>As the R Markdown ecosystem becomes larger, users now may encounter situations where they have to make decisions on which output format of R Markdown to use.
One may found &lt;strong>none of the formats suitable&lt;/strong> – the features essential to the output document one wants may scatter across different output formats of R Markdown.&lt;/p>
&lt;p>Here is a real example I encountered. I wanted to create a document that:&lt;/p>
&lt;ol class="example" style="list-style-type: decimal">
&lt;li>supports &lt;a href="https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html">bookdown syntax&lt;/a>, e.g. text references&lt;/li>
&lt;li>has an “Edit on GitHub” button for every chapter that links to the edit page of the source &lt;code>.Rmd&lt;/code> on GitHub&lt;/li>
&lt;/ol>
&lt;p>The two features above can be obtained easily with bookdown’s default GitBook output format, but one more feature is essential to the document I want:&lt;/p>
&lt;ol start="3" class="example" style="list-style-type: decimal">
&lt;li>A document that supports &lt;a href="https://bookdown.org/yihui/rmarkdown/html-document.html#tabbed-sections">tabbed sections&lt;/a>&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>So basically, I wanted &lt;code>bookdown::gitbook&lt;/code> that supports tabbed sections, shown in Fig. &lt;a href="#fig:gitbook">1&lt;/a>. However, it’s not possible. This is a feature unique to &lt;code>rmarkdown::html_document&lt;/code>, &lt;a href="https://github.com/rstudio/bookdown/issues/393">not &lt;code>bookdown::gitbook&lt;/code>&lt;/a>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:gitbook">&lt;/span>
&lt;img src="https://yongfu.name/post_source/rmd_edit_btn/book_tabset.png" alt="`bookdown::gitbook` supports tabbed sections? This is just a fake figure." width="100%" />
&lt;p class="caption">
Figure 1: &lt;code>bookdown::gitbook&lt;/code> supports tabbed sections? This is just a fake figure.
&lt;/p>
&lt;/div>
&lt;p>Now I have to find a way, not provided by the default output formats of R Markdown, to create a document with the above three features.
My first thought was to find out how to add the tabbed sections feature to &lt;code>bookdown::gitbook&lt;/code> via JavaScript, but since I’m not familiar with JS, I gave up JavaScript and decided to use the “&lt;strong>native&lt;/strong>” R Markdown approach.
I turned to &lt;code>bookdown::html_document2&lt;/code>, which is based on &lt;code>rmarkdown::html_document&lt;/code> (supports tabbed sections).&lt;/p>
&lt;p>The &lt;a href="https://github.com/liao961120/parallelCode">source repo of my document&lt;/a> is a bookdown project, which has &lt;strong>several &lt;code>.Rmd&lt;/code> files&lt;/strong>.
Each &lt;code>.Rmd&lt;/code> file starts with a level-one heading and defines a single chapter. Since I wanted tabbed sections from &lt;code>rmarkdown::html_document&lt;/code>, I have to use &lt;code>bookdown::html_document2&lt;/code> as the output format, which creates a single HTML output file.&lt;/p>
&lt;div id="adding-to-level-one-headings" class="section level2">
&lt;h2>Adding &lt;img src='https://bit.ly/2RRirG7' alt='fa-edit' style='display:inline-block;height:1em;width:auto;margin-bottom:0'> to Level-one Headings&lt;/h2>
&lt;p>Setting &lt;code>bookdown::html_document2&lt;/code> as the output format creates a single document output (as opposed to &lt;code>bookdown::gitbook&lt;/code> which creates several HTML files by default),
so if I want to add an “Edit on GitHub” button for every chapter, I have to track the &lt;strong>original &lt;code>.Rmd&lt;/code> that generates the particular chapter&lt;/strong>.&lt;/p>
&lt;div id="merge-and-knit-vs.-knit-and-merge" class="section level3">
&lt;h3>“Merge and Knit” vs. “Knit and Merge”&lt;/h3>
&lt;p>There are &lt;a href="https://bookdown.org/yihui/bookdown/new-session.html">two rendering approaches&lt;/a> in bookdown. The default is &lt;strong>Merge and Knit&lt;/strong>, which combines all source &lt;code>.Rmd&lt;/code> files into one single &lt;code>.Rmd&lt;/code> file &lt;strong>then&lt;/strong> knits the document.
In this case, it would be impossible to track the source &lt;code>.Rmd&lt;/code> file for each chapter (unless I create a lookup table manually).&lt;/p>
&lt;p>I can track the source &lt;code>.Rmd&lt;/code> files easily, however, if the document is rendered using the &lt;strong>Knit and Merge&lt;/strong> approach. When using the &lt;strong>Knit and Merge&lt;/strong> approach, the code chunks in the source &lt;code>.Rmd&lt;/code> files are run and the results embedded &lt;strong>before&lt;/strong> the documents get combined together in a single output file. This mean that I can retrieve the source &lt;code>.Rmd&lt;/code> file name while knitting the file (by &lt;code>knitr::current_input()&lt;/code>). This gives me all I need to create the link to the edit page of the &lt;code>.Rmd&lt;/code> source file on GitHub.&lt;/p>
&lt;/div>
&lt;div id="bookdownyml" class="section level3">
&lt;h3>Setting up: &lt;code>_bookdown.yml&lt;/code>&lt;/h3>
&lt;p>To switch from the default “&lt;strong>Merge and Knit&lt;/strong>” to “&lt;strong>Knit and Merge&lt;/strong>”, set &lt;code>new_session: yes&lt;/code> in &lt;code>_bookdown.yml&lt;/code>:&lt;/p>
&lt;pre class="language-yaml">&lt;code class="language-yaml">&lt;span class="fu">new_session:&lt;/span>&lt;span class="at"> yes&lt;/span>
&lt;span class="fu">before_chapter_script:&lt;/span>&lt;span class="at"> &lt;/span>&lt;span class="st">&amp;#39;addons/pre_chap.R&amp;#39;&lt;/span>&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="setting-up-addonspre_chap.r" class="section level3">
&lt;h3>Setting up: &lt;code>addons/pre_chap.R&lt;/code>&lt;/h3>
&lt;p>To insert the link to the edit page on GitHub, put &lt;code>`r edit_btn`&lt;/code> (inline R code) at the end of the h1 heading of each &lt;code>.Rmd&lt;/code> file, for &lt;a href="https://github.com/liao961120/parallelCode/blob/4ea55dba03feef91ecf12e1014000fedabcc184b/00-functional_programming.Rmd#L1">example&lt;/a>:&lt;/p>
&lt;pre class="language-markdown">&lt;code class="language-markdown">&lt;span class="fu"># Function Factories `r edit_btn`&lt;/span>&lt;/code>&lt;/pre>
&lt;p>&lt;code>edit_btn&lt;/code> is a string variable holding the link&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> to GitHub. It is computed in the R script &lt;code>addons/pre_chap.R&lt;/code>, which is run every time before knitting a &lt;code>.Rmd&lt;/code> file:&lt;/p>
&lt;pre class="language-r">&lt;code class="language-r">url &amp;lt;-&lt;span class="st"> &amp;#39;https://github.com/liao961120/parallelCode/edit/master/&amp;#39;&lt;/span>
gh_edit_path &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">paste0&lt;/span>(url, knitr&lt;span class="op">::&lt;/span>&lt;span class="kw">current_input&lt;/span>())
edit_btn &amp;lt;-&lt;span class="st"> &lt;/span>&lt;span class="kw">paste0&lt;/span>(&lt;span class="st">&amp;#39;&amp;lt;a href=&amp;quot;&amp;#39;&lt;/span>, gh_edit_path, &lt;span class="st">&amp;#39;&amp;quot;&amp;gt;&amp;#39;&lt;/span>,
&lt;span class="st">&amp;#39;&amp;lt;img src=&amp;quot;https://bit.ly/2RRirG7&amp;quot; &amp;#39;&lt;/span>,
&lt;span class="st">&amp;#39;alt=&amp;quot;fa-edit&amp;quot; &amp;#39;&lt;/span>,
&lt;span class="st">&amp;#39;class=&amp;quot;edit&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;#39;&lt;/span>)&lt;/code>&lt;/pre>
&lt;p>To make &lt;code>addons/pre_chap.R&lt;/code> run every time before knitting a &lt;code>.Rmd&lt;/code> file, include it in the &lt;code>before_chapter_script&lt;/code> field in &lt;code>_bookdown.yml&lt;/code>, as shown in the section, &lt;a href="#bookdownyml">Setting up: &lt;code>_bookdown.yml&lt;/code>&lt;/a>, above.&lt;/p>
&lt;/div>
&lt;div id="setting-up-_output.yml" class="section level3">
&lt;h3>Setting up: &lt;code>_output.yml&lt;/code>&lt;/h3>
&lt;p>This is the output format I set in &lt;code>_output.yml&lt;/code>:&lt;/p>
&lt;pre class="language-yaml">&lt;code class="language-yaml">&lt;span class="fu">bookdown:&lt;/span>&lt;span class="at">:html_document2:&lt;/span>
&lt;span class="fu">theme:&lt;/span>&lt;span class="at"> readable&lt;/span>
&lt;span class="fu">highlight:&lt;/span>&lt;span class="at"> default&lt;/span>
&lt;span class="fu">toc:&lt;/span>&lt;span class="at"> true&lt;/span>
&lt;span class="fu">toc_depth:&lt;/span>&lt;span class="at"> 2&lt;/span>
&lt;span class="fu">toc_float:&lt;/span>
&lt;span class="fu">collapsed:&lt;/span>&lt;span class="at"> false&lt;/span>
&lt;span class="fu">css:&lt;/span>&lt;span class="at"> addons/style.css&lt;/span>
&lt;span class="fu">self_contained:&lt;/span>&lt;span class="at"> false&lt;/span>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div id="path-problems-caused-by-self_contained-false" class="section level2">
&lt;h2>Path Problems Caused by &lt;code>self_contained: false&lt;/code>&lt;/h2>
&lt;p>You may notice that I set &lt;code>self_contained: false&lt;/code> in &lt;code>_output.yml&lt;/code>. This isn’t necessary&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a> and it actually makes things a little more complicated, since, when using &lt;code>bookdown::html_document2&lt;/code> as the output format, bookdown generates all its dependencies (CSS, JS libraries, figures, etc) in &lt;code>_bookdown_files&lt;/code> and generates its ouput HTML file in the project root directory but doesn’t move the dependencies and the output HTML to the output directory (defaults to &lt;code>_book&lt;/code>) nor modify the relative path to dependency files in the output HTML. This causes the links in the output HTML to break.&lt;/p>
&lt;p>To fix this, I wrote a &lt;a href="https://github.com/liao961120/parallelCode/blob/master/build.sh">bash script&lt;/a> to move the output files&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a> to the correct places:&lt;/p>
&lt;pre class="language-bash">&lt;code class="language-bash">&lt;span class="fu">mv&lt;/span> out.html _bookdown_files/index.html
&lt;span class="kw">[[&lt;/span> &lt;span class="ot">-e&lt;/span> out.rds&lt;span class="kw"> ]]&lt;/span> &lt;span class="kw">&amp;amp;&amp;amp;&lt;/span> &lt;span class="fu">mv&lt;/span> out.rds _bookdown_files/
&lt;span class="fu">cp&lt;/span> -r addons/ _bookdown_files/
&lt;span class="kw">[[&lt;/span> &lt;span class="ot">-d&lt;/span> docs&lt;span class="kw"> ]]&lt;/span> &lt;span class="kw">&amp;amp;&amp;amp;&lt;/span> &lt;span class="fu">rm&lt;/span> -r docs
&lt;span class="fu">mv&lt;/span> _bookdown_files docs&lt;/code>&lt;/pre>
&lt;p>For users who don’t use bash, R can be used instead. Check out &lt;a href="http://theautomatic.net/2018/07/11/manipulate-files-r/">this post about file manipulation in R&lt;/a>.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>The reason I wanted this feature is that I’m writing a document that works like a &lt;a href="https://en.wikipedia.org/wiki/Parallel_text">parallel text&lt;/a>.&lt;/p>
&lt;p>I know three programming languages – R, Bash, and Python, but I’m only familiar with R, struggle with the weird syntax of Bash sometimes (that said, I like Bash pretty much, for its power to do quick and dirty works), and don’t use Python often enough to memorize commonly used syntax.&lt;/p>
&lt;p>By creating a &lt;a href="https://liao961120.github.io/parallelCode">parallel text for R, Bash, and Python&lt;/a>, I can write down the code to deal with some common tasks in these three langauges, so I don’t have to look it up on google every time I forgot the syntax of the langauge.&lt;/p>
&lt;p>I planned to write this document myself initially. But why not do a little more work to make it convenient for others to contribute to this document? This is why I decided to add an “Edit on GitHub” button at the start of every chapter of the document.&lt;a href="#fnref1" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>&lt;code>edit_btn&lt;/code> holds a &lt;code>&amp;lt;a&amp;gt;&lt;/code> tag with &lt;code>&amp;lt;img&amp;gt;&lt;/code> tag in it. The style of &lt;code>img.edit&lt;/code> is defined in &lt;a href="https://github.com/liao961120/parallelCode/blob/907a6c760e7b447bdd12074db785d89455e4009d/addons/style.css#L59-L69">&lt;code>addons/style.css&lt;/code>&lt;/a>. I didn’t use &lt;a href="https://fontawesome.com">fontawesome&lt;/a> for the edit icon &lt;img src='https://bit.ly/2RRirG7' alt='fa-edit' style='display:inline-block;height: 1em;width:auto;margin-bottom:0'>, since &lt;code>bookdown::html_document2&lt;/code> doesn’t support it and I don’t want to introduce another dependency. I use &lt;code>&amp;lt;img&amp;gt;&lt;/code> tag to source the edit icon from &lt;a href="https://commons.wikimedia.org/wiki/File:Edit_font_awesome.svg">wikimedia&lt;/a> instead.&lt;a href="#fnref2" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>I set &lt;code>self_contained&lt;/code> to &lt;code>false&lt;/code> because I didn’t want a large output file that takes too much time to load in the browser.&lt;a href="#fnref3" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>Note that I changed some default filenames by adding some lines in &lt;code>_bookdown.yml&lt;/code>:&lt;/p>
&lt;pre class="language-yaml">&lt;code class="language-yaml">&lt;span class="fu">book_filename:&lt;/span>&lt;span class="at"> &lt;/span>&lt;span class="st">&amp;quot;out&amp;quot;&lt;/span>&lt;span class="er"> # output HTML filename&lt;/span>
&lt;span class="fu">output_dir:&lt;/span>&lt;span class="at"> &lt;/span>&lt;span class="st">&amp;quot;docs&amp;quot;&lt;/span>&lt;span class="er"> # output dir, defaults to `_book`&lt;/span>&lt;/code>&lt;/pre>
&lt;a href="#fnref4" class="footnote-back">↩&lt;/a>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2019-02-10
&lt;/p></description><category>R</category><category>R Markdown</category><category>R-bloggers</category></item><item><title>Create a Glossary in R Markdown</title><link>https://yongfu.name/2018/10/24/glossary-maker/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/10/24/glossary-maker/</guid><description>
&lt;p>I was thinking about creating a glossary in &lt;a href="https://bookdown.org/yihui/bookdown/">bookdown&lt;/a> and found out that there was already an &lt;a href="https://github.com/rstudio/bookdown/issues/199">issue&lt;/a> about it. I like Yihui’s &lt;a href="https://github.com/rstudio/bookdown/issues/199#issuecomment-246888361">recommendation&lt;/a>: use Pandoc’s &lt;a href="http://pandoc.org/MANUAL.html#definition-lists">definition lists&lt;/a>. This was exactly what I had been doing, but I quickly found out that there was a major drawback – the definition lists &lt;strong>won’t order alphabetically unless written in that way&lt;/strong>.&lt;/p>
&lt;p>So I wrote an R function to reorder the definition lists written in R Markdown. Note that this functions &lt;strong>only works for R Markdown files containing defintion lists exclusively&lt;/strong>. If the R Markdown files aren’t whole-definition-lists, the function will fail.&lt;/p>
&lt;div id="usage" class="section level2">
&lt;h2>Usage&lt;/h2>
&lt;p>To order the definition lists alphabetically, simply put the Rmd file path in the function. To have a different output file, provide the output file path as the second argument.&lt;/p>
&lt;pre>&lt;code class="r">sort_def_list(&amp;quot;glossary.Rmd&amp;quot;)
# sort_def_list(&amp;quot;glossary.Rmd&amp;quot;, &amp;quot;reordered.Rmd&amp;quot;)&lt;/code>&lt;/pre>
&lt;p>The output in PDF looks like this (I used the &lt;code>multicol&lt;/code> package)&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>:&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/liao961120/blog/master/post_source/glossary-maker/glossary.png" />&lt;/p>
&lt;/div>
&lt;div id="source-code" class="section level2">
&lt;h2>Source Code&lt;/h2>
&lt;pre>&lt;code class="r">sort_def_list &amp;lt;- function(in_file, out_file = NULL) {
library(stringr)
library(dplyr)
data &amp;lt;- readLines(in_file)
# Extract, remove yaml header
yaml &amp;lt;- which(data == &amp;quot;---&amp;quot;)
head &amp;lt;- c(data[yaml[1]:yaml[2]], &amp;quot;\n&amp;quot;)
data &amp;lt;- data[(yaml[2]+1):length(data)]
# Indexing lines
def_start &amp;lt;- which(stringr::str_detect(data, &amp;quot;^: &amp;quot;)) - 1
def_end &amp;lt;- c(def_start[2:length(def_start)] - 1, length(data))
def_ranges &amp;lt;- dplyr::data_frame(term = data[def_start],
start = def_start,
end = def_end) %&amp;gt;%
dplyr::arrange(term) %&amp;gt;%
dplyr::mutate(new_start =
cumsum(
c(1, (end-start+1)[-length(term)])
)
) %&amp;gt;%
dplyr::mutate(new_end = new_start + (end-start))
# Create ordered definition list
data2 &amp;lt;- rep(NA, length(data))
for (i in seq_along(def_ranges$term)) {
start &amp;lt;- def_ranges$start[i]
end &amp;lt;- def_ranges$end[i]
n_start &amp;lt;- def_ranges$new_start[i]
n_end &amp;lt;- def_ranges$new_end[i]
data2[n_start:n_end] &amp;lt;- data[start:end]
}
# Rewrite rmd
if (is.null(out_file)) out_file &amp;lt;- in_file
data2 &amp;lt;- c(head, data2[!is.na(data2)])
writeLines(paste(data2, collapse = &amp;quot;\n&amp;quot;),
out_file)
}&lt;/code>&lt;/pre>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>To see the source R Markdown file, visit &lt;a href="https://github.com/liao961120/blog/tree/master/post_source/glossary-maker/glossary.rmd">&lt;code>glossary.rmd&lt;/code>&lt;/a>. To see the output PDF, visit &lt;a href="https://github.com/liao961120/blog/tree/master/post_source/glossary-maker/glossary.pdf">&lt;code>glossary.pdf&lt;/code>&lt;/a>&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
&lt;a href="https://www.r-bloggers.com/">Visit R-bloggers&lt;/a>&lt;br>
Last updated: 2020-02-13
&lt;/p></description><category>R</category><category>R Markdown</category><category>R-bloggers</category></item><item><title>Easy Linguistics Document Writing with R Markdown</title><link>https://yongfu.name/2018/09/09/linguistics-down/</link><pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/09/09/linguistics-down/</guid><description>
&lt;p>I’ve written &lt;a href="https://liao961120.github.io/2018/09/06/ipa-symbols.html">a post&lt;/a> about rendering IPA symbols properly regardless of the output format of the R Markdown document. I implemented the ideas into an R package, &lt;strong>linguisticsdown&lt;/strong>. &lt;/p>
&lt;p>&lt;strong>linguisticsdown&lt;/strong> provides a Shiny interface to facilitate inserting IPA symbols in R Markdown. See a quick demo of the current feature of &lt;strong>linguisticsdown&lt;/strong> in the gif at the end of the post.&lt;/p>
&lt;p>A &lt;a href="https://liao961120.shinyapps.io/IPA-Easily-Written/">live demo&lt;/a> is hosted on shinyapps.io. For more details, visit &lt;a href="https://liao961120.github.io/linguisticsdown/">&lt;strong>linguisticsdown&lt;/strong>&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://liao961120.github.io/linguisticsdown/man/figs/features.gif" />&lt;/p>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2019-03-07
&lt;/p></description><category>Linguistics</category><category>R Markdown</category><category>R</category><category>R-bloggers</category></item><item><title>Rendering IPA Symbols in R Markdown</title><link>https://yongfu.name/2018/09/06/ipa-symbols/</link><pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/09/06/ipa-symbols/</guid><description>
&lt;p>I was thinking about promoting reproducible research in Linguistics, or more precisely, how to attract people with no programming skills to have incentives to learn at least a bit programming, so that they have the ability to make their research more reproducible. &lt;/p>
&lt;p>I arrived at the solution: start by adopting R Markdown to write articles (see &lt;a href="#obstacles-to-adopting-a-reproducible-workflow">the last section&lt;/a> for details), but making R Markdown more friendly to novices in a particular field of academia is crucial to enhance their incentives to learn programming.&lt;/p>
&lt;div id="tasks-specific-to-linguistics" class="section level2">
&lt;h2>Tasks Specific to Linguistics&lt;/h2>
&lt;p>I came out with some common tasks related to document writing in Linguistics (I will thank you if you tell me other tasks I missed):&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Typing IPA symbols.&lt;/li>
&lt;li>Drawing syntax trees.&lt;/li>
&lt;/ol>
&lt;p>To enhance R Markdown’s ability to do these tasks without compromising one of it’s great feature: render nicely to both HTML and PDF with the same source, one needs to consider the incompatiblity of LaTeX and HTML code.&lt;/p>
&lt;p>Solving the first problem (IPA symbol) is easy, draing syntax trees is hard and I haven’t have a solution yet&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="typing-ipa-symbols" class="section level2">
&lt;h2>Typing IPA Symbols&lt;/h2>
&lt;p>There are two problems to be solved in order to facilitate using IPA symbols in R Markdown:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Input method&lt;/li>
&lt;li>Font support (only related to PDF output)&lt;/li>
&lt;/ol>
&lt;p>The first one is essentially about mapping some combination of keys to unicode strings. This post demenstates how to solve the second, which is more fundamental.&lt;/p>
&lt;p>After doing a little research, I came out with a quick solution which stems from the combination of &lt;a href="http://www.languagebits.com/phonetics-english/ipa-symbols-in-r/">IPA Symbols in R&lt;/a>, &lt;a href="https://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document">How do I use a particular font for a small section of text in my document?&lt;/a>, and &lt;a href="https://github.com/rstudio/bookdown/issues/168">Conditional compilation of book chunks to ensure compatibility with both HTML and XeLaTeX&lt;/a>.&lt;/p>
&lt;p>The solution is very simple: define a new font family that supports IPA symbols in LaTeX and use conditional compilation to render the document: when compiled to HTML, use raw unicode string; when compiled to PDF, wrap LaTeX code around IPA unicode strings.&lt;/p>
&lt;p>To define a new font family for IPA symbols, set &lt;code>header.tex&lt;/code> and include it by setting the yaml header of R Markdown document:&lt;/p>
&lt;pre class="yaml">&lt;code>output:
bookdown::pdf_document2:
includes:
in_header: header.tex&lt;/code>&lt;/pre>
&lt;p>Here’s &lt;code>header.tex&lt;/code>:&lt;/p>
&lt;pre class="latex">&lt;code>% Set font size
\usepackage[fontsize=12pt]{scrextend}
% Set font family
\usepackage{xeCJK}
\usepackage{fontspec}
\setmainfont{Calibri}
\setCJKmainfont[
BoldFont={HanWangHeiHeavy}
]{HanWangHeiLight}
% IPA font
\newfontfamily\ipa{Doulos SIL}
\DeclareTextFontCommand{\ipatext}{\ipa}&lt;/code>&lt;/pre>
&lt;p>The font, &lt;a href="https://software.sil.org/doulos/">Doulos SIL&lt;/a>, which supports IPA symbols can be freely dowloaded.&lt;/p>
&lt;p>The code chunk below is for conditional compilation:&lt;/p>
&lt;pre>&lt;code class="r">ipa &amp;lt;- c('e\u026A', 'a\u026A', '\u0254\u026A')
if (knitr::opts_knit$get('rmarkdown.pandoc.to') == &amp;quot;latex&amp;quot;) {
ipa &amp;lt;- paste0(&amp;quot;\\ipatext{&amp;quot;, ipa, &amp;quot;}&amp;quot;)
}&lt;/code>&lt;/pre>
&lt;p>The IPA symbols are set in the variable &lt;code>ipa&lt;/code> and can be access inline in R Markdown with, e.g., &lt;code>r ipa&lt;/code> or &lt;code>r ipa[3]&lt;/code>, which renders to &lt;strong>eɪ, aɪ, ɔɪ&lt;/strong> and &lt;strong>ɔɪ&lt;/strong>, respectively.&lt;/p>
&lt;p>The source of this post is in my &lt;a href="https://github.com/liao961120/blog/tree/master/post_source/ipa-symbols">GitHub repo&lt;/a>. You can reproduce it locally to see the differnce between HTML and PDF output of this post.&lt;/p>
&lt;/div>
&lt;div id="obstacles-to-adopting-a-reproducible-workflow" class="section level2">
&lt;h2>Obstacles to Adopting a Reproducible Workflow&lt;/h2>
&lt;p>&lt;em>Skip this section if you’re tired of stuff about reproducibility and R Markdown.&lt;/em>&lt;/p>
&lt;p>Reproducible research not only enhance scientific progress but also saves researchers a great deal of time, by automating repetitive and error-prone tasks in research. So if there are good reasons to adopt a reproducible workflow in research, saving time (in the long run) might be a good one.&lt;/p>
&lt;p>Programming skill is fundamental to automating repetitive tasks, which saves one’s time. However, learning programming to save time makes no sense to many people, since it is terrifying, hard, and time consumming&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. So the problem now becomes:&lt;/p>
&lt;blockquote>
&lt;p>How to reinforce the incentive to learn programming?&lt;/p>
&lt;/blockquote>
&lt;p>Again, by showing people how to save time, but this time, programming skill is not required.&lt;/p>
&lt;p>I think R Markdown is a very promising starting point, since writing is necessary for researcheres, and one can use RStudio without any knowledge of R. When becoming familiar with R Markdown, one begins to adopt a reproducible workflow and might notice the capability of R language, hence gaining more incentive to learn R.&lt;/p>
&lt;p>Many people in academia uses Microsoft Word to write articles and papers. However, R Markdown has several advantages over MS Word:&lt;/p>
&lt;ul>
&lt;li>Easy to inserting images and tables in documents.&lt;/li>
&lt;li>Values of variables (e.g. values in tables or &lt;em>p&lt;/em>-values) are automatically updated when raw data changes.&lt;/li>
&lt;li>Easy citation using citation keys (&lt;a href="https://www.zotero.org/">Zotero&lt;/a> + &lt;a href="https://github.com/retorquere/zotero-better-bibtex">Better BibTeX&lt;/a> greatly facilitates this).&lt;/li>
&lt;li>Mutiple output format, e.g. LaTeX, PDF, Web Page, Book, etc.&lt;/li>
&lt;li>&lt;a href="https://github.com/rstudio/rticles">Template support&lt;/a> for Journel articles, such as Elsevier, Sage, Springer, so no formatting is needed.&lt;/li>
&lt;/ul>
&lt;p>But I think all benefits about R Markdown mentioned above aren’t enough to persuade people into giving up MS Word, since people are conservative in adoping new things.&lt;/p>
&lt;p>If using R Markdown (or R) has benefits specific to the field related to the researcher, it greatly enhances the chance of adopting R Markdown. Hence, if I want to persuade people to use R Markdown, I can first build R packages that enhances the ability of R Markdown in that field.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>There are LaTeX packages supporting drawing syntax tree, but LaTeX package is not compatible with HTML output.&lt;a href="#fnref1" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>I actually stared and gave up learning programming languages three times (C++, C, and then Python) before I successfully learned R.&lt;a href="#fnref2" class="footnote-back">↩&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2019-03-07
&lt;/p></description><category>Linguistics</category><category>R Markdown</category><category>R</category><category>R-bloggers</category></item><item><title>jieba 自訂詞庫斷詞</title><link>https://yongfu.name/2018/07/31/jieba-dict/</link><pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/07/31/jieba-dict/</guid><description>&lt;p>這邊將使用 &lt;a href="https://github.com/qinwf/jiebaR">jiebaR&lt;/a>，介紹使用自訂詞庫的斷詞方式，並提供自訂詞庫的製作方式。&lt;/p>
&lt;div class="section level2">
&lt;h2>示範語料&lt;/h2>
&lt;p>這裡使用金庸&lt;strong>&lt;a href="https://zh.wikipedia.org/wiki/%E7%A5%9E%E9%B5%B0%E4%BF%A0%E4%BE%B6">神雕俠侶&lt;/a>第三十二回 — 情是何物&lt;/strong>作為斷詞的文本。武俠小說在此是個很好的例子，因為裡面有許多人物名稱和專有名詞。&lt;/p>
&lt;p>因為著作權問題&lt;a href="#fn1" class="footnoteRef" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>，語料的原始檔(&lt;code>032.txt&lt;/code>)將不會出現在本文的 &lt;a href="https://github.com/liao961120/blog/tree/master/post_source/jieba-dict">GitHub repo&lt;/a> 中。&lt;/p>
&lt;/div>
&lt;div class="section level2">
&lt;h2>製作自訂詞庫&lt;/h2>
&lt;p>取得小說這類文本的角色名稱與特殊名詞乍看之下可能非常耗工耗時，但有些時候其實相當容易，尤其是著名的小說。這要歸功於&lt;a href="https://en.wikipedia.org/wiki/Main_Page">維基百科&lt;/a>，因為越是著名的小說，其越有可能有詳盡的維基百科頁面，而&lt;strong>維基百科對製作詞庫最重要的特色在於其頁面的超連結&lt;/strong>，因為通常只有&lt;strong>專有名詞才會成為一個維基頁面上的超連結&lt;/strong>。&lt;/p>
&lt;p>這邊使用維基百科的&lt;a href="https://zh.wikipedia.org/wiki/%E7%A5%9E%E9%B5%B0%E4%BF%A0%E4%BE%B6%E8%A7%92%E8%89%B2%E5%88%97%E8%A1%A8">神鵰俠侶角色列表&lt;/a>作為詞庫的來源。以下使用&lt;code>rvest&lt;/code>套件清理此頁面：&lt;/p>
&lt;pre>&lt;code class="r">library(rvest)
library(dplyr)
library(magrittr)
library(knitr)
path &amp;lt;- &amp;quot;神鵰俠侶角色列表.html&amp;quot;
# 這裡已先行下載網頁，若無可直接使用網址
data &amp;lt;- read_html(path) %&amp;gt;%
html_nodes(&amp;quot;ul&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;li&amp;quot;) %&amp;gt;%
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text()&lt;/code>&lt;/pre>
&lt;p>觀察頁面後，可發現多數與小說相關的詞彙都位在 unordered list 下的連結內文(&amp;lt;a&amp;gt; tag)，因此透過 3 個&lt;code>html_nodes()&lt;/code>取得連結，並用&lt;code>html_text()&lt;/code>擷取連結內文。&lt;/p>
&lt;p>接著看看擷取的詞彙，可以發現這些詞彙依照順序大致可區分成三個來源：&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>自維基頁面的&lt;strong>目錄&lt;/strong>擷取之連結&lt;/li>
&lt;li>內文的連結(這是我們要的)&lt;/li>
&lt;li>其它連結
&lt;ul>
&lt;li>對應至頁面最下方，與小說有關但並非小說主要內容的連結，如，「射雕英雄传角色列表」。另外，也包含維基百科頁面的固定連結，如「編輯」、「討論」、「下載為PDF」等。&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="r">data &amp;lt;- unique(data)
data[1:3]&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;1 主角&amp;quot; &amp;quot;2 桃花島&amp;quot; &amp;quot;2.1 「北丐」門派&amp;quot;&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">data[21:25]&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;楊過&amp;quot; &amp;quot;射鵰英雄傳&amp;quot; &amp;quot;楊康&amp;quot; &amp;quot;穆念慈&amp;quot; &amp;quot;全真教&amp;quot; &lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">data[207:211]&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;射雕英雄传角色列表&amp;quot; &amp;quot;倚天屠龙记角色列表&amp;quot; &amp;quot;查&amp;quot;
[4] &amp;quot;论&amp;quot; &amp;quot;编&amp;quot; &lt;/code>&lt;/pre>
&lt;p>我們要的內容介在&lt;code>data[21]&lt;/code>(楊過)至&lt;code>data[206]&lt;/code>(樊一翁)之間。此外，亦可手動加入連結中沒有的詞彙：&lt;/p>
&lt;pre>&lt;code class="r">data &amp;lt;- as_data_frame(data[21:206]) %&amp;gt;%
rbind(&amp;quot;過兒&amp;quot;, &amp;quot;靖哥哥&amp;quot;) # 手動額外輸入
head(data, 4) %&amp;gt;% kable(&amp;quot;markdown&amp;quot;, align=&amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">楊過&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">射鵰英雄傳&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">楊康&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">穆念慈&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>最後，將&lt;code>data&lt;/code>存成&lt;code>.csv&lt;/code>檔，方便未來使用：&lt;/p>
&lt;pre>&lt;code class="r">readr::write_csv(data, &amp;quot;sdxl_wordlist.csv&amp;quot;)&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="jiebar-" class="section level2">
&lt;h2>jiebaR 斷詞&lt;/h2>
&lt;p>準備好自訂詞庫後，要開始對文本進行斷詞。&lt;/p>
&lt;p>jiebaR 斷詞可以選擇外來檔案或將檔案讀入後在進行斷詞，這邊將文本檔案讀入再斷詞：&lt;/p>
&lt;pre>&lt;code class="r">library(stringr)
raw_text &amp;lt;- readr::read_file(&amp;quot;032.txt&amp;quot;)
raw_text %&amp;gt;% str_trunc(80)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;第三十二回　情是何物\r\n\r\n　　當黃蓉、一燈、郭芙等被困大廳之時，楊過和小龍女正在花前並肩共語。不久程英和陸無雙到來。小龍女見程英溫雅靦腆，甚是投緣，拉住...&amp;quot;&lt;/code>&lt;/pre>
&lt;div class="section level3">
&lt;h3>無自訂詞庫&lt;/h3>
&lt;p>首先，我們可以看看沒有自訂詞庫的斷詞效果：&lt;/p>
&lt;pre>&lt;code class="r">library(jiebaR)
stop_words &amp;lt;- readr::read_table2(&amp;quot;stop-zh-tw-withpunc&amp;quot;,
col_names = F) %&amp;gt;%
rbind(&amp;quot;\n&amp;quot;, &amp;quot;\r&amp;quot;) %&amp;gt;%
set_names(&amp;quot;word&amp;quot;)
seg &amp;lt;- worker(bylines = F, symbol = T)
segment(raw_text, seg) %&amp;gt;%
as_data_frame() %&amp;gt;%
anti_join(stop_words, by=c(&amp;quot;value&amp;quot;=&amp;quot;word&amp;quot;)) %&amp;gt;%
count(value) %&amp;gt;%
arrange(desc(n)) %&amp;gt;%
head() %&amp;gt;% kable(&amp;quot;markdown&amp;quot;, align=&amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">value&lt;/th>
&lt;th align="center">n&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">道&lt;/td>
&lt;td align="center">156&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">小龍女&lt;/td>
&lt;td align="center">115&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">楊&lt;/td>
&lt;td align="center">91&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">公孫止&lt;/td>
&lt;td align="center">86&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">楊過&lt;/td>
&lt;td align="center">76&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">黃&lt;/td>
&lt;td align="center">65&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到有些斷詞是正確的，如「公孫止」。但某些似乎常常斷錯，例如，「黃蓉」、「楊過」(某些似乎斷錯，導致有許多單獨的「楊」)。&lt;/p>
&lt;/div>
&lt;div class="section level3">
&lt;h3>使用自訂詞庫&lt;/h3>
&lt;p>在&lt;code>jiebaR::worker()&lt;/code>中設定自訂詞庫的位置：&lt;code>user = &amp;quot;sdxl_wordlist.csv&amp;quot;&lt;/code>，即可在斷詞系統中新增字典：&lt;/p>
&lt;pre>&lt;code class="r">seg &amp;lt;- worker(bylines = F, symbol = T,
user = &amp;quot;sdxl_wordlist.csv&amp;quot;)
segment(raw_text, seg) %&amp;gt;%
as_data_frame() %&amp;gt;%
anti_join(stop_words, by=c(&amp;quot;value&amp;quot;=&amp;quot;word&amp;quot;)) %&amp;gt;%
count(value) %&amp;gt;%
arrange(desc(n)) %&amp;gt;%
head() %&amp;gt;% kable(&amp;quot;markdown&amp;quot;, align=&amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">value&lt;/th>
&lt;th align="center">n&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">楊過&lt;/td>
&lt;td align="center">187&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">道&lt;/td>
&lt;td align="center">150&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">小龍女&lt;/td>
&lt;td align="center">119&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">公孫止&lt;/td>
&lt;td align="center">104&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">黃蓉&lt;/td>
&lt;td align="center">95&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">李莫愁&lt;/td>
&lt;td align="center">59&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到使用自訂詞庫後，斷詞變得有意義多了。&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>本文目的僅在促進教育與學術，並無營利企圖。且本文僅顯示極少的小說內容，應屬合理使用。若有侵犯著作權的疑慮，麻煩透過 &lt;a href="mailto:liaomovie2@gmail.com">Email&lt;/a> 與我聯絡。&lt;a href="#fnref1">↩&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2018-11-10
&lt;/p></description><category>R</category><category>linguistics</category><category>中文</category></item><item><title>Text Mining 前處理</title><link>https://yongfu.name/2018/07/28/quanteda-tutorial/</link><pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/07/28/quanteda-tutorial/</guid><description>&lt;div class="section level2">
&lt;h2>流程&lt;/h2>
&lt;img src="https://img.yongfu.name/blog/mermaid2.svg" alt="" style="width:100%">
&lt;div class="mermaid">
graph LR
html("HTML")
html -.->|"rvest"| df0
subgraph 前處理
df1("斷詞 data_frame")
df0("data_frame")
df0 -.->|"&lt;br> jiebaR &lt;br> (保留標點)&lt;br>"| df1
df1 -.->|"ropencc &lt;br> 簡轉繁"| df1
end
corp("Corpus")
token("Tokens")
subgraph quanteda
df1 -.->|"quanteda &lt;br> corpus()"| corp
corp -.->|"quanteda &lt;br> tokenize()"| token
end
html -.- bls(" ")
style bls fill:none,stroke:none
style html fill:#ccbdb9
style df1 fill:#92ff7f
linkStyle 5 stroke-width:0px,fill:none;
&lt;/div>
&lt;/div>
&lt;div class="section level2">
&lt;h2>資料爬取&lt;/h2>
&lt;p>這邊使用 &lt;a href="https://www.rstudio.com/">RStudio&lt;/a> 軟體工程師 &lt;a href="https://yihui.name/en/about/">Yihui&lt;/a> 的&lt;a href="https://yihui.name/cn/">中文部落格&lt;/a>文章作為練習素材。首先需要取得文章的網址，因此先到部落格的文章列表頁面(&lt;a href="https://yihui.name/cn/" class="uri">https://yihui.name/cn/&lt;/a>)，使用瀏覽器的&lt;a href="https://developers.google.com/web/tools/chrome-devtools/?hl=zh-tw">開發者工具&lt;/a>(按&lt;code>Ctrl + Shift + I&lt;/code>開啟)進行&lt;strong>觀察&lt;/strong>。&lt;/p>
&lt;p>接著使用&lt;a href="https://github.com/hadley/rvest">&lt;code>rvest&lt;/code>&lt;/a>套件擷取網頁中所有文章的連結，並將文章網址儲存成&lt;code>list_of_post.txt&lt;/code>：&lt;/p>
&lt;pre>&lt;code class="r">library(dplyr)
library(rvest)
list_of_posts &amp;lt;- read_html(&amp;quot;https://yihui.name/cn/&amp;quot;) %&amp;gt;%
html_nodes(&amp;quot;.archive&amp;quot;) %&amp;gt;% # 列表在 div.archive 之下
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% # 文章標題在 &amp;lt;div&amp;gt; 下之 &amp;lt;p&amp;gt;
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) # 文章連結在 &amp;lt;p&amp;gt; 下之 &amp;lt;a&amp;gt;
readr::write_lines(list_of_posts, &amp;quot;yihui/list_of_post.txt&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">head(list_of_posts, 2)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;/cn/2018/10/middle-school-teachers/&amp;quot;
[2] &amp;quot;/cn/2018/10/potato-pancake/&amp;quot; &lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">tail(list_of_posts, 2)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;/cn/2005/01/rtx/&amp;quot; &amp;quot;/cn/2005/01/20-13-00/&amp;quot;&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">length(list_of_posts)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] 1097&lt;/code>&lt;/pre>
&lt;p>可以看到總共有 1097 篇文章，時間從 2005 年到今年七月都有發文的紀錄。&lt;/p>
&lt;p>由於文章數量相當多，因此之後僅會下載部分文章，&lt;strong>避免造成伺服器負擔過大&lt;/strong>。下載網頁時，可以在 R 中直接使用&lt;code>rvest&lt;/code>(見下文&lt;strong>資料前處理&lt;/strong>)，但我比較建議使用 Bash&lt;a href="#fn1" class="footnoteRef" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>的&lt;code>wget&lt;/code>指令，才不會因為重複下載網頁造成伺服器負擔。&lt;/p>
&lt;p>在下載前，需先決定目標文章的網址&lt;code>sub_list&lt;/code>：&lt;/p>
&lt;pre>&lt;code class="r">library(stringr)
set.seed(2018) # 設隨機種子 固定隨機函數的結果
idx &amp;lt;- str_detect(list_of_posts, &amp;quot;2018|2015|2010&amp;quot;)
sub_list &amp;lt;- list_of_posts[idx]
sub_list &amp;lt;- sub_list[sample(seq_along(sub_list), 20)] %&amp;gt;% # 抽出 20 篇
str_replace_all(pattern = &amp;quot;^/&amp;quot;, # 將站內連結改為完整 url
replacement = &amp;quot;https://yihui.name/&amp;quot;) %&amp;gt;%
str_replace_all(pattern = &amp;quot;/$&amp;quot;, &amp;quot;/index.html&amp;quot;)
readr::write_lines(sub_list, &amp;quot;yihui/sublist.txt&amp;quot;)
# 給 Bash 用的
sub_list %&amp;gt;%
str_replace_all(&amp;quot;https://yihui.name/cn/&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
str_replace_all(&amp;quot;/index.html&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
str_replace_all(&amp;quot;/&amp;quot;, &amp;quot;-&amp;quot;) %&amp;gt;%
str_replace_all(&amp;quot;-$&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
readr::write_lines(&amp;quot;yihui/sublist_name.txt&amp;quot;)&lt;/code>&lt;/pre>
&lt;div id="bash-" class="section level3">
&lt;h3>Bash 指令下載網頁&lt;/h3>
&lt;blockquote>
&lt;p>無法使用 bash 指令者，可跳過此節&lt;/p>
&lt;/blockquote>
&lt;p>為了自動化下載網頁，我寫了一個簡單的 Bash script &lt;code>wget_list&lt;/code>，用法如下:&lt;/p>
&lt;ul>
&lt;li>&lt;code>wget_list &amp;lt;網址文字檔&amp;gt; &amp;lt;檔名文字檔&amp;gt;&lt;/code>&lt;a href="#fn2" class="footnoteRef" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>
&lt;ul>
&lt;li>&lt;code>&amp;lt;網址文字檔&amp;gt;&lt;/code>： 每一列(row)由一個網址組成&lt;/li>
&lt;li>&lt;code>&amp;lt;檔名文字檔&amp;gt;&lt;/code>： 每一列由一個名稱組成，每個名稱與&lt;code>&amp;lt;網址文字檔&amp;gt;&lt;/code>的網址對應&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;!--FOOTNOTE START-->
&lt;p>在這裡，執行下列指令即可下載網頁&lt;/p>
&lt;pre>&lt;code class="bash">cd yihui/html
wget_list ../sublist.txt ../sublist_name.txt
cd -&lt;/code>&lt;/pre>
&lt;p>&lt;strong>&lt;code>wget_list&lt;/code>&lt;/strong>:&lt;/p>
&lt;pre>&lt;code class="bash">#!/bin/bash
#&amp;lt;&amp;lt;&amp;lt; wget_list: dowload webpages listed in a file &amp;gt;&amp;gt;&amp;gt;#
### Argument 1 is the file of links, 1 url per row ###
### Argument 2 is the file of names, 1 name per row ###
file1=$1
file2=$2
## Get the number of lines in the link list
num_lines=$(wc -l $file1 | egrep -o '^[0-9]*')
## loop over the lines in file1, dowload the the file &amp;amp; name them as listed in file2
for (( i=1; i&amp;lt;=${num_lines}; ++i )); do
wget &amp;quot;$(sed -n ${i}p $file1)&amp;quot; \
-O &amp;quot;$(sed -n ${i}p $file2)&amp;quot;
done&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="section level2">
&lt;h2>資料前處理&lt;/h2>
&lt;p>在清理資料之前，需先剖析網頁結構(就如同之前剖析文章列表頁面一樣)。 這邊觀察&lt;a href="https://yihui.name/cn/2015/11/peer-review/">這篇文章&lt;/a>，大致可以找出這些資訊：&lt;/p>
&lt;pre>&lt;code class="r">path &amp;lt;- &amp;quot;https://yihui.name/cn/2015/11/peer-review/&amp;quot;
all &amp;lt;- read_html(path) %&amp;gt;%
html_nodes(&amp;quot;article&amp;quot;)
header &amp;lt;- all %&amp;gt;% html_nodes(&amp;quot;header&amp;quot;)
title &amp;lt;- header %&amp;gt;% # 文章標題
html_nodes(&amp;quot;h1&amp;quot;) %&amp;gt;% html_text()
post_date &amp;lt;- header %&amp;gt;% # 發文日期
html_node(&amp;quot;h3&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
str_extract(&amp;quot;201[0-9]-[0-9]{2}-[0-9]{2}&amp;quot;)
article &amp;lt;- all %&amp;gt;% # 內文
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
html_text() %&amp;gt;% paste(collapse = &amp;quot;\n&amp;quot;)
# 這裡將 chr vector collapse 至 1 個字串，
# 簡化資料結構，並以分行符號保留段落資訊
num_sec &amp;lt;- all %&amp;gt;% # 內文段落數
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% length
links &amp;lt;- all %&amp;gt;% html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% # 內文連結
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;)
link_text &amp;lt;- all %&amp;gt;% html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% # 內文連結標題
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text()&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">library(tibble)
df &amp;lt;- data_frame(title = title,
date = post_date,
content = article,
num_sec = num_sec,
links = list(links),
link_text = list(link_text)
)
df %&amp;gt;%
mutate(title = str_trunc(title, 8),
content = str_trunc(content, 8),
links = str_trunc(links, 8),
link_text = str_trunc(link_text, 8)) %&amp;gt;%
kable(&amp;quot;markdown&amp;quot;, align = &amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">title&lt;/th>
&lt;th align="center">date&lt;/th>
&lt;th align="center">content&lt;/th>
&lt;th align="center">num_sec&lt;/th>
&lt;th align="center">links&lt;/th>
&lt;th align="center">link_text&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">同行评审&lt;/td>
&lt;td align="center">2015-11-11&lt;/td>
&lt;td align="center">看到这么一…&lt;/td>
&lt;td align="center">8&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“一则…&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我們可以將上面的程式碼改寫成函數&lt;code>post_data()&lt;/code>，自動讀取文章並輸出 data frame：&lt;/p>
&lt;pre>&lt;code class="r">post_data &amp;lt;- function (path) {
all &amp;lt;- read_html(path) %&amp;gt;%
html_nodes(&amp;quot;article&amp;quot;)
header &amp;lt;- all %&amp;gt;% html_nodes(&amp;quot;header&amp;quot;)
title &amp;lt;- header %&amp;gt;% # 文章標題
html_nodes(&amp;quot;h1&amp;quot;) %&amp;gt;% html_text()
post_date &amp;lt;- header %&amp;gt;% # 發文日期
html_node(&amp;quot;h3&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
str_extract(&amp;quot;201[0-9]-[0-9]{2}-[0-9]{2}&amp;quot;)
article &amp;lt;- all %&amp;gt;% # 內文
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
html_text() %&amp;gt;% paste(collapse = &amp;quot;\n&amp;quot;)
# 這裡將 chr vector collapse 至 1 個字串，
# 簡化資料結構，並以分行符號保留段落資訊
num_sec &amp;lt;- all %&amp;gt;% # 內文段落數
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% length
links &amp;lt;- all %&amp;gt;% html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;% # 內文連結
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;)
link_text &amp;lt;- all %&amp;gt;% # 內文連結標題
html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text()
df &amp;lt;- tibble::data_frame(title = title,
date = post_date,
content = article,
num_sec = num_sec,
links = list(links),
link_text = list(link_text)
)
}&lt;/code>&lt;/pre>
&lt;p>接著，將所有文章讀取至一個 data frame &lt;code>all_post&lt;/code>：&lt;/p>
&lt;pre>&lt;code class="r">library(dplyr)
library(tidyr)
html_list &amp;lt;- list.files(&amp;quot;yihui/html/&amp;quot;) # 列出資料夾下的檔案
all_post &amp;lt;- vector(&amp;quot;list&amp;quot;, length(html_list))
for (i in seq_along(html_list)) {
path &amp;lt;- paste0(&amp;quot;yihui/html/&amp;quot;, html_list[i])
all_post[[i]] &amp;lt;- post_data(path)
}
all_post &amp;lt;- bind_rows(all_post) %&amp;gt;% arrange(desc(date))
head(all_post) %&amp;gt;%
mutate(title = str_trunc(title, 8),
content = str_trunc(content, 8),
links = str_trunc(links, 8),
link_text = str_trunc(link_text, 8)) %&amp;gt;%
kable(&amp;quot;markdown&amp;quot;, align = &amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">title&lt;/th>
&lt;th align="center">date&lt;/th>
&lt;th align="center">content&lt;/th>
&lt;th align="center">num_sec&lt;/th>
&lt;th align="center">links&lt;/th>
&lt;th align="center">link_text&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">修辞还是真实&lt;/td>
&lt;td align="center">2018-06-21&lt;/td>
&lt;td align="center">说两封让我…&lt;/td>
&lt;td align="center">12&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">花椒香料&lt;/td>
&lt;td align="center">2018-05-31&lt;/td>
&lt;td align="center">古人似乎喜…&lt;/td>
&lt;td align="center">2&lt;/td>
&lt;td align="center">/cn/2…&lt;/td>
&lt;td align="center">去年的花椒&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">CSS 的…&lt;/td>
&lt;td align="center">2018-05-14&lt;/td>
&lt;td align="center">CSS 中…&lt;/td>
&lt;td align="center">15&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“查阅…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">毛姆的文学回忆录&lt;/td>
&lt;td align="center">2018-05-04&lt;/td>
&lt;td align="center">前段时间看…&lt;/td>
&lt;td align="center">14&lt;/td>
&lt;td align="center">c(“/c…&lt;/td>
&lt;td align="center">c(“职业…&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">距离的组织&lt;/td>
&lt;td align="center">2018-05-03&lt;/td>
&lt;td align="center">前面《闲情…&lt;/td>
&lt;td align="center">5&lt;/td>
&lt;td align="center">/cn/2…&lt;/td>
&lt;td align="center">闲情赋&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">语言圣战的终结？&lt;/td>
&lt;td align="center">2018-04-19&lt;/td>
&lt;td align="center">一直以来我…&lt;/td>
&lt;td align="center">3&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“惊天…&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;div class="section level3">
&lt;h3>直接從網路讀取&lt;/h3>
&lt;p>如果無法使用 Bash 指令下載網頁，可將上面程式碼的&lt;code>html_list&lt;/code>改為讀取&lt;code>sublist.txt&lt;/code>中的 url，並修改&lt;code>for&lt;/code>迴圈中的&lt;code>path&lt;/code>：&lt;/p>
&lt;pre>&lt;code class="r">html_list &amp;lt;- read_lines(&amp;quot;yihui/sublist.txt&amp;quot;) # 讀取 url
all_post &amp;lt;- vector(&amp;quot;list&amp;quot;, length(html_list))
for (i in seq_along(html_list)) {
path &amp;lt;- html_list[i]
all_post[[i]] &amp;lt;- post_data(path)
}
all_post &amp;lt;- bind_rows(all_post) %&amp;gt;% arrange(desc(date))&lt;/code>&lt;/pre>
&lt;/div>
&lt;div class="section level3">
&lt;h3>斷詞&lt;/h3>
&lt;p>在處理中文、日語等文本資料，需先經過斷詞處理，因為其不像英語等歐洲語言的文本，以空格表示字詞的界線。&lt;/p>
&lt;p>我們將使用&lt;code>jiebaR&lt;/code>套件的&lt;code>segment()&lt;/code>進行斷詞。由&lt;code>?segment()&lt;/code>查看其 documentation 可知&lt;strong>&lt;code>segment()&lt;/code>只吃文字檔或 一個句子&lt;/strong>，因此需先搞清楚&lt;code>all_post&lt;/code>的結構才能進行斷詞：&lt;/p>
&lt;p>&lt;code>all_post&lt;/code>: 20*5 的&lt;code>data_frame&lt;/code>，每列(row)為一篇文章 - $title: 每列為 1 個值 - $date: 每列為 1 個值 - $content: 每列為 1 個值，段落資訊藏在字串中的&lt;code>\n&lt;/code>符號 - $links: 每列為 1 個 list - $link_text: 每列為 1 個 list&lt;/p>
&lt;p>&lt;code>all_post$content&lt;/code>的結構相當簡單(一篇文章一個字串)，因此不須經過額外處理。其它變項不須斷詞處理，因此在此不加細談。&lt;/p>
&lt;div id="jiebarsegment" class="section level4">
&lt;h4>jiebaR::segment&lt;/h4>
&lt;p>因為&lt;code>all_post$content&lt;/code>簡單的結構符合&lt;code>jiebaR&lt;/code>套件的預設需求，但有時資料會比較複雜，因此記錄下來供未來參考。&lt;/p>
&lt;p>前面提到&lt;code>jiebaR::segment&lt;/code>只吃一個句子(一個字串)或文字檔，那如果丟一個 vector 給它會怎樣？答案是看&lt;code>worker()&lt;/code>的設定：&lt;/p>
&lt;pre>&lt;code class="r">library(jiebaR)
seg &amp;lt;- worker(symbol = T, bylines = F)
segment(c(&amp;quot;妳很漂亮&amp;quot;, &amp;quot;我不喜歡你&amp;quot;), seg)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;妳&amp;quot; &amp;quot;很漂亮&amp;quot; &amp;quot; &amp;quot; &amp;quot;我&amp;quot; &amp;quot;不&amp;quot; &amp;quot;喜歡&amp;quot; &amp;quot;你&amp;quot; &lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">seg &amp;lt;- worker(symbol = T, bylines = T)
segment(c(&amp;quot;妳很漂亮&amp;quot;, &amp;quot;我不喜歡你&amp;quot;), seg)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[[1]]
[1] &amp;quot;妳&amp;quot; &amp;quot;很漂亮&amp;quot;
[[2]]
[1] &amp;quot;我&amp;quot; &amp;quot;不&amp;quot; &amp;quot;喜歡&amp;quot; &amp;quot;你&amp;quot; &lt;/code>&lt;/pre>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>&lt;code>bylines = F&lt;/code>：回傳 1 個 chr vector，其每個元素為 1 個詞。&lt;/p>&lt;/li>
&lt;li>&lt;p>&lt;code>bylines = T&lt;/code>：回傳 1 個 list，其長度(元素的數量)等於輸入之 vector 的長度，每個元素為一個 chr vector。&lt;/p>&lt;/li>
&lt;/ol>
&lt;p>&lt;code>bylines = F&lt;/code>的設定在此符合我們的需求，並且為配合&lt;code>quanteda&lt;/code>套件的特性而將斷詞結果以一個字串(以空格分開字詞)而非一個 chr vector 的形式儲存。 以下&lt;strong>對第一篇文章進行斷詞&lt;/strong>：&lt;/p>
&lt;pre>&lt;code class="r">library(jiebaR)
all_post_seg &amp;lt;- all_post
seg &amp;lt;- worker(symbol = T, bylines = F)
all_post_seg$content[1] &amp;lt;- all_post$content[1] %&amp;gt;%
segment(seg) %&amp;gt;% paste(collapse = &amp;quot; &amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">all_post$content[1] %&amp;gt;% str_trunc(20)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;说两封让我感到“我天，给亲友的书信...&amp;quot;&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">all_post_seg$content[1] %&amp;gt;% str_trunc(30)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;说 两封 让 我 感到 “ 我 天 ， 给 亲友 的 ...&amp;quot;&lt;/code>&lt;/pre>
&lt;p>要處理所有文章，僅需外包一個 for loop：&lt;/p>
&lt;pre>&lt;code class="r">all_post_seg &amp;lt;- all_post
seg &amp;lt;- worker(symbol = T, bylines = F)
idx &amp;lt;- seq_along(all_post$content)
for (i in idx){
all_post_seg$content[i] &amp;lt;- all_post$content[i] %&amp;gt;%
segment(seg) %&amp;gt;% paste(collapse = &amp;quot; &amp;quot;)
}
head(all_post$content, 3) %&amp;gt;% str_trunc(20)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;说两封让我感到“我天，给亲友的书信...&amp;quot;
[2] &amp;quot;古人似乎喜欢把花椒当香料用。在《古...&amp;quot;
[3] &amp;quot;CSS 中的位置（position...&amp;quot; &lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">head(all_post_seg$content, 3) %&amp;gt;% str_trunc(30)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;说 两封 让 我 感到 “ 我 天 ， 给 亲友 的 ...&amp;quot;
[2] &amp;quot;古人 似乎 喜欢 把 花椒 当 香料 用 。 在 《 ...&amp;quot;
[3] &amp;quot;CSS 中 的 位置 （ position ） 属...&amp;quot; &lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="section level3">
&lt;h3>簡轉繁&lt;/h3>
&lt;p>&lt;a href="https://github.com/BYVoid/OpenCC">OpenCC&lt;/a> 是一個簡體字與繁體字轉換的專案，非常優秀，因為其不僅是單純字轉字，甚至處理了地區性的用法(如「軟體」vs.「软件」)。因此，其簡繁轉換的選項有非常多：&lt;/p>
&lt;ul>
&lt;li>&lt;code>s2t.json&lt;/code> Simplified Chinese to Traditional Chinese 簡體到繁體&lt;/li>
&lt;li>&lt;code>t2s.json&lt;/code> Traditional Chinese to Simplified Chinese 繁體到簡體&lt;/li>
&lt;li>&lt;code>s2tw.json&lt;/code> Simplified Chinese to Traditional Chinese (Taiwan Standard) 簡體到臺灣正體&lt;/li>
&lt;li>&lt;code>tw2s.json&lt;/code> Traditional Chinese (Taiwan Standard) to Simplified Chinese 臺灣正體到簡體&lt;/li>
&lt;li>&lt;code>s2hk.json&lt;/code> Simplified Chinese to Traditional Chinese (Hong Kong Standard) 簡體到香港繁體（香港小學學習字詞表標準）&lt;/li>
&lt;li>&lt;code>hk2s.json&lt;/code> Traditional Chinese (Hong Kong Standard) to Simplified Chinese 香港繁體（香港小學學習字詞表標準）到簡體&lt;/li>
&lt;li>&lt;code>s2twp.json&lt;/code> Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom 簡體到繁體（臺灣正體標準）並轉換爲臺灣常用詞彙&lt;/li>
&lt;li>&lt;code>tw2sp.json&lt;/code> Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom 繁體（臺灣正體標準）到簡體並轉換爲中國大陸常用詞彙&lt;/li>
&lt;li>&lt;code>t2tw.json&lt;/code> Traditional Chinese (OpenCC Standard) to Taiwan Standard 繁體（OpenCC 標準）到臺灣正體&lt;/li>
&lt;li>&lt;code>t2hk.json&lt;/code> Traditional Chinese (OpenCC Standard) to Hong Kong Standard 繁體（OpenCC 標準）到香港繁體（香港小學學習字詞表標準）&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/qinwf/ropencc">&lt;code>ropencc&lt;/code>&lt;/a>套件是 OpenCC 的 R 語言接口，其不在 CRAN 上，需以&lt;code>devtools&lt;/code>從 GitHub 下載：&lt;/p>
&lt;pre>&lt;code class="r">devtools::install_github(&amp;quot;qinwf/ropencc&amp;quot;)&lt;/code>&lt;/pre>
&lt;p>使用上非常容易：&lt;/p>
&lt;pre>&lt;code class="r">library(ropencc)
trans &amp;lt;- converter(TW2SP) # 臺灣用法轉大陸用法
run_convert(trans, &amp;quot;開放中文轉換軟體&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;开放中文转换软件&amp;quot;&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">trans &amp;lt;- converter(T2S) # 單純繁轉簡
run_convert(trans, &amp;quot;開放中文轉換軟體&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;开放中文转换软体&amp;quot;&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="r">trans &amp;lt;- converter(S2TWP) # 簡轉臺灣用法
run_convert(trans, &amp;quot;开放中文转换软件&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="nohighlight">[1] &amp;quot;開放中文轉換軟體&amp;quot;&lt;/code>&lt;/pre>
&lt;p>在此我使用&lt;code>S2TWP&lt;/code>轉換&lt;code>$content&lt;/code>；&lt;code>S2T&lt;/code>轉換&lt;code>$title&lt;/code>：&lt;/p>
&lt;pre>&lt;code class="r">library(ropencc)
all_post_seg$content &amp;lt;- run_convert(converter(S2TWP),
all_post_seg$content)
all_post_seg$title &amp;lt;- run_convert(converter(S2T),
all_post_seg$title)
head(all_post_seg) %&amp;gt;%
mutate(title = str_trunc(title, 8),
content = str_trunc(content, 8),
links = str_trunc(links, 8),
link_text = str_trunc(link_text, 8)) %&amp;gt;%
kable(&amp;quot;markdown&amp;quot;, align = &amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">title&lt;/th>
&lt;th align="center">date&lt;/th>
&lt;th align="center">content&lt;/th>
&lt;th align="center">num_sec&lt;/th>
&lt;th align="center">links&lt;/th>
&lt;th align="center">link_text&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">修辭還是真實&lt;/td>
&lt;td align="center">2018-06-21&lt;/td>
&lt;td align="center">說 兩封 …&lt;/td>
&lt;td align="center">12&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">花椒香料&lt;/td>
&lt;td align="center">2018-05-31&lt;/td>
&lt;td align="center">古人 似乎…&lt;/td>
&lt;td align="center">2&lt;/td>
&lt;td align="center">/cn/2…&lt;/td>
&lt;td align="center">去年的花椒&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">CSS 的…&lt;/td>
&lt;td align="center">2018-05-14&lt;/td>
&lt;td align="center">CSS …&lt;/td>
&lt;td align="center">15&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“查阅…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">毛姆的文學回憶錄&lt;/td>
&lt;td align="center">2018-05-04&lt;/td>
&lt;td align="center">前段時間 …&lt;/td>
&lt;td align="center">14&lt;/td>
&lt;td align="center">c(“/c…&lt;/td>
&lt;td align="center">c(“职业…&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">距離的組織&lt;/td>
&lt;td align="center">2018-05-03&lt;/td>
&lt;td align="center">前面 《 …&lt;/td>
&lt;td align="center">5&lt;/td>
&lt;td align="center">/cn/2…&lt;/td>
&lt;td align="center">闲情赋&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">語言聖戰的終結？&lt;/td>
&lt;td align="center">2018-04-19&lt;/td>
&lt;td align="center">一直 以來…&lt;/td>
&lt;td align="center">3&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“惊天…&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;/div>
&lt;div id="quanteda" class="section level2">
&lt;h2>quanteda&lt;/h2>
&lt;p>我們前面進行的資料前處理，已經將資料整理成符合&lt;a href="https://tutorials.quanteda.io/basic-operations/corpus/corpus/">&lt;code>quanteda::corpus()&lt;/code>輸入的格式&lt;/a>：&lt;/p>
&lt;blockquote>
&lt;p>A data frame consisting of a character vector for documents, and additional vectors for document-level variables&lt;/p>
&lt;/blockquote>
&lt;p>因此，依以下指令即可將&lt;code>all_post_seg&lt;/code>轉換成&lt;code>corpus&lt;/code>物件：&lt;/p>
&lt;pre>&lt;code class="r">library(quanteda)
corp &amp;lt;- corpus(all_post_seg,
docid_field = &amp;quot;title&amp;quot;,
text_field = &amp;quot;content&amp;quot;)
corp %&amp;gt;% summary() %&amp;gt;% as_data_frame() %&amp;gt;%
head(3) %&amp;gt;%
mutate(links = str_trunc(links, 8),
link_text = str_trunc(link_text, 8)) %&amp;gt;%
kable(&amp;quot;markdown&amp;quot;, align = &amp;quot;c&amp;quot;)&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="center">Text&lt;/th>
&lt;th align="center">Types&lt;/th>
&lt;th align="center">Tokens&lt;/th>
&lt;th align="center">Sentences&lt;/th>
&lt;th align="center">date&lt;/th>
&lt;th align="center">num_sec&lt;/th>
&lt;th align="center">links&lt;/th>
&lt;th align="center">link_text&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="center">修辭還是真實&lt;/td>
&lt;td align="center">217&lt;/td>
&lt;td align="center">375&lt;/td>
&lt;td align="center">15&lt;/td>
&lt;td align="center">2018-06-21&lt;/td>
&lt;td align="center">12&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;td align="center">chara…&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="center">花椒香料&lt;/td>
&lt;td align="center">149&lt;/td>
&lt;td align="center">246&lt;/td>
&lt;td align="center">9&lt;/td>
&lt;td align="center">2018-05-31&lt;/td>
&lt;td align="center">2&lt;/td>
&lt;td align="center">/cn/2…&lt;/td>
&lt;td align="center">去年的花椒&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="center">CSS 的位置屬性以及如何居中對齊超寬元素&lt;/td>
&lt;td align="center">347&lt;/td>
&lt;td align="center">805&lt;/td>
&lt;td align="center">23&lt;/td>
&lt;td align="center">2018-05-14&lt;/td>
&lt;td align="center">15&lt;/td>
&lt;td align="center">c(“ht…&lt;/td>
&lt;td align="center">c(“查阅…&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>有了&lt;code>corpus&lt;/code>的資料結構後，即進入了下圖&lt;code>quanteda&lt;/code>的分析架構，也結束了資料前處理的階段，開始進入 EDA 的階段。&lt;/p>
&lt;img src="https://img.yongfu.name/blog/mermaid.svg" alt="">
&lt;div class="mermaid">
graph TD
C(Corpus)
token(Tokens)
AP["Positional analysis"]
AN["Non-positional analysis"]
dfm(DFM)
tidy("Tidy Text Format")
vis("Visualize")
C --> token
token --> dfm
token -.-> AP
dfm -.-> AN
tidy -->|"cast_dfm()"| dfm
dfm -->|"tidy()"| tidy
dfm -.- vis
tidy -.-> vis
AP -.- vis
style C stroke-width:0px,fill:#6bbcff
style token stroke-width:0px,fill:#6bbcff
style dfm stroke-width:0px,fill:#6bbcff
style tidy stroke-width:0px,fill:orange
linkStyle 6 stroke-width:0px,fill:none;
linkStyle 8 stroke-width:0px,fill:none;
&lt;/div>
&lt;p>&lt;a href="https://quanteda.io/">quanteda&lt;/a> 有相當完整的&lt;a href="https://tutorials.quanteda.io/">教學資源&lt;/a>，且有很多有用的函數。同時，&lt;a href="https://github.com/juliasilge/tidytext">&lt;code>tidytext&lt;/code>&lt;/a> 套件也能輕易與 &lt;code>quanteda&lt;/code> 配合，在 &lt;code>document-feature matrix&lt;/code> 與&lt;code>tidytext&lt;/code>所提倡的 &lt;strong>tidy data frame&lt;/strong>(one-token-per-document-per-row) 兩種資料結構間自由轉換。&lt;strong>tidy data frame&lt;/strong> 的格式與&lt;a href="https://github.com/tidyverse/ggplot2">&lt;code>ggplot2&lt;/code>&lt;/a>相吻合，有助於資料視覺化的進行。&lt;/p>
&lt;p>這裡選擇以&lt;code>quanteda&lt;/code>而非&lt;code>tidytext&lt;/code>作為主要架構的原因在於&lt;code>tidytext&lt;/code>的架構僅容許 &lt;strong>bag-of-words&lt;/strong> 的架構，但&lt;code>quanteda&lt;/code>除了 &lt;strong>bag-of-words&lt;/strong> 之外，還保有 &lt;strong>Positional analysis&lt;/strong> 的潛力。&lt;/p>
&lt;p>由於篇幅有限，這裡不多加細談&lt;code>quanteda&lt;/code>套件&lt;a href="#fn3" class="footnoteRef" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>。關於&lt;code>quanteda&lt;/code>的使用，可以參考 &lt;a href="https://tutorials.quanteda.io/">quanteda tutorial&lt;/a>，內容非常詳盡。&lt;/p>
&lt;/div>
&lt;div id="reproduce" class="section level2">
&lt;h2>Reproduce&lt;/h2>
&lt;p>這篇文章的原始碼在我的 &lt;a href="https://github.com/liao961120/blog/tree/master/post_source/quanteda-chinese">GitHub&lt;/a>，歡迎下載至自己的電腦執行。&lt;/p>
&lt;/div>
&lt;div class="section level2 unnumbered">
&lt;h2>參考資料&lt;/h2>
&lt;div id="refs" class="references">
&lt;div id="ref-silge2017">
&lt;p>Silge, Julia, and David Robinson. 2017. &lt;em>Text Mining with R: A Tidy Approach&lt;/em>. 1st ed. O’Reilly Media, Inc.&lt;/p>
&lt;/div>
&lt;div id="ref-watanabe2018">
&lt;p>Watanabe, Kohei, and Stefan Müller. 2018. “Quanteda Tutorials.” &lt;em>Quanteda Tutorials&lt;/em>. https://tutorials.quanteda.io/.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;ol>
&lt;li id="fn1">&lt;p>Mac 和 Linux 內建有 Bash，但 Windows 沒有。&lt;a href="#fnref1">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>要能直接執行&lt;code>wget_list&lt;/code>需先給予其執行的權限，因此需設置&lt;code>chmod 755 &amp;lt;path to wget_list&amp;gt;&lt;/code>，並且將&lt;code>wget_list&lt;/code>置於 shell 會自動搜尋程式的地方(如&lt;code>/usr/bin/&lt;/code>)。&lt;/p>
&lt;p>另一個方法是不設置權限，直接執行&lt;code>wget_list&lt;/code>：&lt;br />
&lt;code>bash &amp;lt;path to wget_list&amp;gt; &amp;lt;file1&amp;gt; &amp;lt;file2&amp;gt;&lt;/code> &lt;!--FOOTNOTE END-->&lt;a href="#fnref2">↩&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>未來可能會發一篇續作。&lt;a href="#fnref3">↩&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;p style="text-align:right;font-size:7px;margin-top:0px;margin-bottom:0px;padding-top:0px;padding-bottom:1px">
Last updated: 2018-11-10
&lt;/p>
&lt;style>
div.mermaid {
display:none;
}
&lt;/style></description><category>R</category><category>linguistics</category><category>中文</category></item><item><title>我的 R 學習歷程</title><link>https://yongfu.name/2018/01/31/rlearningpath/</link><pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/01/31/rlearningpath/</guid><description>&lt;p>接觸 &lt;strong>R&lt;/strong> 的時間大約五個月了，從原本對電腦、程式一竅不通到現在能有效率的 debug、寫出簡潔有條理的 R code、甚至用 R 與 Markdown 架站寫部落格。算一算，我每週通常至少 3 天會用到 R，不是督促自己熟悉 R，是因為它太有魅力了。&lt;/p>
&lt;p>學 R 語言很有趣&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>，但也相當耗時耗力。我在學習 R 上花了大量的時間，若不是運氣好找到對的方向，我不可能花費那麼多力氣去學 R，甚至可能直接放棄。雖然如此，我還是感到有點可惜，若能更早知道有效率的學習方式，就不必花費如此大量的時間 (及紙張) 在學習上。因此，我希望將學習 R 的心路歷程寫下，給想學 R 的人做為參考，或許可以減短學習初期耗費精神的時期。接下來，我將說明：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>我學習 R 的動力來源&lt;/p>
&lt;/li>
&lt;li>
&lt;p>R 學習路徑：若從頭來過，我會如何學習 R、挑選哪些資源，讓自己更有效率地學習。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>為何我能持續學習 R 而未放棄？其實現在我比較頭大的問題是：&lt;strong>要怎麼克制自己不要一直打開 Rstudio？&lt;strong>正是因為我學 R 的過程如同打電動一樣歡樂，所以&lt;/strong>放棄&lt;/strong>這個問題根本不存在。但事實上，剛開始學 R 時並不如何歡樂，直到上了課才越來越喜歡 R。&lt;/p>
&lt;h2 id="課程回顧">課程回顧&lt;/h2>
&lt;p>106學年上學期，我最喜歡、也意外收穫最多的課是謝舒凱老師開設的 &lt;a href="https://nol2.aca.ntu.edu.tw/nol/coursesearch/print_table.php?course_id=142%20U0750&amp;amp;class=&amp;amp;dpt_code=1420&amp;amp;ser_no=76601&amp;amp;semester=106-1&amp;amp;lang=CH">R 語言與資料科學導論&lt;/a>，或許會是我大學生涯最喜歡的課吧。&lt;/p>
&lt;p>起初是抱著學好 R 語言 (加上通識快修不完) 的心情去加簽這門課的，但課堂上 R 語言的語法卻教得不多。然正是這種&lt;strong>不聚焦在程式語法的課程安排&lt;/strong>，讓我把 R 學得非常好。如果課程從頭到尾都在教 R 語言，到期末我一定會受不了越顯複雜的語法，最後便隨意敷衍了事。我在這門課收穫最大的，反而不是課堂學到的技巧，而是老師、助教們傳達的一些概念與想法，以及自己探索這些概念想法的樂趣與收穫：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Open Source&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> 以及 R 的生態圈&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>資料科學與 Story telling: &lt;strong>Coding&lt;/strong> 於資料科學中的應用，不僅是傳統所強調的&lt;strong>功能性&lt;/strong>，Coding 亦於&lt;strong>美觀及呈現&lt;/strong>上扮演重要的角色。資料科學透過 Coding 處理、分析資料；同時也透過 Coding 作圖將資料視覺化，好將資料科學上的發現&lt;strong>說成故事給其他人聽&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>文本分析: 我本來對這部分最沒有興趣，直到我意識到文本資料是研究人類行為、社會與文化最重要的資料來源之一&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Reproducibility: &lt;a href="http://rmarkdown.rstudio.com/">R Markdown&lt;/a> (&lt;code>.Rmd&lt;/code>) 是我見過最強大的文件格式，可以輸出成網頁、投影片、PDF、Word、Markdown 等。這讓使用者可以在一份文件中做事情 (&lt;code>Rmd&lt;/code>)，並依據所需輸出成各種文件，而不用剪剪貼貼 (從 Word 貼到 PPT)，也省下排版所花費的心力。例如，要報告可輸出成投影片 (&lt;code>.html&lt;/code> 或 &lt;code>.pptx&lt;/code>) 、要列印可輸出成 PDF、要放到網路上可輸出成網頁。R Markdown 讓使用者能以簡單有效率的方式工作，這是達到 &lt;a href="https://en.wikipedia.org/wiki/Reproducibility">Reproducibility&lt;/a> 的前提與重要基礎。&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://img.yongfu.name/assets/images/post_img/reproducible_research.png" alt="Reproducible Research with R">
&lt;figcaption>Reproducible Research with R&lt;/figcaption>
&lt;/figure>
&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="課程起手式">課程起手式&lt;/h3>
&lt;p>這門課與我個性相符，完全顯現在第一堂課實習課 (Lab Session) 的作業上。作業與 R 並無相關，而是要使用 Markdown 格式撰寫自我介紹。由於我對簡約風有點痴迷&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> (受 Markdown 影響後更為痴迷)，在 Markdown 上花了一些時間研究。目前，透過 R 與 Pandoc Markdown 語法的整合 (即 R Markdown)，我可以快速簡潔地寫出含有上下標、Footnotes、超連結、citation 的精美文件&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>或文章 (例如，這篇部落格) 而完全不須使用 html 語法。這讓我在寫許多作業時，效率提高不少。&lt;/p>
&lt;p>對 R Markdown 的痴迷與熱愛可說是我R語言功力進步的關鍵。&lt;strong>R Markdown 賦予每個人將想法傳達給其他人的能力&lt;/strong>&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>，因此，&lt;strong>能做出美好的東西並與其他人分享&lt;/strong>成為我學習 R 最強烈的動機之一。例如，當初在做這門課的&lt;a href="https://rlads2017g1.github.io/presentation.html">期末專案&lt;/a> 時，發覺 R 可以畫出很多種&lt;a href="http://gallery.htmlwidgets.org/">互動式圖表&lt;/a> (不須懂 JavaScript)，於是我開始尋找最適合呈現資料的方式。由於不同種類的圖，常有不同資料格式的要求&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>，因此為了畫出最適合的圖，我反覆整理許多資料以符合格式。這使得我在 data wrangling 上，能力大幅提升。&lt;/p>
&lt;h2 id="r-學習路徑">R 學習路徑&lt;/h2>
&lt;h3 id="起步">起步&lt;/h3>
&lt;p>我剛開始學 R 時不算順遂&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>。剛進到 R 的世界，一定會被為數眾多的 function&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup> (以及更多的學習資源) 搞得迷失方向，不知從何開始。事實上，這不是我們的錯，因為連 R 的社群內也有相關的爭論：&lt;strong>Base R first vs. Tidyverse first&lt;/strong> (詳細可參考&lt;a href="http://varianceexplained.org/r/teach-tidyverse/">此文&lt;/a>)。R 真的是一個使用門檻不低的軟體，要精通它不是一、兩年內的事情&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>，所以一定要對 R 有熱情，才可能持續支撐對於 R 的學習&lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>。&lt;/p>
&lt;p>R 語言的基礎 ─ &lt;strong>Base R&lt;/strong> 是剛開始學 R 的一大障礙，這也是為何會有 &lt;strong>Base R first vs. Tidyverse first debate&lt;/strong>。由於學好 R (更精確地說，是學好 &lt;strong>Base R&lt;/strong>) 需一段時間，我認為 &lt;strong>Base R first&lt;/strong> 的學習方式很容易使初學者放棄。但學習 R 不可能不接觸 &lt;strong>Base R&lt;/strong> ，因為它是 R 語言最重要的基礎。因此，我覺得最好的方式是&lt;strong>交錯並進&lt;/strong>：&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Base R&lt;/strong> 學到一定的基礎後 (不必精通) &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>，開始學習用&lt;code>tidyverse&lt;/code>套件處理資料。之後隨著時間，自然而然就會熟悉這兩者。&lt;/p>
&lt;/blockquote>
&lt;h3 id="維持">維持&lt;/h3>
&lt;p>學習一項新技能最困難的地方大概是要能&lt;strong>持續穩定前進&lt;/strong>。我認為有幾種方法可以幫助自己：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>參考書&lt;/strong>：一本好的參考書，能作為一個參照指標，&lt;strong>幫助自己安排進度&lt;/strong>，也&lt;strong>讓自己知道學到了哪裡&lt;/strong>。參考書指出了一條方向，比較不會在學習的路途中迷失 (相比零散的網路資源)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>修課&lt;/strong>：我非常建議在學校修一門 R 語言的課 (最好 3 學分以上)，但&lt;strong>不建議線上課程&lt;/strong>&lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>。修課的話，有老師和助教提供經驗，能省下許多摸索的時間；每週有固定進度，或多或少可迫使自己持續接觸程式；若有期末專案，這會是 R 功力突飛猛進的最佳機會 (效果比起純粹練習好太多)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>讓自己喜歡 R&lt;/strong>：初學 R 時因為不知道這項的重要性，所以學得有點辛苦。這點真的很微妙，有時候太過專注於學習某項&amp;quot;技能&amp;quot;，反而會&lt;strong>過於專注實用性而忽略了趣味性&lt;/strong>，於是只剩理性說服自己：&lt;em>學這很有用，你一定要堅持下去&lt;/em>。通常開始這樣想，代表離放棄不遠了。這也是為何我很推 &lt;a href="https://nol2.aca.ntu.edu.tw/nol/coursesearch/print_table.php?course_id=142%20U0750&amp;amp;class=&amp;amp;dpt_code=1420&amp;amp;ser_no=76601&amp;amp;semester=106-1&amp;amp;lang=CH">R 語言與資料科學導論&lt;/a> 這門課的原因 ─ 老師真的很有智慧，不斷鼓勵學生們去想&lt;strong>有趣又可用資料科學回答的問題&lt;/strong>，也常介紹一些 R 意想不到的有趣應用，但從未將焦點放在程式的硬實力上。我自己由 R 善於&lt;strong>視覺呈現&lt;/strong>的特質切入 &amp;ndash; 從 R Markdown 開始，擴展到資料視覺化，又到網頁設計 (縱使我&lt;code>html&lt;/code>和&lt;code>css&lt;/code>基礎很差)，每一項對我來說都非常有趣，R 學起來因而樂此不疲。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="實作專案">實作：專案&lt;/h3>
&lt;p>看再多參考書、修再多課、做再多練習題都比不上實際去完成一個專案。在實作專案的過程中，會&lt;strong>遇到一缸子自己不知如何解決的問題&lt;/strong>，因此過程中就是在不斷吸收新知識、學習如何問問題&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>，如此的學習效果是零散練習題的好幾倍。此外，做專案是自頭至尾走過一次資料科學的流程，從爬資料、整理、分析到呈現，將先前習得的零碎技巧組織在一起，會讓 R 的功力大增；同時，資料科學專案是在實際解決 (回答) 一個問題，這不僅對於自己，且對於社會有實質的意義與價值。&lt;/p>
&lt;h4 id="對於專案的一些建議">對於專案的一些建議&lt;/h4>
&lt;p>我認為要從實作專案收穫那麼多，不可或缺的因素同樣是&lt;strong>興趣與熱情&lt;/strong>。沒有這些一定不可能堅持完美，不能堅持完美，專案就會淪落為練習題 (反正只要跑出東西就好)。題目的選擇因而會是成敗的關鍵；與組員 (若為團體專案) 的溝通與共識更是關鍵中的關鍵，一定要在專案中找到最適合自己的角色，否則專案只是浪費時間。&lt;/p>
&lt;h2 id="學習資源">學習資源&lt;/h2>
&lt;p>起初為了學 R 所印的兩本書以及聽的線上課程效果頗糟，讓我覺得自己起步不算順遂。這是因為當時對 R 不太了解，也不知如何觸及其社群，因而並未慎選書籍及課程即步入學習歷程。&lt;/p>
&lt;h3 id="學習歷程">學習歷程&lt;/h3>
&lt;p>若回到五個多月前，我會如此安排 R 的學習歷程：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>基礎參考書 (若有餘力的話 + &lt;code>swirl&lt;/code>&lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup> 當練習題)&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://rstudio-education.github.io/hopr">Hands-On Programming with R&lt;/a>：這是一本寫得非常好且相當簡單的書，目標讀者是沒有程式經驗的初學者 (雖然我覺得縱使有程式經驗也可看這本書了解 R 的邏輯)。此書內容僅聚焦在非常基礎的 &lt;strong>Base R&lt;/strong>，但重點是其將 R 的邏輯寫得非常清楚。我是在學 R 一陣子之後才看了這本書，對我有莫大的幫助。這本書相當短，可以很快看完，更多資訊請 &lt;a href="https://www.google.com.tw/search?biw=1280&amp;amp;bih=560&amp;amp;ei=rF90WtefO8W48QXcv4y4Dg&amp;amp;q=Hands-On+Programming+with+R&amp;amp;oq=Hands-On+Programming+with+R&amp;amp;gs_l=psy-ab.3...1714146.1714146.0.1714717.1.1.0.0.0.0.0.0..0.0....0...1c.1.64.psy-ab..1.0.0....0.FFjeBpwN_VQ">google 書名&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://r4ds.had.co.nz">R for Data Science&lt;/a>：這本書應該只會越來越紅。如同其名，這本書就在教 R 於資料科學上的應用。與上本書不同的是，這本書以&lt;code>tidyverse&lt;/code>套件為基礎而非 Base R。雖然如此，此書並不需要 Base R 的基礎 (有的話會更好，所以我才推薦 &lt;strong>Hands-On&lt;/strong>)，需要的基礎書中都有介紹。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>我建議可以先從 &lt;strong>Hands-On Programming with R&lt;/strong> 開始，因為這本書很短可以快速看完。之後再按照章節慢慢看 &lt;strong>R for Data Science&lt;/strong> (可配合學校上課進度，這本書短時間內看不完)，逐步累積自己的功力。&lt;/p>
&lt;p>上述兩本書都是英文的，或許有些人很抗拒英文，但中文書並非一個好選擇。當 R 學到某個程度後，更進階的資源幾乎全為英文；此外，google 問題解決方法時，用中文十之八九會找不到答案。看英文書可讓自己熟悉 R 的英文用語，而且這些書的英文都很簡單。&lt;/p>
&lt;ul>
&lt;li>若有志探索 R 的其它可能性，可以參考 &lt;a href="https://bookdown.org">bookdown.org&lt;/a> 的書 (僅推薦 Star 100 以上，上面還是有一些雷書)。這裡的書都是用&lt;code>bookdown&lt;/code>套件寫的，而且皆為 open source，可以在網頁上看或下載到電腦 (詳見&lt;code>bookdown&lt;/code>套件)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>學校課程：如&lt;a href="#%E8%AA%B2%E7%A8%8B%E5%9B%9E%E9%A1%A7">課程回顧&lt;/a>所提及，一門 R 的課可以讓自己視野更加廣博。僅是自修閉門造車進步很慢、常常會落掉進度、而且頗為苦悶。有人帶領比較不會走偏方向，同時還能學到許多有趣的東西。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>將 R 當玩具：如果學 R 如同遊戲一般好玩，那就沒有堅不堅持的問題了，這也是我自認學 R 相當成功的地方。以下，我將介紹一些我知道的有趣資源，不見得直接與資料分析相關，但一定可與 R 結合。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://www.r-graph-gallery.com/">R Graph Gallery&lt;/a> 、&lt;a href="http://www.htmlwidgets.org/">htmlwidgets for R&lt;/a> : R 最為吸引人的特徵之一就是其強大的繪圖功能，加上其它套件的擴充，幾乎所有跟資料有關的圖都可以用 R 畫。沒事多多欣賞其他人用 R 畫出的圖，不僅療癒放鬆，同時對 R 的能力有個大概的想像、未來更有可能會有使用需求。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://rmarkdown.rstudio.com/gallery.html">R Markdown&lt;/a> : 先前提過 R Markdown 很強大，可以點進 gallery 感受一下其生產力。我認為不會用 R Markdown 比起不會用 R 還要可惜。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bookdown.org/yihui/blogdown/">Blogdown&lt;/a> : 我目前的網站 (一部份) 是透過 &lt;code>blogdown&lt;/code> 套件在經營&lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>的。Blogdown 大幅降低了架站的門檻，即使完全不懂 HTML/CSS 也能快速上手，同時配合 Git/GitHub 能讓發表 (及修改) 文章的過程非常有效率。此外，以 &lt;code>blogdown&lt;/code> 架設的網頁是靜態的，不同於 Wordpress 等的動態網頁，靜態網頁的速度要快許多、不需要付費伺服器，而且容易搬遷。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>集大成：專案製作&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如&lt;a href="#%E5%AF%A6%E4%BD%9C%EF%BC%9A%E5%B0%88%E6%A1%88">實作：專案&lt;/a>所述，專案是讓自己快速進步的最佳方案。同時，結合上述的學習資源：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>資料分析：&lt;strong>R for Data Science&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>視覺化：&lt;strong>R Graph Gallery&lt;/strong>, &lt;strong>htmlwidgets&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>網頁製作：&lt;strong>R Markdown&lt;/strong>, &lt;strong>Blogdown&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>這些都可以透過 R 完成，並且讓自己更有動機實作專案、讓專案看起來更完美。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="更上一層樓following-and-followers">更上一層樓：Following and Followers&lt;/h3>
&lt;p>&lt;strong>Following&lt;/strong> and &lt;strong>Followers&lt;/strong> 是我直接借用 GitHub 上的用詞：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Following: 該用戶追蹤的其他用戶&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Followers: 追蹤該用戶的其他用戶&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>透過追蹤 R 社群是一個讓自己跟上快速成長的 R 的好方法，讓自己每日都長一些知識。例如，我透過臉書追蹤 &lt;a href="https://www.facebook.com/rbloggers/">R bloggers&lt;/a>，裡面常出現優質好文，且內容通常清楚易讀。&lt;/p>
&lt;p>另一個讓 R 功力快速成長的方式，就是自己成為 R bloggers：我不是指寫的文章要登上 &lt;a href="https://www.facebook.com/rbloggers/">R bloggers&lt;/a>，而是像那些作者一樣在部落格上寫文章。寫文章是一種很好的學習方式，能幫自己重新組織所學，同時也可檢視是否真正了解正在撰寫的主題。文章不見得是寫給別人看的，有時忘記一些東西，透過自己的文字重新學習相當方便。推薦兩篇簡短的優質文說明寫 blog 的好處：&lt;a href="https://bookdown.org/yihui/blogdown/personal-experience.html">Yihui Xie&lt;/a> 、&lt;a href="https://www.r-bloggers.com/advice-to-aspiring-data-scientists-start-a-blog/">David Robertson&lt;/a>。&lt;/p>
&lt;h2 id="小結">小結&lt;/h2>
&lt;p>我真的很慶幸能在大三時接觸到 R。以往我都把時間花在讀書上，也不知何時才能用得上。R 讓我發現了自己的嗜好&lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>，也讓我體驗到&lt;strong>為了處理想解決的問題而學習&lt;/strong>的感覺。我也才慢慢體會到為了&lt;strong>興趣&lt;/strong>所學與為了&lt;strong>其它目的&lt;/strong> (競爭力、考試、學分、跟風？) 所學，能帶來的可觀差異。當然，興趣與其它目的並非不能同時存在，只是有時&lt;strong>專注在其它目的會抑制興趣&lt;/strong>，而最能讓學習持續的動力來源卻是興趣。&lt;/p>
&lt;p>這篇文章的目的，旨在提供有志學 R 卻不確定如何開始的人作參考。文章介紹的學習資源 (或是任何嘮叨的內容)，不只是 (我認為) 有用的，且能強烈引起我的興趣。希望讀過這篇文章的人，不僅僅獲益於文章介紹的學習資源，且能為了興趣而學 R ─ 這是此篇文章最想傳達的想法。&lt;/p>
&lt;br>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;strong>R 語言&lt;/strong>本身不算有趣，而是 R 及其生態圈所能做到的事。幾乎任何想得到、可用電腦處理的事情，都可找到相關的 R 套件。&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>我覺得 Open Source 是一個很強盛且很令人感動的文化。其展現了透過分享與合作，人們能夠創造出多少美好的事物。&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>我常在想，不知何時 &lt;strong>SAS&lt;/strong> 與 &lt;strong>SPSS&lt;/strong> 會被 &lt;strong>R&lt;/strong> 幹掉。由於 SAS 與 SPSS 是私人版權軟體，在網路上的社群資源相當稀少且日漸縮小 (問問題找不到答案)，而 R 社群巨大且快速成長，加上其開放的特性，功能的擴充與成長是 SAS 與 SPSS 等封閉軟體無法企及的。&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>至少是資訊量最大的資料來源，比較看看社群網站、網路、書籍 (見 &lt;a href="https://en.wikipedia.org/wiki/Google_Ngram_Viewer">Google Ngram&lt;/a>，體驗一下文本資料的強大) 累積的資料量與心理學實驗累積的資料量。&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>語言學不僅可用於研究語言、還可拿來當作研究人類社會與文化的工具。&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>圖片擷取自 British Ecological Society 編寫之指南 &lt;a href="https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf">A Guide to Reproducible Code&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>我本來對美感、簡約、使用者經驗完全不在乎，但受我好朋友的影響，越來越在意這些東西 (看到乾淨舒服的版面就會很興奮，有點點不和諧就會想修改它)。&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>見 &lt;a href="http://rmarkdown.rstudio.com/">R Markdown&lt;/a> 了解其強大。&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>Markdown 的學習門檻很低，使大家能簡單地寫出排版整潔漂亮的文章。R Markdown 大幅擴充 Markdown 的功能，秉持著相同的精神，R Markdown 使大家能簡單地寫出排版整潔漂亮 (且含有R跑出之圖表) 的&lt;strong>文章&lt;/strong>、&lt;strong>投影片&lt;/strong>、&lt;strong>書籍&lt;/strong>，甚至&lt;strong>網站&lt;/strong>。&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>例如欲繪製時間序列的資料&lt;code>ggplot2&lt;/code>及&lt;code>dygraphs&lt;/code>對資料就有不同的要求。&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>我在開學前的暑假即開始自學 &lt;strong>R&lt;/strong>，進步相當緩慢，而且語法時常忘記。最重要的是，我必須花心力督促自己才能繼續。開學後在課堂上教導的內容不多，僅做些簡單的介紹，剩下的要靠自己發掘。由於深深感受到 Markdown 的潛力 (見&lt;strong>課程起手式&lt;/strong>，我開始「不務正業」，去學一些和 R (看似) 不怎麼相關的東西 (例如，研究如何用 GitHub Pages 架部落格)，也越來越喜歡 html 格式 (以前偏愛 PDF，但發現 html 能呈現的東西比 PDF 多太多，又比較美觀)。於是，R 成為我處理作業最常使用的程式之一。&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>R 除了函數非常多，使用起來也非常彈性，對使用者的限制相當少：其它語言很容易跑出 error message，但在 R 卻相對不容易 (例如，輸入&lt;code>c (1, 2, &amp;quot;c&amp;quot;) &lt;/code>不會跑出 error 但會將&lt;code>1&lt;/code>, &lt;code>2&lt;/code>從&lt;strong>數字&lt;/strong>轉換成&lt;strong>字串&lt;/strong>)。這對初學者不見得是好事：初學者常因此難以釐清 R 運作的邏輯。&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>精通 R 應該可算是一輩子的事吧，因為 R 是個不斷在成長的語言。R 的社群與生態圈成長非常迅速 (例如，目前 &lt;a href="https://cran.r-project.org/">CRAN&lt;/a> 有 12,081 個 package)，要精通 R 勢必要與它一起成長。即使只求在傳統的資料分析上熟悉 R，也需要花不少的時間，而且很難在短時間內大幅進步。&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>這裡把 R 說得有點負面。當 R 融入你生活中，你喜歡它都來不及了 (不論是它讓你做事更有效率或它能做出很厲害的東西)，根本就沒有&lt;strong>支撐學習&lt;/strong>的問題，只有&lt;strong>時間不足&lt;/strong>的問題。&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>&lt;strong>一定的基礎&lt;/strong>：了解 R 的&lt;strong>向量式運算邏輯&lt;/strong>、&lt;strong>資料結構&lt;/strong> (vector, list, matrix, dataframe) 、熟悉 &lt;strong>Subsetting&lt;/strong> (&lt;code>object[index]&lt;/code>)。見&lt;strong>學習資源&lt;/strong>了解更多。&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>我覺得學習任何東西，如果有實體 (面對面) 的課程，都比線上課程還好。我自己很難集中注意力在線上課程，加上時間太自由容易怠惰，或有事情時就會將其順位往後挪，因此成效往往不彰。&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>在網路上要輸對關鍵字才找得到答案。起初由於不清楚 R 的運作邏輯，找答案的效率會很低。但隨著功力提升，會越容易知道問題的癥結在哪，找答案 (或是確認問題能否被解決) 也會變得更有效率。&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>swirl 是 R 的套件，可以讓使用者直接在 R 的環境中互動式學習 R。其提供許多課程可供使用者下載，我覺得學完最基礎的課程就夠了，可以熟悉 Base R 的環境。詳細的課程見 &lt;a href="https://github.com/swirldev/swirl_courses#swirl-courses">swirl course&lt;/a>。&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>我是在逛 bookdown.org 的書時，無意間發現 Blogdown 的書，可惜當時我早已用 Jekyll / GitHub Pages 架好網站。Jekyll 的使用門檻相當高，我也並未學會如何使用 (我直接用別人做好的&lt;a href="https://github.com/kitian616/jekyll-TeXt-theme">模板&lt;/a>，因此只要修改一些&lt;code>.html&lt;/code>和&lt;code>.css&lt;/code>)。反之，&lt;code>blogdown&lt;/code>支援的 &lt;a href="https://gohugo.io/">Hugo&lt;/a> 並沒有 Jekyll 那麼多的限制。&lt;code>blogdown&lt;/code>同時大幅降低了架站的難度，例如，&lt;code>blogdown&lt;/code>支援的 Markdown 語法 (Pandoc Markdown) 很豐富且很適合寫部落格，因此用其寫部落格就不必理會 Jekyll 或 Hugo 等網頁產生器所支援的特定 Markdown 語法 (Jekyll 支援&lt;code>kramdown&lt;/code>、Hugo 支援&lt;code>blackfriday&lt;/code>)。&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>大學以前，我很排斥程式 (詳見&lt;a href="https://liao961120.github.io/2017/11/26/mathematics.html">此文&lt;/a>)。後來發覺程式的實用性而開始想學程式語言。學了 R 後發現自己其實有資工魂 (雖然 CS 領域似乎不常用 R ？)，其實很喜歡程式語言、電腦等東西。&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>R</category><category>Course</category><category>thoughts</category><category>中文</category></item><item><title>Constructing Life Tables with R</title><link>https://yongfu.name/2017/12/11/life_tables/</link><pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate><guid>https://yongfu.name/2017/12/11/life_tables/</guid><description>&lt;p>I have been using the package &lt;code>dplyr&lt;/code> to handle with data for a while, and I thought I can use it with ease until I was stuck with my homework on &lt;strong>contructing a life table&lt;/strong>. I found spreadsheets (either &lt;em>Excel&lt;/em> or &lt;em>Google Spreadsheets&lt;/em>) easy for handling this task, but had a hard time dealing with it in R. I think it was due to my unfamiliarity with the built-in functions and insufficient practice in R. So, I wrote this post as a review and practice of my data-wrangling skills in R.&lt;/p>
&lt;p>I will illustrate how I constructed a life table with R, and you&amp;rsquo;ll find out how easy it is (and wonder how could I stumble on it).&lt;/p>
&lt;h2 id="load-packages">Load Packages&lt;/h2>
&lt;p>I used these packages to construct a Life table.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(readr)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(dplyr)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(knitr)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="load-data">Load data&lt;/h2>
&lt;p>Load the csv file to &lt;code>life_table&lt;/code>. The raw data contains 3 columns: &lt;strong>Age&lt;/strong>, &lt;strong>Survivorship at Age x ($l_x$)&lt;/strong>, and &lt;strong>Fecundity at Age x ($m_x$)&lt;/strong>. I added &lt;code>options(scipen=999)&lt;/code> to disable scientific notations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">options&lt;/span>(scipen&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">999&lt;/span>) &lt;span style="color:#75715e"># Disable Scientific Notation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>life_table &lt;span style="color:#f92672">&amp;lt;-&lt;/span> &lt;span style="color:#a6e22e">read_csv&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;life_table.csv&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>life_table
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code># A tibble: 10 x 3
Age lx mx
&amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 0 1.0000000 0
2 1 0.0000620 4600
3 2 0.0000340 8700
4 3 0.0000200 11600
5 4 0.0000155 12700
6 5 0.0000110 12700
7 6 0.0000065 12700
8 7 0.0000020 12700
9 8 0.0000020 12700
10 9 0.0000000 0
&lt;/code>&lt;/pre>&lt;h2 id="variables-to-construct">Variables to Construct&lt;/h2>
&lt;p>Here are the variables that need to be calculated.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Statistic&lt;/th>
&lt;th style="text-align:center">Notation&lt;/th>
&lt;th style="text-align:center">Calculation Formula&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">&lt;/td>
&lt;td style="text-align:center">$l_x m_x$&lt;/td>
&lt;td style="text-align:center">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;/td>
&lt;td style="text-align:center">$x l_x m_x$&lt;/td>
&lt;td style="text-align:center">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;/td>
&lt;td style="text-align:center">$l_x m_x e^{-rx}$&lt;/td>
&lt;td style="text-align:center">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Average survivorship&lt;br>(age class)&lt;/td>
&lt;td style="text-align:center">$L_x$&lt;/td>
&lt;td style="text-align:center">&lt;code>$L_x = (l_x + l_{x+1})/2$&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Life expectancy&lt;/td>
&lt;td style="text-align:center">$e_x$&lt;/td>
&lt;td style="text-align:center">&lt;code>$e_x = (L_x + L_{x+1} + … + L_{max})/l_x$&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Reproductive value&lt;/td>
&lt;td style="text-align:center">$V_x$&lt;/td>
&lt;td style="text-align:center">$\displaystyle \frac{\sum_{y=x}^{max\hspace{0.3mm}x} e^{-ry} l_y m_y}{e^{-rx} l_x}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Net reproductive rate&lt;/td>
&lt;td style="text-align:center">$R_0$&lt;/td>
&lt;td style="text-align:center">$\sum_{all \hspace{0.3mm} x} l_x m_x$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Generation time&lt;/td>
&lt;td style="text-align:center">$G$&lt;/td>
&lt;td style="text-align:center">$\frac{\sum_{all \hspace{0.3mm} x} x l_x m_x}{R_0}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Intrinsic rate of increase&lt;/td>
&lt;td style="text-align:center">Approximate $r$&lt;/td>
&lt;td style="text-align:center">$r \approx \frac{ln(R_0)}{G}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Intrinsic rate of increase&lt;/td>
&lt;td style="text-align:center">(True) $r$&lt;/td>
&lt;td style="text-align:center">$\displaystyle \sum_{all \hspace{0.3mm} x} e^{-rx}l_x m_x = 1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$l_xm_xe^{-rx}$ and $V_x$ will be calculated twice for the approximate and the true $r$.&lt;/p>
&lt;h2 id="constructing-variables">Constructing Variables&lt;/h2>
&lt;h3 id="mutate-creating-new-columns">&lt;code>mutate&lt;/code>: Creating new columns&lt;/h3>
&lt;p>Using the pipe &lt;code>%&amp;gt;%&lt;/code> and the function &lt;code>mutate&lt;/code> in package &lt;code>dplyr&lt;/code>, I first constructed 7 new variables. Note the dependencies of the variables, so that I couldn&amp;rsquo;t construct the life tables at once.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>life_table &lt;span style="color:#f92672">&amp;lt;-&lt;/span> life_table &lt;span style="color:#f92672">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mutate&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;lx*mx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>lx&lt;span style="color:#f92672">*&lt;/span>mx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;x*lx*mx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>Age&lt;span style="color:#f92672">*&lt;/span>lx&lt;span style="color:#f92672">*&lt;/span>mx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;Lx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>(lx&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#a6e22e">lead&lt;/span>(lx))&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;Lx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">replace&lt;/span>(Lx, &lt;span style="color:#ae81ff">10&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;ex&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">rev&lt;/span>(&lt;span style="color:#a6e22e">cumsum&lt;/span>(&lt;span style="color:#a6e22e">rev&lt;/span>(Lx)))&lt;span style="color:#f92672">/&lt;/span>lx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;R0&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(lx&lt;span style="color:#f92672">*&lt;/span>mx),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;G&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(Age&lt;span style="color:#f92672">*&lt;/span>lx&lt;span style="color:#f92672">*&lt;/span>mx)&lt;span style="color:#f92672">/&lt;/span>R0,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;approx.r&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">log&lt;/span>(R0)&lt;span style="color:#f92672">/&lt;/span>G
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Two things worth noting in the &lt;code>mutate&lt;/code> function:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The code &lt;code>&amp;quot;Lx&amp;quot;=(lx+lead(lx))/2, &amp;quot;Lx&amp;quot;=replace(Lx, 10, 0)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;code>lead(lx)&lt;/code> shifts the whole column of $l_x$ to its next value, i.e. the column $l_x$ becomes $l_{(x+1)}$.&lt;/li>
&lt;li>Due to &lt;code>lead(lx)&lt;/code>, the last entry of the new column $L_x$ must be a &lt;code>NA&lt;/code>, so I have to assign &lt;code>0&lt;/code> to it (otherwise all calculations based on it will become &lt;code>NA&lt;/code>s).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The code &lt;code>&amp;quot;ex&amp;quot;=rev(cumsum(rev(Lx)))/lx&lt;/code> (This is where I was stuck)&lt;/p>
&lt;ol>
&lt;li>The numerator of &lt;code>ex&lt;/code> is calculated by summing over $L_x$ to $L_{max}$, the maximum age of $L_x$. This is not so intuitive when working with R.&lt;/li>
&lt;li>&lt;code>rev(Lx)&lt;/code> reverse the order of $L_x$, and &lt;code>cumsum()&lt;/code> is for &lt;em>cummulative sum&lt;/em>. &lt;code>cumsum(rev(Lx))&lt;/code> then is equivalent to &lt;em>summing $L_x$ backwards&lt;/em> (i.e. from $L_{max}$ to $L_x$).&lt;/li>
&lt;li>But since $L_x$ is reversed in the first place, &lt;code>cumsum(rev(Lx))&lt;/code> is also in reverse order. Reversing &lt;code>cumsum(rev(Lx))&lt;/code> with &lt;code>rev()&lt;/code> then gives what I want.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="calculating-r-by-while-loop">Calculating $r$ by while loop&lt;/h3>
&lt;p>By the &lt;strong>Eular-Lotka equation&lt;/strong>, I can calculate $r$.&lt;/p>
&lt;p>$$\displaystyle \sum_{all \hspace{0.3mm} x} e^{-rx}l_x m_x = 1$$&lt;/p>
&lt;p>Using &lt;code>while&lt;/code> loop and Approximate $r$ calculated earlier as the starting value for &lt;code>r&lt;/code>, $r$ is calculated as below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>df &lt;span style="color:#f92672">&amp;lt;-&lt;/span> &lt;span style="color:#a6e22e">as.data.frame&lt;/span>(life_table)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>r &lt;span style="color:#f92672">&amp;lt;-&lt;/span> &lt;span style="color:#ae81ff">0.0812198&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>x &lt;span style="color:#f92672">&amp;lt;-&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>r&lt;span style="color:#f92672">*&lt;/span>df&lt;span style="color:#f92672">$&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>df&lt;span style="color:#f92672">$&lt;/span>`lx*mx`)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">while &lt;/span>(&lt;span style="color:#a6e22e">abs&lt;/span>(x&lt;span style="color:#ae81ff">-1&lt;/span>) &lt;span style="color:#f92672">&amp;gt;=&lt;/span> &lt;span style="color:#ae81ff">0.000001&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#a6e22e">if &lt;/span>(x&lt;span style="color:#ae81ff">-1&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> r &lt;span style="color:#f92672">&amp;lt;-&lt;/span> r&lt;span style="color:#ae81ff">+0.00000001&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> else{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> r &lt;span style="color:#f92672">&amp;lt;-&lt;/span> r&lt;span style="color:#ae81ff">-0.00000001&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> x &lt;span style="color:#f92672">&amp;lt;-&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>r&lt;span style="color:#f92672">*&lt;/span>df&lt;span style="color:#f92672">$&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>df&lt;span style="color:#f92672">$&lt;/span>`lx*mx`)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> r
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The rest is simple, and the logic applied is the same.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>life_table &lt;span style="color:#f92672">&amp;lt;-&lt;/span> life_table &lt;span style="color:#f92672">%&amp;gt;%&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mutate&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;approx.r&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">log&lt;/span>(R0)&lt;span style="color:#f92672">/&lt;/span>G,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;r&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>r,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;Vx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">rev&lt;/span>(&lt;span style="color:#a6e22e">cumsum&lt;/span>(&lt;span style="color:#a6e22e">rev&lt;/span>(&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>r&lt;span style="color:#f92672">*&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>lx&lt;span style="color:#f92672">*&lt;/span>mx)))&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>r&lt;span style="color:#f92672">*&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>lx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;lx*mx*e^-rx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span> lx&lt;span style="color:#f92672">*&lt;/span>mx&lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>r&lt;span style="color:#f92672">*&lt;/span>Age),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;approx.Vx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">rev&lt;/span>(&lt;span style="color:#a6e22e">cumsum&lt;/span>(&lt;span style="color:#a6e22e">rev&lt;/span>(&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>approx.r&lt;span style="color:#f92672">*&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>lx&lt;span style="color:#f92672">*&lt;/span>mx)))&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>approx.r&lt;span style="color:#f92672">*&lt;/span>Age)&lt;span style="color:#f92672">*&lt;/span>lx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span> &lt;span style="color:#e6db74">&amp;#34;approx.lx*mx*e^-rx&amp;#34;&lt;/span>&lt;span style="color:#f92672">=&lt;/span> lx&lt;span style="color:#f92672">*&lt;/span>mx&lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>approx.r&lt;span style="color:#f92672">*&lt;/span>Age)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9&lt;/span>&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, I&amp;rsquo;m done constructing a life table.&lt;/p>
&lt;h2 id="printing-pretty-life-table">Printing pretty Life Table&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">kable&lt;/span>(life_table, format&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;markdown&amp;#34;&lt;/span>, align&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;c&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$Age$&lt;/th>
&lt;th style="text-align:center">$l_x$&lt;/th>
&lt;th style="text-align:center">$m_x$&lt;/th>
&lt;th style="text-align:center">$l_x m_x$&lt;/th>
&lt;th style="text-align:center">$x l_x m_x$&lt;/th>
&lt;th style="text-align:center">$L_x$&lt;/th>
&lt;th style="text-align:center">$e_x$&lt;/th>
&lt;th style="text-align:center">$R_0$&lt;/th>
&lt;th style="text-align:center">$G$&lt;/th>
&lt;th style="text-align:center">$Approximate$ &lt;br>$r$&lt;/th>
&lt;th style="text-align:center">$r$&lt;/th>
&lt;th style="text-align:center">$V_x$&lt;/th>
&lt;th style="text-align:center">$l_x m_x e^{-rx}$&lt;/th>
&lt;th style="text-align:center">$Approximate$ &lt;br>$V_x$&lt;/th>
&lt;th style="text-align:center">$Approximate$ &lt;br>$l_x m_x e^{-rx}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1.0000000&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0.00000&lt;/td>
&lt;td style="text-align:center">0.0000&lt;/td>
&lt;td style="text-align:center">0.5000310&lt;/td>
&lt;td style="text-align:center">0.500153&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">1.0000010&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">1.0099103&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.0000620&lt;/td>
&lt;td style="text-align:center">4600&lt;/td>
&lt;td style="text-align:center">0.28520&lt;/td>
&lt;td style="text-align:center">0.2852&lt;/td>
&lt;td style="text-align:center">0.0000480&lt;/td>
&lt;td style="text-align:center">1.967742&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000675&lt;/td>
&lt;td style="text-align:center">0.2620352&lt;/td>
&lt;td style="text-align:center">0.0000679&lt;/td>
&lt;td style="text-align:center">0.2629518&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0.0000340&lt;/td>
&lt;td style="text-align:center">8700&lt;/td>
&lt;td style="text-align:center">0.29580&lt;/td>
&lt;td style="text-align:center">0.5916&lt;/td>
&lt;td style="text-align:center">0.0000270&lt;/td>
&lt;td style="text-align:center">2.176471&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000297&lt;/td>
&lt;td style="text-align:center">0.2497000&lt;/td>
&lt;td style="text-align:center">0.0000299&lt;/td>
&lt;td style="text-align:center">0.2514499&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0.0000200&lt;/td>
&lt;td style="text-align:center">11600&lt;/td>
&lt;td style="text-align:center">0.23200&lt;/td>
&lt;td style="text-align:center">0.6960&lt;/td>
&lt;td style="text-align:center">0.0000178&lt;/td>
&lt;td style="text-align:center">2.350000&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000126&lt;/td>
&lt;td style="text-align:center">0.1799362&lt;/td>
&lt;td style="text-align:center">0.0000126&lt;/td>
&lt;td style="text-align:center">0.1818310&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">0.0000155&lt;/td>
&lt;td style="text-align:center">12700&lt;/td>
&lt;td style="text-align:center">0.19685&lt;/td>
&lt;td style="text-align:center">0.7874&lt;/td>
&lt;td style="text-align:center">0.0000132&lt;/td>
&lt;td style="text-align:center">1.887097&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000067&lt;/td>
&lt;td style="text-align:center">0.1402737&lt;/td>
&lt;td style="text-align:center">0.0000067&lt;/td>
&lt;td style="text-align:center">0.1422467&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">0.0000110&lt;/td>
&lt;td style="text-align:center">12700&lt;/td>
&lt;td style="text-align:center">0.13970&lt;/td>
&lt;td style="text-align:center">0.6985&lt;/td>
&lt;td style="text-align:center">0.0000087&lt;/td>
&lt;td style="text-align:center">1.454546&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000028&lt;/td>
&lt;td style="text-align:center">0.0914634&lt;/td>
&lt;td style="text-align:center">0.0000028&lt;/td>
&lt;td style="text-align:center">0.0930743&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">6&lt;/td>
&lt;td style="text-align:center">0.0000065&lt;/td>
&lt;td style="text-align:center">12700&lt;/td>
&lt;td style="text-align:center">0.08255&lt;/td>
&lt;td style="text-align:center">0.4953&lt;/td>
&lt;td style="text-align:center">0.0000042&lt;/td>
&lt;td style="text-align:center">1.115385&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000008&lt;/td>
&lt;td style="text-align:center">0.0496567&lt;/td>
&lt;td style="text-align:center">0.0000008&lt;/td>
&lt;td style="text-align:center">0.0507081&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">0.0000020&lt;/td>
&lt;td style="text-align:center">12700&lt;/td>
&lt;td style="text-align:center">0.02540&lt;/td>
&lt;td style="text-align:center">0.1778&lt;/td>
&lt;td style="text-align:center">0.0000020&lt;/td>
&lt;td style="text-align:center">1.500000&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000001&lt;/td>
&lt;td style="text-align:center">0.0140380&lt;/td>
&lt;td style="text-align:center">0.0000001&lt;/td>
&lt;td style="text-align:center">0.0143853&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">8&lt;/td>
&lt;td style="text-align:center">0.0000020&lt;/td>
&lt;td style="text-align:center">12700&lt;/td>
&lt;td style="text-align:center">0.02540&lt;/td>
&lt;td style="text-align:center">0.2032&lt;/td>
&lt;td style="text-align:center">0.0000010&lt;/td>
&lt;td style="text-align:center">0.500000&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000001&lt;/td>
&lt;td style="text-align:center">0.0128978&lt;/td>
&lt;td style="text-align:center">0.0000001&lt;/td>
&lt;td style="text-align:center">0.0132632&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">9&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0.00000&lt;/td>
&lt;td style="text-align:center">0.0000&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">NaN&lt;/td>
&lt;td style="text-align:center">1.2829&lt;/td>
&lt;td style="text-align:center">3.06727&lt;/td>
&lt;td style="text-align:center">0.0812198&lt;/td>
&lt;td style="text-align:center">0.0847117&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;td style="text-align:center">0.0000000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Note that I edited the column names of the table in a text editor to make it display in &lt;strong>LaTeX&lt;/strong> style. There is no simple &lt;code>knitr&lt;/code> function (at least I don&amp;rsquo;t know) that print out pretty displayed style text in a table generated from a data frame&lt;/p></description><category>R</category></item></channel></rss>