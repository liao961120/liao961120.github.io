<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>reproducibility on Yongfu's Blog</title><link>https://yongfu.name/tags/reproducibility/</link><description>Recent content in reproducibility on Yongfu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 06 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/reproducibility/feed.xml" rel="self" type="application/rss+xml"/><item><title>Demystifying Item Response Theory (2/4)</title><link>https://yongfu.name/2023/03/06/irt2/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/03/06/irt2/</guid><description>&lt;p>In &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>, we went through the simplest item response model,
the 1PL model, from the perspective of simulations. Starting with item
difficulty and testee ability, we &lt;strong>worked forward&lt;/strong> to simulate item
responses that mimic real-world data. Back then, we were precisely
laying out the &lt;strong>data generating process&lt;/strong> that is assumed by the item
response theory. In this post, we &lt;strong>work backward&lt;/strong>. We will start with
the item responses and work back toward the unobserved difficulties and
abilities, with the help of statistical models. But first, let’s
simulate the data we will be using!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) ) &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">13&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>n_item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">20&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>n_subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">80&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_resp &lt;span style="color:#f92672">=&lt;/span> n_subj &lt;span style="color:#f92672">*&lt;/span> n_item &lt;span style="color:#75715e"># number of responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n_subj, mean&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, sd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> ) &lt;span style="color:#75715e"># Subjects&amp;#39; ability&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1.6&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, length&lt;span style="color:#f92672">=&lt;/span>n_item ) &lt;span style="color:#75715e"># Items&amp;#39; difficulty&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># The data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( S&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj, I&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_item, KEEP.OUT.ATTRS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">FALSE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>(A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I]) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>S &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>S)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>I)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 1600 obs. of 3 variables:
$ S: Factor w/ 80 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
$ I: Factor w/ 20 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
$ R: int 1 0 1 1 1 1 1 1 0 1 ...
&lt;/code>&lt;/pre>
&lt;p>In the code above, the first two lines are the definitions for the
logistic and the Bernoulli functions used previously. The second chunk
of code sets the shape of our data. This time, the simulated data is
much larger, with 20 items and 80 testees, or &lt;strong>subjects&lt;/strong> (I will use
the more general term “subject” hereafter). Since we assume here that
each subject responds to every item, this gives us 1600 responses in the
data.&lt;/p>
&lt;p>The subject abilities come from a normal distribution with a zero mean
and a standard deviation of one (the standard normal). The item
difficulties are equally-spaced values that range from -1.6 to 1. A
notable change from the previous post is that &lt;strong>number indices&lt;/strong> are
used here for labeling items (&lt;code>I&lt;/code>) and subjects (&lt;code>S&lt;/code>). For simple
illustrations, letter indices are clearer. But for larger data sets,
number indices are easier to manipulate with code. Now, since &lt;code>S&lt;/code> and
&lt;code>I&lt;/code> are coded as integers, we need to explicitly convert them into
factors. Otherwise, the model will treat the number indices as values in
a continuous variable.&lt;/p>
&lt;h2 id="dags-revisited">DAGs Revisited&lt;/h2>
&lt;p>Before we move on to the statistical model, let me lay out the DAGs
again. The DAG on the right below is identical to the one in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a> (the left DAG here), but with a slight modification that
emphasizes the perspective from the statistical model. Here, the
observed $S$ and $I$ take place of the unobserved $A$ and $D$,
respectively. So why the difference?&lt;/p>
&lt;div class="goat svg-container ">
&lt;svg
xmlns="http://www.w3.org/2000/svg"
font-family="Menlo,Lucida Console,monospace"
viewBox="0 0 352 137"
>
&lt;g transform='translate(8,16)'>
&lt;path d='M 88,16 L 120,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 280,16 L 312,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 168,16 L 168,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 32,80 L 48,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 224,80 L 240,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,48 L 96,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,48 L 288,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 48,48 L 56,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='66.000000,48.000000 54.000000,42.400002 54.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 48.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 72,32 L 80,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='98.000000,48.000000 86.000000,42.400002 86.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 80.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='128.000000,16.000000 116.000000,10.400000 116.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 120.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 240,48 L 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='258.000000,48.000000 246.000000,42.400002 246.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 240.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 264,32 L 272,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='290.000000,48.000000 278.000000,42.400002 278.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 272.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='320.000000,16.000000 308.000000,10.400000 308.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 312.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 64,0 A 16,16 0 0,0 48,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 64,0 A 16,16 0 0,1 80,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 256,0 A 16,16 0 0,0 240,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 256,0 A 16,16 0 0,1 272,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 48,16 A 16,16 0 0,0 64,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,16 A 16,16 0 0,1 64,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 240,16 A 16,16 0 0,0 256,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,16 A 16,16 0 0,1 256,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 A 16,16 0 0,0 8,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 A 16,16 0 0,1 40,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,80 A 16,16 0 0,0 88,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 104,80 A 16,16 0 0,1 120,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 8,96 A 16,16 0 0,0 24,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 40,96 A 16,16 0 0,1 24,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 88,96 A 16,16 0 0,0 104,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 120,96 A 16,16 0 0,1 104,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;text text-anchor='middle' x='24' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='64' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='104' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='144' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;text text-anchor='middle' x='216' y='100' fill='currentColor' style='font-size:1em'>S&lt;/text>
&lt;text text-anchor='middle' x='256' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='296' y='100' fill='currentColor' style='font-size:1em'>I&lt;/text>
&lt;text text-anchor='middle' x='336' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;/g>
&lt;/svg>
&lt;/div>
&lt;p>Recall that the nodes $A$ and $D$ represent the joint influences of a
subject’s ability and an item’s difficulty on the probability of success
on that item. However, the statistical model cannot notice $A$ and $D$
since they are &lt;strong>theoretical concepts&lt;/strong> proposed by the IRT. What the
model “sees” is more similar to the DAG on the right. This DAG is
theoretically neutral. All it says is that the probability of success is
influenced by the particular subject and item present in an observation.
It does not further comment on the factors underlying each subject/item
that lead to the results.&lt;/p>
&lt;p>Given the data and the right DAG, the statistical model estimates the
so-called &lt;strong>subject effects&lt;/strong> and &lt;strong>item effects&lt;/strong>. These effects will
be estimates of subject ability and item difficulty &lt;strong>if the IRT
assumptions are met&lt;/strong>: when a subject and an item influence the result
&lt;strong>only through subject ability and item difficulty&lt;/strong>. With the concepts
of &lt;strong>subject/item effects&lt;/strong> in place, we can move on to the formulas of
the statistical model.&lt;/p>
&lt;h2 id="equation-index-and-annoying-things">Equation, Index and Annoying Things&lt;/h2>
&lt;p>The equations in (1) are the formulation of our model. This model is
known as the &lt;a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
regression&lt;/a>, or in
GLM terms, the Generalized Linear Model of the binomial family with the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit link&lt;/a> (more on this later).
Lots of things are going on here. Don’t panic, I’ll walk you through
slowly.&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim Bernoulli( P_i ) \\
&amp;amp; P_i = logistic( \mu_i ) \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{1}
$$&lt;/p>
&lt;p>First, note the common subscript $_i$ to the variables above. The
presence of this common $_i$ indicates that &lt;strong>the equations are read at
the &lt;em>observational&lt;/em> level&lt;/strong>. The observational level is easier to think
of with help of the &lt;a href="https://en.wikipedia.org/wiki/Wide_and_narrow_data">long data
format&lt;/a>. In this
long form of data, each row records an observation and is indexed by the
subscript $_i$. So you can think of the set of three equations as
describing the links among the variables for each observation. Note that
the long data format is also the format we have been using for the data
frames.&lt;/p>
&lt;p>The last equation in (1), also related to the reading of the subscript
$_i$, deserves some elaboration, as some might feel confused about the
square brackets after $\alpha$ and $\delta$. Actually, we have already
met these brackets in &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>. The brackets here serve a similar
function to R’s subset function &lt;code>[]&lt;/code> that we have used for linking
particular ability/difficulty levels of a subject/item to the rows
(observations) of the data frame. So what the square brackets after
$\alpha$ and $\delta$ do exactly, is to “look up” the index of the
subject and item for the $_i$th observation such that the $\alpha$
corresponding to the subject and the $\delta$ corresponding to the item
could be correctly retrieved. The model can thus “know” which $\alpha$
and $\delta$ to update when it encounters an observation. For instance,
suppose we are on the 3rd row (observation) of the data, in which
$S_3 = 5$ and $I_3 = 8$. This tells the model that the observation gives
information about $\alpha_5$ and $\delta_8$. The model thus learns
something about them and updates accordingly.&lt;/p>
&lt;p>I haven’t written about $\alpha$ and $\delta$ yet, but based on the
previous paragraph, you might already know what they are: $\alpha$s are
the subject effects and $\delta$s the item effects to be estimated by
the model.&lt;/p>
&lt;p>Now, let me walk you through the equations from bottom to top:&lt;/p>
&lt;ul>
&lt;li>$\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$&lt;br>
No surprise here. This equation simply illustrates how the model
computes a new variable $\mu$ from $\alpha$ and $\delta$.&lt;/li>
&lt;li>$P_i = logistic( \mu_i )$&lt;br>
The equation should look familiar. It indicates how the model maps
$\mu$, which can range from $-\infty$ to $\infty$, to probability,
$P$, through the logistic function.&lt;/li>
&lt;li>$R_i \sim Bernoulli( P_i )$&lt;br>
This equation describes that each observed response is generated from
a Bernoulli distribution with probability $P_i$. Or even simpler,
$R_i$ would be $1$ with probability $P_i$ and $0$ with probability
$1 - P_i$.&lt;/li>
&lt;/ul>
&lt;p>These equations all look familiar because they are essentially
mathematical representations of the simulation we have done. Here, the
model formulation is simply simulation in reverse.&lt;/p>
&lt;h3 id="the-logit-link">The Logit Link&lt;/h3>
&lt;p>The GLM formulation of (1) is often seen in an alternative form in (2).
The only difference between (2) and (1) lies in the second equation.
Instead of the logistic function, the second equation in (2) uses the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit&lt;/a> function. What is the
logit?&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim Bernoulli( P_i ) \\
&amp;amp; logit(P_i) = \mu_i \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{2}
$$&lt;/p>
&lt;p>The logit function is simply the &lt;strong>mirror of the logistic&lt;/strong>. They do the
same mapping but in &lt;strong>reverse directions&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>the logistic function maps real numbers to probabilities&lt;/li>
&lt;li>the logit function maps probabilities to real numbers&lt;/li>
&lt;/ul>
&lt;p>The logistic and the logit are &lt;strong>inverse functions&lt;/strong> to each other. So
if a real number gets converted to the probability by the logistic, the
logit can convert it back to the original real, and vice versa.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>logit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p ) &lt;span style="color:#a6e22e">log&lt;/span>( p&lt;span style="color:#f92672">/&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">-&lt;/span>p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(x) ) &lt;span style="color:#75715e"># Transformed x on probability space&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] 0.2689414 0.2890505 0.3100255 0.3318122 0.3543437 0.3775407 0.4013123
[8] 0.4255575 0.4501660 0.4750208 0.5000000 0.5249792 0.5498340 0.5744425
[15] 0.5986877 0.6224593 0.6456563 0.6681878 0.6899745 0.7109495 0.7310586
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Logit gives x back&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">logit&lt;/span>(p)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4
[16] 0.5 0.6 0.7 0.8 0.9 1.0
&lt;/code>&lt;/pre>
&lt;p>Some arithmetics would get us from the logistic to the logit:&lt;/p>
&lt;p>$$
\begin{aligned}
logistic(x) &amp;amp;= \frac{1}{1 + e^{-x}} = p \\
&amp;amp; \Rightarrow ~~ e^{-x} = \frac{1-p}{p} \\
&amp;amp; \Rightarrow ~ -x = log(\frac{1-p}{p}) \\
&amp;amp; \Rightarrow ~~ x = -log(\frac{1-p}{p}) = log(\frac{p}{1-p}) = logit(p)
\end{aligned}
$$&lt;/p>
&lt;p>There is really nothing special about the logit function. We have
learned all the important things through the logistic back in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a>. I mention the logit here simply because the term is
frequently used. When people talk about GLMs, they prefer to use the
&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">link
function&lt;/a>
to characterize the model. The link function, in the case of the
logistic regression here, is the logit function. It transforms the
outcome probabilities into real numbers that are modeled linearly. It’s
just the logistic, but works in the reverse direction.&lt;/p>
&lt;h2 id="fitting-glm">Fitting GLM&lt;/h2>
&lt;p>Now, we are packed with the statistical muscles to carry out the
analysis. Let’s fit the model on the data we’ve simulated. In R, this is
done through the function &lt;code>glm()&lt;/code>. The first argument of &lt;code>glm()&lt;/code> is the
formula, in which we specify our linear model with R’s model syntax.
There are in principle two ways, one succinct and the other tedious, to
express the formula &lt;strong>when there are &lt;em>categorical predictors&lt;/em> in the
model&lt;/strong>. I will first demonstrate the tedious one, as it exposes all the
details hidden by the succinct form. Though tedious, it saves us from
confusion.&lt;/p>
&lt;h3 id="dummy-coding">Dummy Coding&lt;/h3>
&lt;p>The formulas we specify in &lt;code>glm()&lt;/code> (and other model fitting functions in
general) correspond pretty well to their mathematical counterparts. So
let me first present the math before we move on to the code. Lots of
things to explain here.&lt;/p>
&lt;p>Equation (3.2) is rewritten from the last two equations,
$logit(P_i) = \mu_i$ and $\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$ in
(2), which I reproduce here in Equation (3.1) by combining the two
equations.&lt;/p>
&lt;p>Earlier I mentioned that the square brackets after $\alpha$ and $\delta$
serve as a “look up” function to locate the relevant $\alpha$ and
$\delta$ of each subject and item in an observation. There is an
equivalent way to express the same formula without the use of these
“look up” functions, which is shown in equation (3.2). For the sake of
simplicity, let’s assume here that we have only two items (A, B) and
three subjects (J, K, L). For real data, equation (3.2) would be
extremely long.&lt;/p>
&lt;p>$$
\begin{align}
\tag{3.1} logit(\mu_i) &amp;amp;= \alpha_{[S_i]} + \delta_{[I_i]} \\
\tag{3.2} logit(\mu_i) &amp;amp;= J_i \alpha_J + K_i \alpha_K + L_i \alpha_L + A_i \delta_A + B_i \delta_B
\end{align}
$$&lt;/p>
&lt;p>The variables ($J_i, K_i, L_i, A_i, B_i$) in front of the $\alpha$s and
$\delta$s have a value of either 0 or 1. Here, they serve as a “switch”
that turns on the relevant $\alpha$ and $\delta$ and turns off the
others in each observation. This is easier to see with the help of the
tables below. Table 3.1 corresponds to Equation (3.1), and Table 3.2
corresponds to Equation (3.2). So, for instance, in row 2 of Table 3.2,
$K$ and $A$ are 1 while the others are 1. This turns on, or picks out,
$\alpha_K$ and $\delta_A$. As such, they would be updated by the model
when it reaches this observation. In row 2 of Table 3.1, $\alpha_K$ and
$\delta_A$ are picked out too, but not by the switches. They are
directly picked out through the &lt;em>K&lt;/em> and &lt;em>A&lt;/em> present in the row.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.1
&lt;/th>
&lt;th>
Table 3.2
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$S$&lt;/th>
&lt;th style="text-align:center">$I$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">J&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">K&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">L&lt;/td>
&lt;td style="text-align:center">B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>The re-expression of Table 3.1 as Table 3.2 by coding the categories
into zeros and ones is known as &lt;strong>dummy coding&lt;/strong>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Why do we need
dummy coding? In short, this is because regression programs do not
“understand” the difference between categorical and continuous
variables. They read only numbers. Dummy coding is essentially
representing categorical variables as continuous ones so that the
program would know how to deal with them. Most programs dummy code for
the users (such as &lt;code>glm()&lt;/code>) if you give them categories. But there are
various ways to dummy code the categories and each of which results in a
different output. The interpretation of the output coefficients depends
on how the categories were coded. This confuses the novice as too much
is happening under the hood.&lt;/p>
&lt;h3 id="coding-models-the-long-route">Coding models: the long route&lt;/h3>
&lt;p>With the concepts of dummy coding in place, let’s code the model. I use
&lt;code>dummy_cols()&lt;/code> from the &lt;code>fastDummies&lt;/code> package to help me with dummy
coding. In the code below, I recode the item and subject variables into
zeros and ones. The result is identical to Table 3.2, except that it now
expands to 20 items and 80 subjects (100 columns in total). I won’t
print out the full dummy-coded data frame to save space. But be sure to
take a look at &lt;code>d_dummy&lt;/code> to see how it corresponds to Table 3.2.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(fastDummies)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>d_dummy &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">dummy_cols&lt;/span>( d, &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>), remove_selected_columns&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">TRUE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">dim&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 1600 101
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">colnames&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] &amp;quot;R&amp;quot; &amp;quot;I_1&amp;quot; &amp;quot;I_2&amp;quot; &amp;quot;I_3&amp;quot; &amp;quot;I_4&amp;quot; &amp;quot;I_5&amp;quot; &amp;quot;I_6&amp;quot; &amp;quot;I_7&amp;quot; &amp;quot;I_8&amp;quot; &amp;quot;I_9&amp;quot;
[11] &amp;quot;I_10&amp;quot; &amp;quot;I_11&amp;quot; &amp;quot;I_12&amp;quot; &amp;quot;I_13&amp;quot; &amp;quot;I_14&amp;quot; &amp;quot;I_15&amp;quot; &amp;quot;I_16&amp;quot; &amp;quot;I_17&amp;quot; &amp;quot;I_18&amp;quot; &amp;quot;I_19&amp;quot;
[21] &amp;quot;I_20&amp;quot; &amp;quot;S_1&amp;quot; &amp;quot;S_2&amp;quot; &amp;quot;S_3&amp;quot; &amp;quot;S_4&amp;quot; &amp;quot;S_5&amp;quot; &amp;quot;S_6&amp;quot; &amp;quot;S_7&amp;quot; &amp;quot;S_8&amp;quot; &amp;quot;S_9&amp;quot;
[31] &amp;quot;S_10&amp;quot; &amp;quot;S_11&amp;quot; &amp;quot;S_12&amp;quot; &amp;quot;S_13&amp;quot; &amp;quot;S_14&amp;quot; &amp;quot;S_15&amp;quot; &amp;quot;S_16&amp;quot; &amp;quot;S_17&amp;quot; &amp;quot;S_18&amp;quot; &amp;quot;S_19&amp;quot;
[41] &amp;quot;S_20&amp;quot; &amp;quot;S_21&amp;quot; &amp;quot;S_22&amp;quot; &amp;quot;S_23&amp;quot; &amp;quot;S_24&amp;quot; &amp;quot;S_25&amp;quot; &amp;quot;S_26&amp;quot; &amp;quot;S_27&amp;quot; &amp;quot;S_28&amp;quot; &amp;quot;S_29&amp;quot;
[51] &amp;quot;S_30&amp;quot; &amp;quot;S_31&amp;quot; &amp;quot;S_32&amp;quot; &amp;quot;S_33&amp;quot; &amp;quot;S_34&amp;quot; &amp;quot;S_35&amp;quot; &amp;quot;S_36&amp;quot; &amp;quot;S_37&amp;quot; &amp;quot;S_38&amp;quot; &amp;quot;S_39&amp;quot;
[61] &amp;quot;S_40&amp;quot; &amp;quot;S_41&amp;quot; &amp;quot;S_42&amp;quot; &amp;quot;S_43&amp;quot; &amp;quot;S_44&amp;quot; &amp;quot;S_45&amp;quot; &amp;quot;S_46&amp;quot; &amp;quot;S_47&amp;quot; &amp;quot;S_48&amp;quot; &amp;quot;S_49&amp;quot;
[71] &amp;quot;S_50&amp;quot; &amp;quot;S_51&amp;quot; &amp;quot;S_52&amp;quot; &amp;quot;S_53&amp;quot; &amp;quot;S_54&amp;quot; &amp;quot;S_55&amp;quot; &amp;quot;S_56&amp;quot; &amp;quot;S_57&amp;quot; &amp;quot;S_58&amp;quot; &amp;quot;S_59&amp;quot;
[81] &amp;quot;S_60&amp;quot; &amp;quot;S_61&amp;quot; &amp;quot;S_62&amp;quot; &amp;quot;S_63&amp;quot; &amp;quot;S_64&amp;quot; &amp;quot;S_65&amp;quot; &amp;quot;S_66&amp;quot; &amp;quot;S_67&amp;quot; &amp;quot;S_68&amp;quot; &amp;quot;S_69&amp;quot;
[91] &amp;quot;S_70&amp;quot; &amp;quot;S_71&amp;quot; &amp;quot;S_72&amp;quot; &amp;quot;S_73&amp;quot; &amp;quot;S_74&amp;quot; &amp;quot;S_75&amp;quot; &amp;quot;S_76&amp;quot; &amp;quot;S_77&amp;quot; &amp;quot;S_78&amp;quot; &amp;quot;S_79&amp;quot;
[101] &amp;quot;S_80&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now, we can fit the model with the dummy-coded data &lt;code>d_dummy&lt;/code>. The first
argument in &lt;code>glm()&lt;/code> specifies the formula in R’s model syntax. Based on
Equation (3.2), we include all the dummy variables in the table. In R,
this means typing out all the variables as
&lt;code>R ~ S_1 + S_2 + ... + S_80 + I_1 + I_2 + ... + I_20&lt;/code>. That’s a lot of
work!&lt;/p>
&lt;p>Luckily, R provides a handy syntax for this. Since we are including all
the variables except the outcome on the right-hand side of the formula,
we can simply type &lt;code>R ~ .&lt;/code>. Here, the dot serves as a placeholder for
all the remaining variables not specified in the formula. We also need a
&lt;code>-1&lt;/code> in front of the dot: &lt;code>R ~ -1 + .&lt;/code>. The &lt;code>-1&lt;/code> tells the model not to
estimate a global intercept, which is done by default&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. We don’t need
a global intercept here since we want all the effects to be presented in
the items and subjects. If a global intercept is estimated, it will
“suck out” what should have been part of the subject/item effects,
rendering the results hard to interpret.&lt;/p>
&lt;p>The last thing to note in &lt;code>glm()&lt;/code> is the &lt;code>family&lt;/code> argument, which
characterizes the type of GLM used. Since we are fitting the data with
logistic regression, we pass &lt;code>binomial(&amp;quot;logit&amp;quot;)&lt;/code> to &lt;code>family&lt;/code>. The GLM
will then adopt the binomial distribution&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> with the logit link to map
the right-hand linear terms to the outcome space.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">summary&lt;/span>(m1)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Call:
glm(formula = R ~ -1 + ., family = binomial(&amp;quot;logit&amp;quot;), data = d_dummy)
Deviance Residuals:
Min 1Q Median 3Q Max
-2.7802 -0.8847 0.3734 0.8776 2.4877
Coefficients: (1 not defined because of singularities)
Estimate Std. Error z value Pr(&amp;gt;|z|)
I_1 1.197e+00 5.710e-01 2.096 0.03605 *
I_2 1.197e+00 5.710e-01 2.096 0.03605 *
I_3 1.108e+00 5.677e-01 1.951 0.05102 .
I_4 6.308e-01 5.547e-01 1.137 0.25546
I_5 4.872e-01 5.520e-01 0.883 0.37742
I_6 2.813e-01 5.489e-01 0.512 0.60835
I_7 1.928e-02 5.464e-01 0.035 0.97186
I_8 8.375e-02 5.469e-01 0.153 0.87829
I_9 -2.966e-01 5.451e-01 -0.544 0.58643
I_10 1.488e-01 5.475e-01 0.272 0.78572
I_11 -3.589e-01 5.451e-01 -0.658 0.51022
I_12 -5.458e-01 5.454e-01 -1.001 0.31699
I_13 -8.601e-01 5.473e-01 -1.571 0.11607
I_14 -7.335e-01 5.463e-01 -1.343 0.17940
I_15 -1.712e-01 5.454e-01 -0.314 0.75357
I_16 -1.321e+00 5.532e-01 -2.389 0.01691 *
I_17 -1.253e+00 5.521e-01 -2.270 0.02323 *
I_18 -9.240e-01 5.479e-01 -1.686 0.09171 .
I_19 -1.461e+00 5.558e-01 -2.629 0.00855 **
I_20 -2.003e+00 5.700e-01 -3.515 0.00044 ***
S_1 1.520e+00 7.375e-01 2.062 0.03925 *
S_2 -2.412e-01 6.952e-01 -0.347 0.72858
S_3 1.520e+00 7.375e-01 2.062 0.03925 *
S_4 2.371e-01 6.891e-01 0.344 0.73081
S_5 2.233e+00 8.196e-01 2.725 0.00643 **
S_6 -2.275e-15 6.906e-01 0.000 1.00000
S_7 2.233e+00 8.196e-01 2.725 0.00643 **
S_8 1.520e+00 7.375e-01 2.062 0.03925 *
S_9 1.231e+00 7.173e-01 1.717 0.08602 .
S_10 1.520e+00 7.375e-01 2.062 0.03925 *
S_11 -4.913e-01 7.035e-01 -0.698 0.48492
S_12 1.846e+00 7.686e-01 2.402 0.01631 *
S_13 -1.044e+00 7.366e-01 -1.418 0.15620
S_14 -1.369e+00 7.674e-01 -1.784 0.07439 .
S_15 -2.009e-15 6.906e-01 0.000 1.00000
S_16 2.371e-01 6.891e-01 0.344 0.73081
S_17 1.231e+00 7.173e-01 1.717 0.08602 .
S_18 9.661e-01 7.040e-01 1.372 0.16999
S_19 -3.203e-15 6.906e-01 0.000 1.00000
S_20 7.156e-01 6.955e-01 1.029 0.30349
S_21 1.231e+00 7.173e-01 1.717 0.08602 .
S_22 2.736e+00 9.148e-01 2.991 0.00278 **
S_23 1.846e+00 7.686e-01 2.402 0.01631 *
S_24 -1.369e+00 7.674e-01 -1.784 0.07439 .
S_25 4.742e-01 6.907e-01 0.687 0.49234
S_26 1.231e+00 7.173e-01 1.717 0.08602 .
S_27 -1.044e+00 7.366e-01 -1.418 0.15620
S_28 -7.563e-01 7.166e-01 -1.055 0.29128
S_29 -2.412e-01 6.952e-01 -0.347 0.72858
S_30 -2.412e-01 6.952e-01 -0.347 0.72858
S_31 2.371e-01 6.891e-01 0.344 0.73081
S_32 1.520e+00 7.375e-01 2.062 0.03925 *
S_33 4.742e-01 6.907e-01 0.687 0.49234
S_34 2.371e-01 6.891e-01 0.344 0.73081
S_35 -2.412e-01 6.952e-01 -0.347 0.72858
S_36 -2.412e-01 6.952e-01 -0.347 0.72858
S_37 -2.412e-01 6.952e-01 -0.347 0.72858
S_38 4.742e-01 6.907e-01 0.687 0.49234
S_39 2.371e-01 6.891e-01 0.344 0.73081
S_40 -1.044e+00 7.366e-01 -1.418 0.15620
S_41 7.156e-01 6.955e-01 1.029 0.30349
S_42 1.520e+00 7.375e-01 2.062 0.03925 *
S_43 2.233e+00 8.196e-01 2.725 0.00643 **
S_44 2.736e+00 9.148e-01 2.991 0.00278 **
S_45 7.156e-01 6.955e-01 1.029 0.30349
S_46 2.371e-01 6.891e-01 0.344 0.73081
S_47 -1.958e-15 6.906e-01 0.000 1.00000
S_48 -1.755e+00 8.180e-01 -2.145 0.03194 *
S_49 1.520e+00 7.375e-01 2.062 0.03925 *
S_50 7.156e-01 6.955e-01 1.029 0.30349
S_51 1.231e+00 7.173e-01 1.717 0.08602 .
S_52 1.846e+00 7.686e-01 2.402 0.01631 *
S_53 -4.913e-01 7.035e-01 -0.698 0.48492
S_54 4.742e-01 6.907e-01 0.687 0.49234
S_55 4.742e-01 6.907e-01 0.687 0.49234
S_56 -1.915e-15 6.906e-01 0.000 1.00000
S_57 -1.044e+00 7.366e-01 -1.418 0.15620
S_58 1.846e+00 7.686e-01 2.402 0.01631 *
S_59 1.520e+00 7.375e-01 2.062 0.03925 *
S_60 9.661e-01 7.040e-01 1.372 0.16999
S_61 -4.913e-01 7.035e-01 -0.698 0.48492
S_62 2.371e-01 6.891e-01 0.344 0.73081
S_63 -4.913e-01 7.035e-01 -0.698 0.48492
S_64 4.742e-01 6.907e-01 0.687 0.49234
S_65 1.846e+00 7.686e-01 2.402 0.01631 *
S_66 -4.913e-01 7.035e-01 -0.698 0.48492
S_67 2.371e-01 6.891e-01 0.344 0.73081
S_68 7.156e-01 6.955e-01 1.029 0.30349
S_69 2.233e+00 8.196e-01 2.725 0.00643 **
S_70 4.742e-01 6.907e-01 0.687 0.49234
S_71 -7.563e-01 7.166e-01 -1.055 0.29128
S_72 9.661e-01 7.040e-01 1.372 0.16999
S_73 -3.227e-15 6.906e-01 0.000 1.00000
S_74 -1.755e+00 8.180e-01 -2.145 0.03194 *
S_75 -4.913e-01 7.035e-01 -0.698 0.48492
S_76 -1.044e+00 7.366e-01 -1.418 0.15620
S_77 2.233e+00 8.196e-01 2.725 0.00643 **
S_78 4.742e-01 6.907e-01 0.687 0.49234
S_79 2.371e-01 6.891e-01 0.344 0.73081
S_80 NA NA NA NA
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 2218.1 on 1600 degrees of freedom
Residual deviance: 1719.1 on 1501 degrees of freedom
AIC: 1917.1
Number of Fisher Scoring iterations: 5
&lt;/code>&lt;/pre>
&lt;p>Take a look at the output. Something’s strange. Since there are 20 items
and 80 subjects in the data, we expect the model to return 100
coefficients, one for each subject/item effect. However, the last
coefficient, &lt;code>S_80&lt;/code> in this case, is &lt;code>NA&lt;/code>. The model does not
estimate this coefficient. Why?&lt;/p>
&lt;h4 id="identifiability">Identifiability&lt;/h4>
&lt;p>The reason is that there are &lt;strong>infinite&lt;/strong> sets of parameter combinations
that generate the same probabilities underlying our data. Thus, the
model is unable to work in reverse to infer a unique set of coefficients
from the data. To deal with this issue, R silently sets a constraint on
the parameters: it simply drops one of the parameters and estimates the
rest. When this is done, the remaining parameters become
&lt;a href="https://en.wikipedia.org/wiki/Identifiability">identifiable&lt;/a>, and the
model would be able to estimate them.&lt;/p>
&lt;p>But still, where did the infinity come from? Didn’t we simulate the
data? We didn’t introduce infinity, did we?&lt;/p>
&lt;p>We &lt;em>&lt;strong>did&lt;/strong>&lt;/em> actually, in silence. Recall that the probability of
success on an item is determined by the difference between ability and
difficulty. Since it is the &lt;em>&lt;strong>difference&lt;/strong>&lt;/em> that matters, there could
be an infinite number of ability and difficulty pairs that yield the
same difference. By adding any common value to a pair, we get a new pair
of ability and difficulty that yields the same probability. The code
below demonstrates this. Here, I shift the ability and difficulty levels
by a common value &lt;code>s&lt;/code>. The resulting probabilities should be identical
before and after the shift (except for a tiny floating point
imprecision). You can play with the code below by changing the value of
&lt;code>s&lt;/code>. Identical results should always be yielded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Shift A/D by a common factor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>A2 &lt;span style="color:#f92672">=&lt;/span> A &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>D2 &lt;span style="color:#f92672">=&lt;/span> D &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>p1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities before shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A2[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D2[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities after shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sum&lt;/span>( &lt;span style="color:#a6e22e">abs&lt;/span>(p1 &lt;span style="color:#f92672">-&lt;/span> p2) ) &lt;span style="color:#75715e"># Should be extremely close to zero&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 7.013834e-14
&lt;/code>&lt;/pre>
&lt;p>The way R deals with this issue of identifiability is not preferable
since we want to recover the parameters in our simulation (i.e., the set
of parameters without the shift). To get around R’s default treatment,
we have to impose constraints on the parameters ourselves.&lt;/p>
&lt;p>Recall that subject abilities are generated according to a standard
normal distribution in the simulation. Since the standard normal has a
mean of zero, the &lt;strong>expectation of the sum of subject abilities is
&lt;em>zero&lt;/em>&lt;/strong>. We can use this expectation as a constraint to the parameters
by constraining the subject effects to &lt;strong>sum to zero&lt;/strong>. This constraint,
however, &lt;strong>does not&lt;/strong> scale the model’s estimates to perfectly match the
true parameters since the true subject abilities never exactly sum to
zero in a single run of the simulation. However, the relative scale
would be close enough for the simulated parameters to be comparable to
those recovered by the model. Later in the next post, when the
&lt;strong>generalized linear mixed model&lt;/strong> is introduced, you will see that
there is no need to impose such a constraint. The constraint is
naturally included through the model’s assumptions. The estimated
subject effects then, do not need to sum to zero. Subject effects would
be assumed to result from a normal distribution with a mean of zero.&lt;/p>
&lt;p>Through dummy coding, we can impose the sum-to-zero constraint on the
subject effects. I illustrate this with the example previously presented
in Table 3.2, where there are only 3 subjects and 2 items. Table 3.3 is
re-coded from Table 3.2, in which the sum-to-zero constraint is imposed.&lt;/p>
&lt;p>The sum-to-zero constraint is imposed by dropping one of the subjects
and coding the remaining as &lt;code>-1&lt;/code> for all the observations where the
dropped subject is originally coded as &lt;code>1&lt;/code>. This is shown in Table 3.3,
where I drop subject $L$ (hence the &lt;code>-&lt;/code> in the column) and code the 3rd
row of $J$ and $K$ as &lt;code>-1&lt;/code>.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.2
&lt;/th>
&lt;th>
Table 3.3
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>With the coding scheme in Table 3.3, the estimated effect of subject $L$
can be reconstructed from the remaining subject effects returned by the
model. Since the sum of all subject effects is zero, the effect of
subject $L$ will be the negative of the others’ sum. This might seem a
bit confusing. But notice that the sum-to-zero constraint
&lt;strong>simultaneously applies to all effects in the variable&lt;/strong>. Once the
effect of the dropped category is reconstructed, each effect will also
be the negative sum of the remaining effects.&lt;/p>
&lt;p>Let’s impose this constraint on the data with code. Here, I will drop
the first subject &lt;code>S_1&lt;/code>. You can choose any subject you like to drop,
and the result will be identical. The code from line 3 to 5 below pick
outs the rows where &lt;code>S_1&lt;/code> is coded as &lt;code>1&lt;/code> and recode them as &lt;code>-1&lt;/code> on all
the subject columns. After the re-coding, the final line of code then
drops &lt;code>S_1&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>d_dummy2 &lt;span style="color:#f92672">=&lt;/span> d_dummy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>toDrop &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;S_1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>allCategories &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">startsWith&lt;/span>( &lt;span style="color:#a6e22e">names&lt;/span>(d_dummy2), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>idx_recode &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">which&lt;/span>( d_dummy2[[toDrop]] &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>d_dummy2[idx_recode, allCategories] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>d_dummy2[[toDrop]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NULL&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, let’s refit the model with this constraint-coded data. I simply
replace &lt;code>d_dummy&lt;/code> with &lt;code>d_dummy2&lt;/code> in the &lt;code>data&lt;/code> argument. Everything
else is the same.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy2, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># summary(m1.1)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you print out the coefficients of the fitted model, you will see that
the result is what we expected. The model returns 99 coefficients, which
match the number of the predictor variables we passed in. No coefficient
is dropped. We already dropped it for the model. And since we dropped
the predictor in a principled way, we know how to reconstruct it. The
effect of the dropped &lt;code>S_1&lt;/code> will be the negative sum of the remaining.
This is shown in the code below, which reconstructs all the item/subject
effects from the model’s coefficients.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>item_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct S_1 from the remaining subject effects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff), subj_eff )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now plot the estimated effects against the true parameter values
from the simulation. The figures below plot the estimated effects on the
x-axis and the true parameters of the y-axis. The dashed line has a
slope of 1 without an intercept. This line indicates perfect matches
between the truth and the estimates. Notice that for the figure on the
right, I reverse the signs of the item effects to match the scale of
item difficulty. This is necessary since $D$ is subtracted from $A$ in
the simulation. In other words, the effect of difficulty assumed by the
1PL model is &lt;strong>negative&lt;/strong>: the larger the difficulty, the less
probability of success on the item. However, &lt;code>glm()&lt;/code> allows only
additive effects. The effects in the model are summed together to yield
predictions. Hence, the item effects estimated by &lt;code>glm()&lt;/code> will be the
negative of those assumed by the 1PL model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( subj_eff, A, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>item_eff, D, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="two-column">
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-1.svg"
data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-2.svg"
data-fig-align="center" />&lt;/p>
&lt;/div>
&lt;p>As seen in both figures, the dots scatter around the lines quite
randomly, which indicates that the model does recover the parameters. To
have a clearer view of the estimates’ accuracy, let me plot some more.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set figure margins&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1.6&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>true_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(D, A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>est_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>item_eff, subj_eff)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_param &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">+&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>cols &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, n_item), &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">4&lt;/span>, n_subj) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.4&lt;/span>, &lt;span style="color:#ae81ff">2.4&lt;/span>), xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, n_param&lt;span style="color:#ae81ff">+1&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Effect&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#ae81ff">+0.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( true_eff, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_eff, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_param)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i, i), &lt;span style="color:#a6e22e">c&lt;/span>(true_eff[i], est_eff[i]), col&lt;span style="color:#f92672">=&lt;/span>cols[i] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mtext&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Items&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Subjects&amp;#34;&lt;/span>), at&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">61&lt;/span>), padj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>In the plot above, I overlay the estimated effects onto the true
parameters. The dots are the true parameters and the circles are the
model’s estimates. The vertical lines connecting the dots and the
circles show the distances between the truth and the estimates. It is
obvious from the plot that, compared to item difficulties, subject
abilities are harder to estimate, as the distances to the truth are in
general larger for subject estimates. This is apparent in hindsight, as
each item is taken by 80 subjects whereas each subject only takes 20
items. Hence, the estimation for the items is more accurate, compared to
the subjects, as there are more data to estimate each.&lt;/p>
&lt;p>The effect of manipulating the number of subjects and items is revealed
in the plot below. Here, I refit the model with data that have the
subject and the item numbers flipped. The subject abilities are now
estimated more accurately than the item difficulties. You can experiment
with this to see how the estimation accuracy changes with different
combinations of subject/item numbers. The functions &lt;code>sim_data()&lt;/code> and
&lt;code>plot_estimate()&lt;/code> in &lt;a href="estimate-acc.R">&lt;code>estimate-acc.R&lt;/code>&lt;/a> can help you
with this.&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-12-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="coding-models-the-easy-route">Coding models: the easy route&lt;/h3>
&lt;p>We have gone through a long route, along which we have learned a lot.
Now, you are qualified to take the easy route: to use R’s handy function
for dummy coding. Note that this route won’t be easy at all if you never
went through the longer one. Rather, confusion is all you will get, and
you will have no confidence in the model you coded. Simple code is
simple only for those who are well-trained. So now, let’s fit the model
again. This time, we take the highway.&lt;/p>
&lt;p>The trick for controlling how the model functions dummy code the
categorical variables is to use the &lt;code>contrasts()&lt;/code> function to set up the
preferred coding scheme. In the code below, I pass the number of the
categories in $S$ (i.e., &lt;code>n_subj&lt;/code>) to &lt;code>contr.sum()&lt;/code>, which is a helper
function that codes the subjects in the exact same way as we did in
Table 3.3 (execute &lt;code>contr.sum(3)&lt;/code> and you will see a table that
corresponds exactly to Table 3.3).&lt;/p>
&lt;p>After the coding scheme is set, we can express categorical predictors in
the model formula directly. Everything else is the same except the last
line. Previously, I demonstrated dummy coding by dropping the first
subject in $S$. Here, &lt;code>contr.sum()&lt;/code> drops the last subject by default.
Thus, the code for constructing the dropped subject is slightly
different here.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># Drop the last S and impose sum-to-zero constraint on S&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>S ) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> I &lt;span style="color:#f92672">+&lt;/span> S, data&lt;span style="color:#f92672">=&lt;/span>dat, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>item_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( subj_eff.m1.2, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff.m1.2) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we have the estimated effects from the model, let’s check the
results. The figure below plots the current estimates (&lt;code>m1.2&lt;/code>) against
previous ones (&lt;code>m1.1&lt;/code>). The estimates from the two models agree, which
confirms that the second model is correctly coded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>est_m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff, subj_eff )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>est_m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff.m1.2, subj_eff.m1.2 )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.2&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.1&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_m1.2, est_m1.1, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-14-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>This post is lengthy, but not because it is hard. Rather, the concepts
presented are fairly simple. The post is lengthy because we got used to
texts that hide details from readers. The text here does the opposite:
it presents all the &lt;strong>necessary details&lt;/strong> to get the statistical model
working, without confusion. People often assume that hidden details are
trivial. But more often, it is just because writers are too lazy to
present the details. Statistics is hard partly because it is loaded with
details that are hidden and ignored. When details get ignored long
enough, they accumulate to become entangled and uncrackable. Coding,
again, is here to help. It dissolves the fuzziness that otherwise
accumulates and hinders understanding.&lt;/p>
&lt;p>In &lt;a href="https://yongfu.name/irt3">Part 3&lt;/a>, we move on to &lt;strong>Generalized Linear Mixed Models&lt;/strong>,
which are extensions to GLMs that improve estimation and efficiency by
harnessing the information from common group memberships in the data. We
will use the same data, and the text would be much shorter, I hope.
Seeya!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I use &lt;em>dummy coding&lt;/em> as a general umbrella term to cover all
systems for coding categorical variables. In R, what is known as
“treatment coding” (&lt;code>contr.treatment&lt;/code>) is sometimes called “dummy
coding” by others. Here, I follow R’s convention. When “dummy
coding” is used, I always refer to the general sense of re-coding
categories as numbers (not necessarily zeros and ones).&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This default behavior of estimating a global intercept makes sense
in the context of continuous predictors, such as the simple linear
model shown below. In this case, we can succinctly express the
formula as &lt;code>y ~ x&lt;/code> in R’s model syntax. The estimate of the global
intercept $\alpha$ would be given as the intercept coefficient in
the model output.&lt;/p>
&lt;p>$$
\begin{aligned}
y_i &amp;amp;\sim Normal(\mu_i, \sigma) \\
\mu_i &amp;amp;= \alpha + \beta x_i
\end{aligned}
$$&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>I haven’t mentioned the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial
distribution&lt;/a>
before. The binomial distribution is the extension of the Bernoulli
distribution to $n$ independent trials. So if you repeat the
Bernoulli process $n$ times and sum the outcomes, say, you toss the
coin $n=10$ times and record the number of heads observed, the
distribution of outcomes would follow a binomial distribution with
parameters $n$ and $p$. So the Bernoulli distribution is simply a
special case of the binomial distribution where $n=1$.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category><category>reproducibility</category></item><item><title>Demystifying Item Response Theory (1/4)</title><link>https://yongfu.name/2023/02/25/irt1/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/25/irt1/</guid><description>&lt;figure>
&lt;img src="irt.jpg" style="width:70.0%"
alt="Ok, so these are the item characteristic curves. What then?" />
&lt;figcaption aria-hidden="true">&lt;em>Ok, so these are the &lt;a
href="https://www.researchgate.net/figure/Item-characteristic-curves-Item-Response-Theory-IRT-1PL-model_fig1_342560715">item
characteristic curves&lt;/a>. What then?&lt;/em>&lt;/figcaption>
&lt;/figure>
&lt;h2 id="mysterious-item-response-theory">Mysterious Item Response Theory&lt;/h2>
&lt;p>&lt;strong>Item response theory is &lt;em>mysterious&lt;/em> and intimidating to students.&lt;/strong>
It is mysterious in the way it is presented in textbooks, at least in
introductory ones. The text often starts with an ambitious conceptual
introduction to IRT, which most students would be able to follow, but
with some confusion. Curious students might bear with the confusion and
expect it to resolve in the following text, only to find themselves
disappointed. At the point where the underlying statistical model should
be further elaborated, the text abruptly stops and tries to convince
readers to trust the results from black-box IRT software packages.&lt;/p>
&lt;p>It isn’t that I have trust issues with black-box software, and I also
agree that it is impractical to understand all the details of model
estimation in IRT. Doing so would be similar to coding an IRT program
from scratch. The issue is that there is a huge gap here, between where
textbooks stopped explaining and where the confusing details of
statistical models should be hidden. Hence, students would be tricked
into believing that they have a &lt;em>sufficient degree of understanding&lt;/em>,
but in reality, it is just blind faith.&lt;/p>
&lt;p>A sufficient degree of understanding should allow the student to deploy
the learned skill to new situations. Therefore, a sufficient degree of
understanding of IRT models, for instance, should allow students to
extend and apply the models to analyses of &lt;a href="https://en.wikipedia.org/wiki/Differential_item_functioning">differential item
functioning
(DIF)&lt;/a> and
&lt;strong>differential rater functioning (DRF)&lt;/strong>.&lt;/p>
&lt;p>I’m arguing here that there is a basic granularity of understanding,
somewhat similar to the concept of &lt;a href="https://en.wikipedia.org/wiki/Basic_category">basic-level
category&lt;/a>, that when
reached, allows a student to smoothly adapt the learned skill to a wide
variety of situations, modifying and extending the skill on demand. And
I believe that item response theory is &lt;em>&lt;strong>too hard&lt;/strong>&lt;/em> for a student to
learn and reach this basic level of understanding, due to its
conventional presentation and historical development&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There is still hope however, thanks to the development of a very general
set of statistical models known as the &lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Models
(GLM)&lt;/a>. Item
response models could be understood in terms of the GLM and its
extensions
(&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_mixed_model">GLMM&lt;/a>
and non-linear form of GLM/GLMM). To be too particular about the
details, the results from IRT software packages and GLMs/GLMMs would be
very close but not identical, since they utilize different estimation
methods. The strengths of GLM, however, lie in its conceptual simplicity
and extensibility. Through GLM, IRT and other models such as the T-test,
ANOVA, and linear regression, are all placed together into the same
conceptual framework. Furthermore, software packages implementing GLMs
are widely available. Users can thus experiment with them—simulate a set
of data based on known parameters, construct the model and feed it the
data, and see if the fitted model correctly recovers the parameters.
This technique of learning statistics is probably the only effective way
for students to understand &lt;em>&lt;strong>a mysterious statistical model&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>In this series of posts, I will walk the readers through the path of
understanding item response theory, with help from simulations and
generalized linear models. No need to worry if you don’t know GLMs yet.
We have another ally—&lt;a href="https://www.r-project.org">R&lt;/a>, in which we will be
simulating artificial data and fitting statistical models along the way.
Although it might seem intimidating at first, coding simulations and
models in fact provides scaffolding for learning. When feeling unsure or
confused, you can always resort to these simulation-based experiments to
resolve the issues at hand. In this very first post, I will start by
teaching you &lt;em>&lt;strong>simulations&lt;/strong>&lt;/em>.&lt;/p>
&lt;h2 id="just-enough-theory-to-get-started">Just Enough Theory to Get Started&lt;/h2>
&lt;p>Jargons aside, the concept behind item response theory is fairly simple.
Consider the case where 80 testees are taking a 20-item English
proficiency test. Under this situation, what are the &lt;em>&lt;strong>factors&lt;/strong>&lt;/em> that
influence whether an item is correctly answered by a testee?
Straightforward right? If an item is easy and if a testee is proficient
in English, he/she would probably get the item correct. Here, &lt;em>&lt;strong>two
factors jointly influence the result&lt;/strong>&lt;/em>:&lt;/p>
&lt;ol>
&lt;li>how difficult (or easy) an item is&lt;/li>
&lt;li>the English ability of a testee&lt;/li>
&lt;/ol>
&lt;p>We can express these variables and the relations between them in the
graphs below. Let’s focus on the left one first. Here, $A$ represents
the &lt;strong>ability&lt;/strong> of the testee, $D$ represents the &lt;strong>difficulty&lt;/strong> of the
item, and $R$ represents the &lt;strong>item response&lt;/strong>, or &lt;strong>score&lt;/strong> on the
item. A response for an item is coded as &lt;code>1&lt;/code> ($R=1$) if it is correctly
answered. Otherwise, it is coded as &lt;code>0&lt;/code> ($R=0$). The arrows $A$ → $R$
and $D$ → $R$ indicate the direction of influence. The arrows enter $R$
since item difficulty and testee ability influence the score on the item
(not the other way around). $A$ and $D$ are drawn as a circled node to
indicate that they are &lt;strong>unobserved&lt;/strong> (or &lt;strong>latent&lt;/strong>, if you prefer a
fancier term), whereas uncircled nodes represent directly observable
variables (i.e., stuff that gets recorded during data collection). This
graphical representation of the variable and their relationships is
known as a &lt;a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed Acyclic Graph
(DAG)&lt;/a>.&lt;/p>
&lt;div class="goat svg-container ">
&lt;svg
xmlns="http://www.w3.org/2000/svg"
font-family="Menlo,Lucida Console,monospace"
viewBox="0 0 352 137"
>
&lt;g transform='translate(8,16)'>
&lt;path d='M 272,16 L 304,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 152,16 L 152,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 24,80 L 40,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 216,80 L 232,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 72,48 L 88,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 264,48 L 280,80' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='52.000000,48.000000 40.000000,42.400002 40.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 40.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='84.000000,48.000000 72.000000,42.400002 72.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 72.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 232,48 L 240,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='250.000000,48.000000 238.000000,42.400002 238.000000,53.599998' fill='currentColor' transform='rotate(300.000000, 232.000000, 48.000000)'>&lt;/polygon>
&lt;path d='M 256,32 L 264,48' fill='none' stroke='currentColor'>&lt;/path>
&lt;polygon points='282.000000,48.000000 270.000000,42.400002 270.000000,53.599998' fill='currentColor' transform='rotate(240.000000, 264.000000, 48.000000)'>&lt;/polygon>
&lt;polygon points='312.000000,16.000000 300.000000,10.400000 300.000000,21.600000' fill='currentColor' transform='rotate(0.000000, 304.000000, 16.000000)'>&lt;/polygon>
&lt;path d='M 248,0 A 16,16 0 0,0 232,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 248,0 A 16,16 0 0,1 264,16' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 232,16 A 16,16 0 0,0 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 264,16 A 16,16 0 0,1 248,32' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 16,80 A 16,16 0 0,0 0,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 16,80 A 16,16 0 0,1 32,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 96,80 A 16,16 0 0,0 80,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 96,80 A 16,16 0 0,1 112,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 208,80 A 16,16 0 0,0 192,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 208,80 A 16,16 0 0,1 224,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 288,80 A 16,16 0 0,0 272,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 288,80 A 16,16 0 0,1 304,96' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 0,96 A 16,16 0 0,0 16,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 32,96 A 16,16 0 0,1 16,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 80,96 A 16,16 0 0,0 96,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 112,96 A 16,16 0 0,1 96,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 192,96 A 16,16 0 0,0 208,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 224,96 A 16,16 0 0,1 208,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 272,96 A 16,16 0 0,0 288,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;path d='M 304,96 A 16,16 0 0,1 288,112' fill='none' stroke='currentColor'>&lt;/path>
&lt;text text-anchor='middle' x='16' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='56' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;text text-anchor='middle' x='96' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='208' y='100' fill='currentColor' style='font-size:1em'>A&lt;/text>
&lt;text text-anchor='middle' x='248' y='20' fill='currentColor' style='font-size:1em'>P&lt;/text>
&lt;text text-anchor='middle' x='288' y='100' fill='currentColor' style='font-size:1em'>D&lt;/text>
&lt;text text-anchor='middle' x='328' y='20' fill='currentColor' style='font-size:1em'>R&lt;/text>
&lt;/g>
&lt;/svg>
&lt;/div>
&lt;p>The DAGs laid out here represent the concept behind the simplest kinds
of item response models, known as the &lt;strong>1-parameter logistic (1PL)
model&lt;/strong> (or Rasch Model). In more formal terms, this model posits that
the &lt;strong>probability&lt;/strong> of correctly answering an item is determined by the
&lt;strong>difference&lt;/strong> between testee ability and item difficulty. So a more
precise DAG representation for this model would be the one shown on the
right above. Here, $P$ is the probability of correctly answering the
item, which cannot be directly observed. However, $P$ directly
influences the item score $R$, hence the arrow $P$ → $R$.&lt;/p>
&lt;p>Believe it or not, the things we have learned so far could get us
started. So let’s simulate some data, based on what we know about item
response theory!&lt;/p>
&lt;h2 id="simulating-item-responses">Simulating Item Responses&lt;/h2>
&lt;figure>
&lt;img src="tenet2.gif" style="width:85.0%"
alt="Simulation is playing god in a small world. Similar to model fitting, but in reverse direction." />
&lt;figcaption aria-hidden="true">Simulation is &lt;em>playing god&lt;/em> in a
small world. Similar to model fitting, but in &lt;em>reverse&lt;/em>
direction.&lt;/figcaption>
&lt;/figure>
&lt;p>Consider the scenario where 3 students (Rob, Tom, and Joe) took a math
test with 2 items (A and B). Since we play gods during simulations, we
know the math ability of the students and the difficulty of the items.
These ability/difficulty levels can range from positive to negative
numbers, unbounded. Larger numbers indicate higher levels of
difficulty/ability. In addition, the levels of difficulty and ability
sit on a common scale, and hence could be directly compared. Also, each
student answers every item, so we get responses from all 6 (3x2)
combinations of students and items. Let’s code this in R. The function
&lt;code>expand.grid()&lt;/code> would pair up the 6 combinations for us.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( A&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.4&lt;/span>, B&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> ) &lt;span style="color:#75715e"># Difficulty of item&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( R&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, T&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>, J&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">-0.4&lt;/span> ) &lt;span style="color:#75715e"># Ability of student (R:Rob, T:Tom, J:Joe)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B&amp;#34;&lt;/span> ), &lt;span style="color:#75715e"># Item index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span> T &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;R&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;T&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;J&amp;#34;&lt;/span> ) &lt;span style="color:#75715e"># Testee index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 2
I T
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;
1 A R
2 B R
3 A T
4 B T
5 A J
6 B J
&lt;/code>&lt;/pre>
&lt;p>After having all possible combinations of the students and the items, we
could collect the values of student ability and item difficulty into the
data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">=&lt;/span> A[ dat&lt;span style="color:#f92672">$&lt;/span>T ] &lt;span style="color:#75715e"># map ability to df by testee index T&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>D &lt;span style="color:#f92672">=&lt;/span> D[ dat&lt;span style="color:#f92672">$&lt;/span>I ] &lt;span style="color:#75715e"># map difficulty to df by item index I&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 4
I T A D
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4
2 B R 0.5 0.1
3 A T 0.1 0.4
4 B T 0.1 0.1
5 A J -0.4 0.4
6 B J -0.4 0.1
&lt;/code>&lt;/pre>
&lt;p>Now, we’ve got all the data needed for simulation, the only thing left
is to precisely lay out the &lt;strong>rules for generating the response data
$R$&lt;/strong>—the scores (zeros and ones) on the items answered by the students.
We are two steps away.&lt;/p>
&lt;h3 id="generating-probabilities">Generating Probabilities&lt;/h3>
&lt;p>When IRT is introduced in the previous section, I mention that the
probability of success on an item is determined by the &lt;strong>difference
between testee ability and item difficulty&lt;/strong>. It is straightforward to
get this difference: simply subtract $D$ from $A$ in the data. This
would give us a new variable $\mu$. I save the values of $\mu$ to column
&lt;code>Mu&lt;/code> in the data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>Mu &lt;span style="color:#f92672">=&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">-&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>D
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 5
I T A D Mu
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4 0.1
2 B R 0.5 0.1 0.4
3 A T 0.1 0.4 -0.3
4 B T 0.1 0.1 0
5 A J -0.4 0.4 -0.8
6 B J -0.4 0.1 -0.5
&lt;/code>&lt;/pre>
&lt;p>From the way $\mu$ is calculated ($A$ - $D$), we can infer that, for a
particular observation, if $\mu$ is positive and large, the testee’s
ability will be much greater than the item’s difficulty, and he would
probably succeed on this item. On the other hand, if $\mu$ is negative
and small, the item difficulty would be much greater in this case, and
the testee would likely fail on this item. Hence, $\mu$ is directly
related to probability, in that $\mu$ of large values result in high
probabilities of success on the items, whereas $\mu$ of small values
result in low probabilities of success. But how exactly is $\mu$ linked
to probability? How could we map $\mu$ to probability in a principled
manner? The solution is to take advantage of the &lt;a href="https://en.wikipedia.org/wiki/Logistic_function">logistic
function&lt;/a>.&lt;/p>
&lt;p>$$
logistic( x ) = \frac{ 1 }{ 1 + e^{-x} }
$$&lt;/p>
&lt;p>The &lt;em>&lt;strong>logistic&lt;/strong>&lt;/em> is a function that maps a real number $x$ to a
probability $p$. In other words, the logistic function transforms the
input $x$ and constrains it to a value between zero and one. Note that
the transformation is &lt;strong>monotonic increasing&lt;/strong>, meaning that a smaller
$x$ would be mapped onto a smaller $p$, and a larger $x$ would be mapped
onto a larger $p$. The ranks of the values before and after the
transformation stay the same. To have a feel of what the logistic
function does, let’s transform some values with the logistic.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set plot margins # (b, l, t, r)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># Outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> ( &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-5&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( x )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( x, p )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-4-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>As the plot shows, the logistic transformation results in an S-shaped
curve. Since the transformed values (p) are bounded by 0 and 1, extreme
values on the poles of the x-axis would be “squeezed” after the
transformation. Real numbers with absolute values greater than 4, after
transformations, would have probabilities very close to the boundaries.&lt;/p>
&lt;h4 id="less-math-less-confusion">Less Math, Less Confusion&lt;/h4>
&lt;p>For many students, the mathematical form of the logistic function leads
to confusion. Staring at the math symbols hardly enables one to arrive
at any insightful interpretation of the logistic. A suggestion here is
to let go of the search for such an interpretation. The logistic
function is introduced not because it is loaded with some crucial
mathematical or statistical meaning. Instead, it is used here solely for
a practical reason: to monotonically map real numbers to probabilities.
You may well use another function here to achieve the same purpose
(e.g., the &lt;strong>cumulative distribution function&lt;/strong> for the normal
distribution).&lt;/p>
&lt;h3 id="generating-responses">Generating Responses&lt;/h3>
&lt;p>We have gone all the way from ability/difficulty levels to the
probabilities of success on the items. Since we cannot directly observe
probabilities in the real world, the final step is to link these
probabilities to observable outcomes. In the case here, the outcomes are
simply item responses of zeros and ones. How do we map probabilities to
zeros and ones? Coin flips, or &lt;a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distributions&lt;/a>,
will get us there.&lt;/p>
&lt;p>Every time a coin is flipped, either a tail or a head is observed. The
Bernoulli distribution is just a fancy way of describing this process.
Assume that we record tails as &lt;code>0&lt;/code>s and heads as &lt;code>1&lt;/code>s, and suppose that
the probability $p$ of observing a head equals 0.75 (since the coin is
imbalanced in some way that the head is more likely observed and we know
it somehow). Then the distribution of the outcomes (zero and one) is a
Bernoulli distribution with parameter $P=0.75$. In graphical terms, the
distribution is just two bars.&lt;/p>
&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-5-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>We’ve got all we need by now. Let’s construct the remaining columns to
complete this simulation. In the code below, I compute the probabilities
(&lt;code>P&lt;/code>) from column &lt;code>Mu&lt;/code>. Column &lt;code>P&lt;/code> could then generate column &lt;code>R&lt;/code>, the
item responses, through the Bernoulli distribution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">13&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>Mu )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>P ) &lt;span style="color:#75715e"># Generate 0/1 from Bernoulli distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 × 7
I T A D Mu P R
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 A R 0.5 0.4 0.1 0.525 0
2 B R 0.5 0.1 0.4 0.599 1
3 A T 0.1 0.4 -0.3 0.426 0
4 B T 0.1 0.1 0 0.5 0
5 A J -0.4 0.4 -0.8 0.310 1
6 B J -0.4 0.1 -0.5 0.378 0
&lt;/code>&lt;/pre>
&lt;p>Now, we have a complete table of simulated item responses. A few things
to notice here. First, look at the fourth row of the data frame, where
the response of testee T (Tom) on item B is recorded. Column &lt;code>Mu&lt;/code> has a
value of zero since Tom’s ability level is identical to the difficulty
of item B. What does it mean to be “identical”? “Identical” implies that
Tom is neither more likely to succeed nor to fail on item B. Hence, you
can see that Tom has a 50% of getting item B correct in the $P$ column.
This is how the ability/difficulty levels and $\mu$ are interpreted.
They are on an abstract scale of real numbers. We need to convert them
to probabilities to make sense of them.&lt;/p>
&lt;p>The second thing to notice is column &lt;code>R&lt;/code>. This is the only column that
has &lt;em>&lt;strong>randomness&lt;/strong>&lt;/em> introduced. Every run of the simulation would
likely give different values of $R$ (unless a random seed is set, or the
&lt;code>P&lt;/code> column consists solely of zeros and ones). The outcomes are not
guaranteed, probability is at work.&lt;/p>
&lt;p>The presence of such randomness is the gist of simulations and
statistical models. We add uncertainty to the simulation, mimicking the
real world, to know that in the presence of such uncertainty, would it
still be possible to discover targets of interest with a statistical
model. Randomness, however, poses some challenges for coding. We need to
equip ourselves for those challenges.&lt;/p>
&lt;h2 id="coding-randomness">Coding Randomness&lt;/h2>
&lt;p>Randomness is inherent in simulations and statistical models, so it is
impossible to run away from it. It is everywhere. The problem with
randomness is that it introduces uncertainty in the outcome produced.
Thus, it would be hard to spot any errors just by &lt;strong>eyeballing the
results&lt;/strong>.&lt;/p>
&lt;p>Take &lt;code>rbern()&lt;/code> for instance. Given a parameter $P=0.5$, we can
repeatedly run &lt;code>rbern(0.5)&lt;/code> a couple of times to produce zeros and ones.
But these zeros and ones cannot tell us whether &lt;code>rbern(0.5)&lt;/code> is working
properly. &lt;code>rbern(0.5)&lt;/code> might be broken somehow, and instead generates
the ones with, say, $P=0.53$.&lt;/p>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large
numbers&lt;/a> can help us
here. Since &lt;code>rbern()&lt;/code> generates ones with probability $P$, if we run
&lt;code>rbern()&lt;/code> many times, the proportion of the ones in the outcomes should
converge to $P$. To achieve this, take a look at the second argument &lt;code>n&lt;/code>
in &lt;code>rbern()&lt;/code>, which is set here to repeatedly generate outcomes ten
thousand times. You can increase &lt;code>n&lt;/code> to see if the result comes even
closer to $0.5$.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> ) &lt;span style="color:#75715e"># Run rbern with P=0.5 10,000 times&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.4991
&lt;/code>&lt;/pre>
&lt;p>A more general way to rerun a chunk of code is through the for loop or
convenient wrappers such as the &lt;code>replicate()&lt;/code> function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{ &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span> ) } )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.5027
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># See if several runs of rbern( p=0.5, n=1e4 ) give results around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#75715e"># Plot Ps to see if they scatter around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">100&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0.47&lt;/span>, &lt;span style="color:#ae81ff">0.53&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;P&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( Ps, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-9-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="randomness-in-models">Randomness in Models&lt;/h3>
&lt;p>The method described above works only for simple cases. What about
complex statistical models? How do we test that they are working as they
claim to? Guess what? &lt;em>&lt;strong>Simulation&lt;/strong>&lt;/em> is the key.&lt;/p>
&lt;p>We simulate data based on the assumptions of the statistical model and
see if it indeed returns what it claims to estimate. The simulation can
be repeated several times, each set to different values of parameters.
If the parameters are always recovered by the statistical model, we can
then be confident that the model is properly constructed and correctly
coded. So &lt;strong>simulation is really &lt;em>not an option&lt;/em> when doing
statistics&lt;/strong>. It is the only safety that helps us guard against bugs in
our statistical models, both programmatical and theoretical ones.
Without first testing the statistical model on simulated data, any
inferences about the empirical data are in doubt.&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>In a real-world scenario of the example presented here, we would only
observe the score ($R$) of an item ($I$) taken by a testee ($T$). The
targets of interest are the unobserved item difficulty ($D$) and testee
ability ($A$). In &lt;a href="https://yongfu.name/irt2">Part 2&lt;/a>, we will work in reverse and fit
statistical models on simulated data. We will see how the models
discover the true $A$s and $D$s from the information of $R$, $I$, and
$T$. See you there!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I mean, what the hack is &lt;em>Joint/Conditional Maximum Likelihood
Estimation&lt;/em>? These are methods developed in psychometrics and are
hardly seen in other fields. Unless obsessed with psychometrics, I
don’t think one would be able to understand these things.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>statistics</category><category>psychology</category><category>reproducibility</category></item><item><title>A Minimalist Structure for Snakemake</title><link>https://yongfu.name/2023/02/15/snakemake/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/15/snakemake/</guid><description>&lt;p>I have &lt;a href="https://kbroman.org/minimal_make">heard of&lt;/a> the use of &lt;a href="https://www.gnu.org/software/make/">GNU Make&lt;/a> for enhancing
reproducibility for some time. I did not incorporate Make into my work however,
since a simple build script written in Bash was sufficient. Everything was well
in control, and I could structure the workflow to my will.&lt;/p>
&lt;p>It was not until I started working in a company setting that I found most things
out of my control. Decades of conventions have been accumulating and passing on,
and personal workflows have to fit into existing ones. In order to fit into my
company&amp;rsquo;s conventions of data analysis (which pretty much just ignore analysis
reproducibility), the number of scripts grew exponentially and quickly fell out
of my control (see figure below, which is automatically generated by Snakemake).
I needed a way to document and track my workflow in a consistent and scalable
manner. This was when I picked up Prof. Broman&amp;rsquo;s &lt;a href="https://kbroman.org/minimal_make">great introductory post&lt;/a>
on GNU Make again. Everything seemed hopeful, but I was soon defeated by the
omnipresent Windows. Since it is required to work onSome Notes for as
Windows machines in my
company, and since &lt;a href="http://gnuwin32.sourceforge.net/packages/make.htm">Make for Windows&lt;/a> has difficulties dealing with
Chinese file paths, I had to give up on Make. &lt;a href="https://snakemake.github.io">Snakemake&lt;/a> then came as my
savior.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://img.yongfu.name/posts/dag.png" alt="Data analysis workflow graph generated by Snakemake">
&lt;figcaption>Data analysis workflow graph generated by Snakemake&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="meeting-snakemake">Meeting Snakemake&lt;/h2>
&lt;p>Snakemake was inspired by, but way more complicated than, GNU Make. Since it is
backed by Python, cross-platform issues such as character encodings are
automatically resolved. Snakemake is a thoughtful project that was originally
developed to facilitate computational research and reproducibility. Thus, it may
take some time to get started since there are many concepts to pick up. It&amp;rsquo;s
totally worth it, however. Dealing with complex tasks requires a complicated
framework. Often, these complications make sense (and are appreciated) only
after we face real-world complex tasks. Going through &lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html">Snakemake&amp;rsquo;s
tutorial&lt;/a> and experimenting with it on the computer would be sufficient
to get an average user started. It is not as complicated as it seems at first
glance.&lt;/p>
&lt;h2 id="snakemake-recommended-workflow">Snakemake Recommended Workflow&lt;/h2>
&lt;p>A great thing about Snakemake is that it is &lt;a href="https://stackoverflow.com/a/82064">opinionated&lt;/a>. This means
that certain conventions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> are proposed, and most users would benefit
from these conventions since they spare the burden of structuring the workflow.&lt;/p>
&lt;p>For instance, Snakemake &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html#distribution-and-reproducibility">recommends&lt;/a> the directory structure listed
below for every Snakemake workflow. This structure is so simple that its genius
might not be obvious at first glance. There are four directories in the project
root&amp;mdash;&lt;code>workflow/&lt;/code>, &lt;code>config/&lt;/code>, &lt;code>results/&lt;/code>, and &lt;code>resources/&lt;/code>. &lt;code>workflow/&lt;/code> holds
the &lt;em>coding&lt;/em> stuff. Code for data analysis, computation, and reproducibility are
all found in this directory. &lt;code>config/&lt;/code> is for optional configuration and I would
skip it here (in my own project, I did not use config files since the
&lt;code>Snakefile&lt;/code> is sufficient for my purposes). &lt;code>results/&lt;/code> and &lt;code>resources/&lt;/code> are what
(I think) make this structure fantastic. &lt;code>resources/&lt;/code> holds all &lt;strong>raw data&lt;/strong>,
i.e., data that are not reproducible on your computer (e.g., manually annotated
data). All data resulting from the computation in the current project are
located in &lt;code>results/&lt;/code>. So ideally, you could delete &lt;code>results/&lt;/code> at any time
without worry. A single command &lt;code>snakmake -c&lt;/code> should generate all the results
from &lt;code>resources/&lt;/code>. The genius of this structure is that it eliminates the need
of worrying about where to place newly arrived data, as commonly encountered in
real-world situations (e.g., an analysis might require data that you did not
foresee).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>├── .gitignore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>├── README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>├── workflow
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>│ ├── rules
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>| │ ├── module1.smk
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>| │ └── module2.smk
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>│ ├── scripts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>| │ ├── script1.py
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>| │ └── script2.R
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>| └── Snakefile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>├── config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>│ └── config.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>├── results
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>└── resources
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="an-enhanced-snakemake-workflow">An Enhanced Snakemake Workflow&lt;/h2>
&lt;p>I adopted the workflow above in my work. It was great, but I still found &lt;strong>two
annoying drawbacks&lt;/strong>.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Long directory names&lt;/strong>&lt;/p>
&lt;p>Since in a &lt;code>Snakefile&lt;/code>, file paths of inputs and outputs are always
repeated, it soon becomes annoying to type in paths starting with
&lt;code>resources/...&lt;/code> and &lt;code>results/...&lt;/code>. In addition, &amp;ldquo;resources&amp;rdquo; and &amp;ldquo;results&amp;rdquo;
have a common prefix, which often confuses me. It would be better off if the
two terms are more readily distinguished visually.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Confusing relative paths&lt;/strong>&lt;/p>
&lt;p>According to the &lt;a href="https://snakemake.readthedocs.io/en/latest/project_info/faq.html#how-does-snakemake-interpret-relative-paths">documentation&lt;/a>, relative paths in different
directives are &lt;strong>interpreted differently&lt;/strong>. To be short, relative paths in
&lt;code>input:&lt;/code>, &lt;code>output:&lt;/code>, and &lt;code>shell:&lt;/code> are interpreted relative to the working
directory (i.e., where you invoke the command &lt;code>snakemake -c&lt;/code>), whereas in
directives such as &lt;code>script:&lt;/code>, they are interpreted as relative to the
&lt;code>Snakefile&lt;/code>. So it would be cognitively demanding to switch back and forth
between the reference points of relative paths while writing the
&lt;code>Snakefile&lt;/code>. Why not have all paths relative to the project root?&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>To deal with the aforementioned problems, I modified the recommended directory
structure and arrived at the structure below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>├── README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>├── Snakefile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>├── made
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>├── raw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>└── src
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>
&lt;p>&lt;strong>Simplified directory names&lt;/strong>&lt;/p>
&lt;p>&lt;code>resources/&lt;/code> is renamed as &lt;code>raw/&lt;/code>, and &lt;code>results/&lt;/code> is renamed as &lt;code>made/&lt;/code>. The
&lt;code>workflow/&lt;/code> directory is broken down into &lt;code>src/&lt;/code> (holding scripts) and the
&lt;code>Snakefile&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Consistent relative paths&lt;/strong>&lt;/p>
&lt;p>Since &lt;code>Snakefile&lt;/code> is now placed in the project root, the problem of
different relative paths for different directives is resolved, as long as
the user always invokes the command &lt;code>snakemake -c&lt;/code> in the project root.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The source code of this Snakemake workflow can be found
&lt;a href="https://github.com/liao961120/minimal-snake">here on GitHub&lt;/a>.&lt;/p>
&lt;h2 id="some-notes-for-using-git-bash-as-shell">Some Notes for Using Git-Bash as Shell&lt;/h2>
&lt;p>The experience of using Snakemake on Windows is great overall. I have run into a
few problems, but the problems were usually solvable. There is one particular
problem that took me a while to solve. On Windows, the default shell executable
used in Snakemake (and Python) is Cmd (or maybe Powershell). But since I am more
familiar with Bash and Unix tools, it is a real inconvenience. I had setup
Git-Bash on the company&amp;rsquo;s Windows machine but then spent a long time figuring
out how to set Git-Bash as the default shell in Snakemake. The information for
Snakemake users on Windows is scarce. I guess Snakemake is just unpopular among
Windows users. After reading the &lt;a href="https://snakemake.readthedocs.io/en/v6.15.2/_modules/snakemake/shell.html">source code&lt;/a>, the solution turned
out to be quite simple. Just put the code below at the top of the &lt;code>Snakefile&lt;/code>
and place the path to Git-Bash executable in &lt;code>shell.executable()&lt;/code>. This will
allow the identical &lt;code>Snakefile&lt;/code> to be used on both Windows and Unix-like
computers without additional configurations.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Additional setup for running with git-bash on Windows&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>name &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;nt&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#f92672">from&lt;/span> snakemake.shell &lt;span style="color:#f92672">import&lt;/span> shell
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> shell&lt;span style="color:#f92672">.&lt;/span>executable(&lt;span style="color:#e6db74">r&lt;/span>&lt;span style="color:#e6db74">&amp;#39;C:\Users\rd\AppData\Local\Programs\Git\bin\bash.exe&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&amp;ldquo;Good&amp;rdquo; conventions here, as opposed to naturally-resulting conventions without the consideration of reproducibility.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>python</category><category>reproducibility</category></item></channel></rss>