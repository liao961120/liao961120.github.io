<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>stats on Yongfu's Blog</title><link>https://yongfu.name/tags/stats/</link><description>Recent content in stats on Yongfu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 27 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/stats/feed.xml" rel="self" type="application/rss+xml"/><item><title>Beyond Item Response Theory</title><link>https://yongfu.name/2023/06/27/irt5/</link><pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/06/27/irt5/</guid><description>&lt;p>My previous four posts have focused on basic item response models&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.
These models are commonly found in an educational testing context but
are less often seen in clinical research settings where questionnaires
are used for the measuring and assessment of individuals’ conditions.
Due to the higher level of complexity in the study design of clinical
studies, IRT models are often discarded for the analyses of
questionnaires. Simple sum scores of the questionnaires, instead of the
more robust estimates derived from IRT models, are taken directly to
represent the quantity of the latent constructs. This has undesirable
effects of introducing bias and overconfidence and ignoring uncertainty
in measuring these latent constructs.&lt;/p>
&lt;p>There is no need to trade measurement inefficiency for study design
complexity. Complex models arising from complicated study designs can be
naturally extended to incorporate IRT models that deal with latent
construct measurement, thus enhancing the quality of the study. In this
post, we will walk through such an example by building up a model for
analyzing individuals’ changes—in terms of latent variables—over time. A
Bayesian framework will be adopted, and we will be working with
&lt;a href="https://mc-stan.org">Stan&lt;/a> code directly. As always, we begin with
simulations. But since the model we are building is quite complicated—as
any model trying to mirror reality is—I’ll first provide some
context.&lt;/p>
&lt;h2 id="treatment-of-alcohol-dependence">Treatment of Alcohol Dependence&lt;/h2>
&lt;p>Suppose some researchers were carrying out a study to examine the
effectiveness of treatments on alcohol dependence. Individuals were
recruited and randomly assigned to one of the three treatment
conditions. All treatments lasted for three months, during which the
participants were assessed for the effectiveness of the treatments to
track their clinical progress. The assessments included collecting data
such as measures of treatment outcomes (e.g., days of heavy drinking in
the past 14 days) and mediating factors on treatment effectiveness
suggested by previous studies (e.g., self-efficacy). It was hypothesized
that treatments for alcohol dependence work partially through raising
individuals’ self-efficacy in controlling alcohol use. Hence, during the
assessments, questionnaires on self-efficacy in alcohol control were
also administered to measure this potential mediating factor.&lt;/p>
&lt;figure>
&lt;img src="dag.svg" style="max-height:210px"
alt="Causal assumptions of the alcohol dependence treatment study" />
&lt;figcaption aria-hidden="true">Causal assumptions of the alcohol
dependence treatment study&lt;/figcaption>
&lt;/figure>
&lt;p>The DAG above explicates the causal assumptions of this fictitious
study&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Here’s the description of the variables in the DAG.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$E$: Participants’ self-efficacy on alcohol use control&lt;/p>
&lt;p>Since self-efficacy $E$ is not directly observed, it is represented as
a circled node in the DAG.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$R$: Item responses collected through self-efficacy questionnaires&lt;/p>
&lt;p>To measure the unobserved self-efficacy $E$, tools like questionnaires
are required to measure such a latent construct. $R$ stands for the
responses collected through the questionnaire. These responses would
allow the estimation of the variable $E$ for each participant. Note
that the item parameter $I$ is left out for simplicity. If present, it
would point to $R$ as item estimates also affect the responses $R$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$A$: Participants’ age&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$T$: Treatment condition received by a participant&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$D$: Latent treatment outcome&lt;/p>
&lt;p>$D$ is the latent quantity that underlies the (observed) treatment
outcome.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$D^{\ast}$: Treatment outcome. Here, it is the days of heavy drinking
in the past 14 days.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The arrows among the nodes in the DAG indicate the directions of
influence. Therefore, the graph is basically saying that the treatments
affect the outcomes through two pathways. One direct, and the other
indirect, through self-efficacy. Age also has direct influences on
self-efficacy and the treatment outcome. The labels on the edges mark
the regression coefficients, which are the parameters of interest for
our later simulations and model testing.&lt;/p>
&lt;p>The DAG presented above ignores the time dimension for simplicity. The
second DAG below includes it. To avoid cluttering the graph, only three,
instead of four, time points are shown. The subscripts on the variables
mark the time points. $t=0$ indicates the baseline (i.e., the first)
assessment. A cautionary note here is that age only &lt;em>directly&lt;/em>
influences self-efficacy and the latent treatment outcome at baseline
($A \rightarrow E_0, D_0$). At subsequent time points, they are
influenced by age only &lt;em>indirectly&lt;/em> through $E_0$ and $D_0$. This slight
complication will become clearer in the following description of the
simulation.&lt;/p>
&lt;figure>
&lt;img src="dag-longitudinal.svg" style="max-height:280px"
alt="Causal assumptions of the alcohol dependence treatment study (simplified illustration of three time points)." />
&lt;figcaption aria-hidden="true">Causal assumptions of the alcohol
dependence treatment study (simplified illustration of three time
points).&lt;/figcaption>
&lt;/figure>
&lt;h2 id="simulating-treatments">Simulating Treatments&lt;/h2>
&lt;p>With the background provided, let’s now simulate the above scenario. The
code for the simulation, as well as later model fitting, will be
inevitably long. It’s a natural consequence of realistic stats modeling.
This post is thus less of a pedagogical step-by-step guide and more of a
conceptual demonstration of practical real-world data analyses. That
said, intricate details of code implementation won’t be hidden and are
laid out as is. If anything seems mysterious, the key to demystifying it
is to run some code. Experimentation is an ally that you should always
trust.&lt;/p>
&lt;h3 id="efficacy-and-treatment-outcome">Efficacy and Treatment Outcome&lt;/h3>
&lt;p>Let’s first sketch out the overall scenario by simulating 30
participants for each treatment condition. The age, gender, and the
assigned treatment condition of each participant are saved in the
variable &lt;code>A&lt;/code>, &lt;code>G&lt;/code>, and &lt;code>Tx&lt;/code>, respectively. I also simulated a “baseline”
for each of the participants. These baselines are the quantification of
individual differences and can be more or less thought of as the amount
of efficacy possessed by the individuals before receiving any treatment.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># remotes::install_github(&amp;#34;liao961120/stom&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(stom)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">1977&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>Ntx &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#75715e"># number of treatments&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>Ns &lt;span style="color:#f92672">=&lt;/span> Ntx &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>Nt &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#75715e"># number of time points&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>Tx &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Ntx, each &lt;span style="color:#f92672">=&lt;/span> Ns &lt;span style="color:#f92672">/&lt;/span> Ntx) &lt;span style="color:#75715e"># Treatment condition for each subj&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>G &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, times &lt;span style="color:#f92672">=&lt;/span> Ns &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#75715e"># Gender of each subj&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rtnorm&lt;/span>( Ns, m &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">36&lt;/span>, lower &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">18&lt;/span>, upper &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">80&lt;/span>, s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">20&lt;/span> ) &lt;span style="color:#75715e"># Age&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>tau &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.8&lt;/span> &lt;span style="color:#75715e"># std for Subject baseline Efficacy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>(Ns, &lt;span style="color:#ae81ff">0&lt;/span>, tau) &lt;span style="color:#75715e"># Subject baseline Efficacy (subject intercept)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#75715e"># Transform Age to a reasonable scale&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>minA &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">min&lt;/span>(A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> (A &lt;span style="color:#f92672">-&lt;/span> minA) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#ae81ff">10&lt;/span> &lt;span style="color:#75715e"># 1 unit = 10 years in original age&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The simulation of the participants’ age deserves some elaboration. These
ages are drawn from a &lt;em>truncated normal distribution&lt;/em>, which is
essentially a normal distribution with an upper and a lower bound
applied. The boundaries are set here such that the participants come
from a normal distribution with a mean age of 36 and a standard
deviation of 20, with ages below 18 or over 80 left out from the
sample&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. After drawing samples for the ages, the ages are scaled such
that (1) the youngest participant has a scaled age of zero, and (2) a
unit of difference in the scaled age corresponds to a 10-year difference
in the raw age. This is done to align the scale of the age to the scales
of the other variables. Otherwise, the original large scale of the age
would make the effects difficult to interpret.&lt;/p>
&lt;p>Now, let’s lay out the parameters for generating participants’
self-efficacy ($E$) and treatment outcomes ($D$):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>B_AE &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">.1&lt;/span> &lt;span style="color:#75715e"># Effect of Age on Efficacy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>B_TE &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">matrix&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#75715e"># Effect of Treatment on Efficacy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> &lt;span style="color:#ae81ff">.3&lt;/span>, &lt;span style="color:#ae81ff">.7&lt;/span>, &lt;span style="color:#ae81ff">1.3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> &lt;span style="color:#ae81ff">.3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">.7&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> ), byrow&lt;span style="color:#f92672">=&lt;/span>T, nrow&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>B_AD &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">.2&lt;/span> &lt;span style="color:#75715e"># Effect of Age on Outcome&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>B_ED &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#75715e"># Effect of Efficacy on Outcome&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>B_TD &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>) &lt;span style="color:#75715e"># Effect of Treatment on Outcome&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>delta &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-1.8&lt;/span> &lt;span style="color:#75715e"># Global Intercept for &amp;#34;E model&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>alpha &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#75715e"># Global Intercept for &amp;#34;D model&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As mentioned previously, it is assumed that the treatments
differentially affect efficacy according to gender (i.e., the treatment
effect on efficacy interacts with gender). This is specified in &lt;code>B_TE&lt;/code>
($\beta_{TE}$) as a 2-by-3 matrix. The first row of &lt;code>B_TE&lt;/code> corresponds
to male and the second to female. The three columns correspond to
Treatment 1 (&lt;code>Tx == 1&lt;/code>), 2 (&lt;code>Tx == 2&lt;/code>), and 3 (&lt;code>Tx == 3&lt;/code>) respectively.
For the direct treatment effects on the outcomes ($\beta_{TD}$), I set
them all to zero and assume no interaction with the gender, to keep
things simple.&lt;/p>
&lt;p>With all these parameters prepared, we can build up the generative
process of efficacy and the outcomes.&lt;/p>
&lt;p>As shown in the code chunk below, we first set up the time points &lt;code>t&lt;/code>.
&lt;code>t&lt;/code> starts with zero because it is assumed that the first assessment
took place &lt;em>right before&lt;/em> the treatments started. Hence, any treatment
effects at this point of measure should be zero. &lt;code>t = 0&lt;/code> does us the
favor as it naturally cancels out the treatment effects $\beta_{TD}$ and
$\beta_{TE}$.&lt;/p>
&lt;p>Let’s now simulate self-efficacy $E$. Before receiving the treatments,
the efficacy is assumed to be slightly influenced by age, through
&lt;code>B_AE&lt;/code>. In addition, baseline individual differences are modeled through
the random subject intercept &lt;code>subj&lt;/code>. After receiving the treatments,
treatment effects get added on through the term &lt;code>B_TE&lt;/code>. Efficacy is then
modeled as the sum of the aforementioned effects.&lt;/p>
&lt;p>After generating participants’ efficacy, we can again follow the arrows
of the DAGs to write down the generative process of $D^{\ast}$, coded as
&lt;code>D_latent&lt;/code> below.&lt;/p>
&lt;p>Finally, the treatment outcomes here, days of heavy drinking in the past
14 days, are assumed to follow a binomial distribution
$D \sim \text{Binomial}(14, p)$. The latent score &lt;code>D_latent&lt;/code> is reversed
with the negative sign since we want higher latent scores to result in
fewer heavy drinking days. Next, we simply map the latent scores to
probabilities with the logistic function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>t &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">:&lt;/span>(Nt &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#75715e"># time points of measure&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#75715e"># E model (causes of E)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>E &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>(t, &lt;span style="color:#a6e22e">function&lt;/span>(time) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> b_TE &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Ns, &lt;span style="color:#a6e22e">function&lt;/span>(i) B_TE[ G[i],Tx[i] ] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> delta &lt;span style="color:#f92672">+&lt;/span> subj &lt;span style="color:#f92672">+&lt;/span> B_AE &lt;span style="color:#f92672">*&lt;/span> A &lt;span style="color:#f92672">+&lt;/span> b_TE &lt;span style="color:#f92672">*&lt;/span> time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#75715e"># D model (causes of D)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>D_latent &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>(t, &lt;span style="color:#a6e22e">function&lt;/span>(time) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> alpha &lt;span style="color:#f92672">+&lt;/span> B_TD[Tx]&lt;span style="color:#f92672">*&lt;/span>time &lt;span style="color:#f92672">+&lt;/span> B_AD &lt;span style="color:#f92672">*&lt;/span> A &lt;span style="color:#f92672">+&lt;/span> B_ED &lt;span style="color:#f92672">*&lt;/span> E[, time &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>(t, &lt;span style="color:#a6e22e">function&lt;/span>(time) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> mu &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>D_latent[, time&lt;span style="color:#ae81ff">+1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span> &lt;span style="color:#a6e22e">rbinom&lt;/span>( Ns, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">14&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">logistic&lt;/span>(mu) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, let’s collect the values we have simulated into a long-form data
frame. Each row of the data frame corresponds to a response ($D$) of a
participant collected at a specific time point. I will name this data
frame the &lt;em>(treatment-)outcome-level&lt;/em> data frame. Later, we will see
another data frame that records &lt;em>item-level&lt;/em> responses.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Outcome-level responses (subject-time)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>dO &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Sid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Ns, time&lt;span style="color:#f92672">=&lt;/span>t, KEEP.OUT.ATTRS&lt;span style="color:#f92672">=&lt;/span>F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">with&lt;/span>( dO, A[Sid] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>Tx &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">with&lt;/span>( dO, Tx[Sid] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>G &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">with&lt;/span>( dO, G[Sid] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>D_latent &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>dO&lt;span style="color:#f92672">$&lt;/span>E &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">nrow&lt;/span>(dO) ) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> s &lt;span style="color:#f92672">=&lt;/span> dO&lt;span style="color:#f92672">$&lt;/span>Sid[i]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> t_ &lt;span style="color:#f92672">=&lt;/span> dO&lt;span style="color:#f92672">$&lt;/span>time[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> dO&lt;span style="color:#f92672">$&lt;/span>E[i] &lt;span style="color:#f92672">=&lt;/span> E[s, t_]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span> dO&lt;span style="color:#f92672">$&lt;/span>D_latent[i] &lt;span style="color:#f92672">=&lt;/span> D_latent[s, t_]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> dO&lt;span style="color:#f92672">$&lt;/span>D[i] &lt;span style="color:#f92672">=&lt;/span> D[s, t_]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(dO)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 360 obs. of 8 variables:
$ Sid : int 1 2 3 4 5 6 7 8 9 10 ...
$ time : int 0 0 0 0 0 0 0 0 0 0 ...
$ A : num 1.57 2.58 1.21 0.92 0 ...
$ Tx : int 1 1 1 1 1 1 1 1 1 1 ...
$ G : int 1 2 1 2 1 2 1 2 1 2 ...
$ D : int 13 9 6 11 12 13 7 12 13 12 ...
$ D_latent: num -2.534 -1.132 0.286 -0.942 -2.003 ...
$ E : num -2.8466 -1.6485 0.0451 -1.1261 -2.0029 ...
&lt;/code>&lt;/pre>
&lt;h3 id="item-responses">Item Responses&lt;/h3>
&lt;p>Item responses are generated similarly to $D$ since they both depend on
efficacy $E$. Let’s assume the questionnaire for measuring participants’
efficacy contains 20 items (&lt;code>Ni = 20&lt;/code>), and that the easiness of the
items is equally spaced and ranges from -.6.3 to 6.3.&lt;/p>
&lt;p>With the item easiness and a baseline choice preference &lt;code>kappa&lt;/code> set, we
now can generate the item responses $R$ from the participants’ efficacy.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>Ni &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">20&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>(&lt;span style="color:#ae81ff">-4.5&lt;/span>, &lt;span style="color:#ae81ff">4.5&lt;/span>, length &lt;span style="color:#f92672">=&lt;/span> Ni) &lt;span style="color:#75715e"># item easiness (sums to zero)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logit&lt;/span>(&lt;span style="color:#a6e22e">cumsum&lt;/span>(&lt;span style="color:#a6e22e">simplex&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>))))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>kappa &lt;span style="color:#f92672">=&lt;/span> kappa[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(kappa)]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>&lt;span style="color:#75715e"># Item-level responses (subject-item-time)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>dI &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Sid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Ns, Iid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Ni, time&lt;span style="color:#f92672">=&lt;/span>t, KEEP.OUT.ATTRS&lt;span style="color:#f92672">=&lt;/span>F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">nrow&lt;/span>(dI)) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> dI&lt;span style="color:#f92672">$&lt;/span>R[i] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">with&lt;/span>(dI, {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> &lt;span style="color:#a6e22e">rordlogit&lt;/span>(E[Sid[i], time[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">+&lt;/span> I[Iid[i]], kappa &lt;span style="color:#f92672">=&lt;/span> kappa)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(dI)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 7200 obs. of 4 variables:
$ Sid : int 1 2 3 4 5 6 7 8 9 10 ...
$ Iid : int 1 1 1 1 1 1 1 1 1 1 ...
$ time: int 0 0 0 0 0 0 0 0 0 0 ...
$ R : int 1 1 1 1 1 1 1 1 1 1 ...
&lt;/code>&lt;/pre>
&lt;h3 id="wrapping-up">Wrapping up&lt;/h3>
&lt;p>When a simulation gets massive, it is always better to pack up all the
code into a function. There will certainly be times when we have to
modify the structure of, or the parameter values in, the simulation. A
function helps a lot in these situations.&lt;/p>
&lt;p>The simulation code we have gone through so far is condensed into the
&lt;code>sim_data()&lt;/code> function in &lt;a href="simulation.R">&lt;code>simulation.R&lt;/code>&lt;/a>. You can just
load it and run the simulation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">source&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;simulation.R&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sim_data&lt;/span>() &lt;span style="color:#75715e"># Change param vals by overwriting default args&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>List of 3
$ dat :List of 18
..$ Ns : num 90
..$ Ntx : num 3
..$ Nt : num 4
..$ Nk : num 6
..$ Ni : num 20
..$ NI : num 7200
..$ Sid_I : int [1:7200] 1 2 3 4 5 6 7 8 9 10 ...
..$ Iid_I : int [1:7200] 1 1 1 1 1 1 1 1 1 1 ...
..$ time_I : int [1:7200] 0 0 0 0 0 0 0 0 0 0 ...
..$ R : int [1:7200] 1 1 1 1 1 1 1 1 1 1 ...
..$ NO : num 360
..$ Sid_O : int [1:360] 1 2 3 4 5 6 7 8 9 10 ...
..$ time_O : int [1:360] 0 0 0 0 0 0 0 0 0 0 ...
..$ G : int [1:360] 1 2 1 2 1 2 1 2 1 2 ...
..$ A : num [1:360] 1.947 5.137 0.634 2.086 5.84 ...
..$ Tx : int [1:360] 1 1 1 1 1 1 1 1 1 1 ...
..$ D : int [1:360] 5 9 13 6 8 14 6 14 9 11 ...
..$ D_latent: num [1:360] -0.534 -2.038 -1.235 -0.354 -0.725 ...
$ params:List of 13
..$ alpha : num 0
..$ delta : num -1.8
..$ B_AE : num 0.1
..$ B_TE : num [1:2, 1:3] 0.3 0.3 0.7 1 1.3 0.7
.. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
.. .. ..$ : chr [1:2] &amp;quot;male&amp;quot; &amp;quot;female&amp;quot;
.. .. ..$ : NULL
..$ B_AD : num 0.2
..$ B_ED : num 1
..$ B_TD : num [1:3] 0 0 0
..$ E : num [1:90, 1:4] -0.923 -3.065 -1.362 -0.772 -1.893 ...
.. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
.. .. ..$ : chr [1:90] &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; ...
.. .. ..$ : NULL
..$ I : num [1:20] -4.5 -4.03 -3.55 -3.08 -2.61 ...
..$ sigma_I: num 2.8
..$ kappa : num [1:5] -2.56 -1.3 0 1.3 2.56
..$ tau : num 0.8
..$ subj : num [1:90] 0.682 -1.779 0.374 0.82 -0.677 ...
$ others:List of 3
..$ minA : num 18.5
..$ D : int [1:90, 1:4] 5 9 13 6 8 14 6 14 9 11 ...
..$ D_latent: num [1:90, 1:4] -0.534 -2.038 -1.235 -0.354 -0.725 ...
.. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
.. .. ..$ : chr [1:90] &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; ...
.. .. ..$ : NULL
&lt;/code>&lt;/pre>
&lt;p>Notice the structure of the returned value by &lt;code>sim_data()&lt;/code>. It is a
nested named list with three elements at the top level: &lt;code>$dat&lt;/code>,
&lt;code>$params&lt;/code>, and &lt;code>$others&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>$dat&lt;/code> holds the data for model fitting. It has this specific
structure to match stan’s data block. It makes more sense later when
we see stan’s counterpart of the code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>$params&lt;/code> are the true parameter values used in the simulation. They
are returned so we can later compare the true parameter values to
those estimated by the model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>$others&lt;/code> holds other information that does not belong to the former
two but is nevertheless needed. For instance, the minimum age &lt;code>minA&lt;/code>
is saved in &lt;code>$others&lt;/code> so that raw ages in new data can be scaled
accordingly if one would like to make predictions with new datasets.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="model-formulation">Model Formulation&lt;/h2>
&lt;p>Now we have the simulated data, let’s begin constructing the model. In a
Bayesian framework, model construction feels pretty much like a
simulation (assume you are using Stan, not some wrappers around it)
since both are essentially describing the same &lt;em>data-generating
process&lt;/em>. Bayes, as complicated as it may seem, really just comes down
to four simple components:&lt;/p>
&lt;ol>
&lt;li>Data-generating process&lt;/li>
&lt;li>Prior distribution&lt;/li>
&lt;li>Data&lt;/li>
&lt;li>Posterior distribution&lt;/li>
&lt;/ol>
&lt;p>The &lt;em>data-generating process&lt;/em> describes the relations between the
parameters, that is, how the parameters arise from others. The &lt;em>priors&lt;/em>
give preliminary information on the parameters (the priors could be
thought of as part of the data-generating process as well). With these
prepared, the Bayesian machine incorporates the information from the
data to update the priors—according to the data-generating process—and
arrives at the &lt;em>posterior distribution&lt;/em>.&lt;/p>
&lt;p>The posterior is nothing else but a &lt;em>joint probability distribution of
all parameters&lt;/em>, after combining the information from the priors, the
data-generating process, and the data. It is what we believe the
parameters are, given the data and our assumptions. Everything we need
for inference derives from the posterior.&lt;/p>
&lt;p>So we’re now ready to formulate our model (data-generating process) in
Bayesian terms. This massive model can be chopped into three parts.
Let’s divide and conquer.&lt;/p>
&lt;h3 id="causes-of-efficacy">Causes of Efficacy&lt;/h3>
&lt;p>The first part of the model deals with the generation of Efficacy. The
formulation is shown below.&lt;/p>
&lt;img src="model-formulation_1.svg" style="max-width:24ch" />
&lt;p>The first two lines of the formulation describe how efficacy $E$ is
generated from the effects of participant baseline $\gamma_{Sid}$, age
$\beta_{AE}$, and treatment $\beta_{TE}$. The last three lines specify
the prior distributions for the relevant parameters. The “+” sign on the
prior distribution of $\tau$ is an abbreviation for the truncated normal
distribution with negative values removed. Since $\tau$ is a standard
deviation, it must not be negative. The truncation at zero imposes this
constraint on $\tau$.&lt;/p>
&lt;h3 id="causes-of-treatment-outcome">Causes of Treatment Outcome&lt;/h3>
&lt;p>With $E$ generated, we can then describe the process generating the
treatment outcome $D^{\ast}$. The outcomes in the current example are
the number of heavy-drinking days in the past 14 days. Hence, it is a
&lt;em>count&lt;/em> variable with an upper bound of 14, which makes the binomial an
ideal distribution for modeling the outcome. The binomial and the logit
link are used here to map the count outcomes to the underlying latent
variable $D$. As seen in the simulation, a negative sign is needed here
to reverse the relation between $D^{\ast}$ and $D$ such that a higher
value of $D$ corresponds to a fewer count of heavy-drinking days. The
third formula lays out the generative process of $D$, linking the
influences of the treatment, age, and efficacy to $D$.&lt;/p>
&lt;img src="model-formulation_2.svg" style="max-width:38ch" />
&lt;p>A special kind of prior distribution—exemplified by the distribution of
$\beta_{TD}$ here—deserves some attention. The distribution is assumed
to be a normal distribution with an unknown mean $\mu_{\beta_{TD}}$ and
unknown standard deviation $\sigma_{\beta_{TD}}$. Their priors are then
given by the next two lines. This nested structure of prior
distributions is known as hierarchical priors. By specifying such a
hierarchical structure in the priors, one can &lt;em>partial-pool&lt;/em> information
across the three treatment effects, thus reducing variation among the
$\beta_{TD}$ parameters.&lt;/p>
&lt;p>Partial pooling is used here for two reasons. First, it makes sense
theoretically to partial-pool the direct treatment effects on the
outcome since different treatments are still similar in some sense that
information across all treatments is useful and should be shared.
Second, partial pooling has the effect of reducing variation among the
pooled parameters. Without such a soft constraint on the $\beta_{TD}$
parameters, the model here will be unidentified (i.e., multiple sets of
solutions exist).&lt;/p>
&lt;h3 id="causes-of-item-response">Causes of Item Response&lt;/h3>
&lt;p>The final piece of the model describes the generation of participants’
responses to the items measuring efficacy. The submodel here has a
structure nearly identical to the rating scale model described in the
&lt;a href="https://yongfu.name/irt4">previous post&lt;/a>. The only difference here is that a sum-to-zero
constraint is imposed on the item parameters, which allows the item
easiness to be anchored and thus comparable to the true parameter values
from the simulation. The item parameters are also partially pooled here,
achieved through the hierarchical prior on $\sigma_I$.&lt;/p>
&lt;img src="model-formulation_3.svg" style="max-width:27ch" />
&lt;h2 id="stan-a-brief-introduction">Stan: A Brief Introduction&lt;/h2>
&lt;p>We have completed our formal model-building. Now, we have to code it in
Stan, a language for implementing Bayesian models.&lt;/p>
&lt;p>Stan is hard for anyone first meeting it. Sadly, I think there is really
no “introductory” material on Stan because Bayesian statistics is
already an advanced topic. How, then, could anyone get started with
Bayes if everything is advanced and hard?&lt;/p>
&lt;p>We need &lt;em>scaffolds&lt;/em>. The scaffolds for learning statistics are
simulations and programming skills that license them. Building up
simulations allows one to experiment with stat models and therefore
provides opportunities for understanding. So to learn Stan, one should
probably begin with examples. The &lt;a href="https://mc-stan.org/docs/stan-users-guide">Stan User’s
Guide&lt;/a> provides tons of
&lt;a href="https://mc-stan.org/docs/stan-users-guide/example-models.html">example
models&lt;/a>.
Start with simple models or any one that you’re familiar with. Simulate
data based on the model’s assumption, construct the model by modifying
the example code, and fit the model to see if it correctly recovers the
parameters. If succeeded, extend the simulation and the model for the
next round of testing. If failed, simplify the model (and the
simulation, if needed) to pinpoint the potential problems.&lt;/p>
&lt;p>Interestingly, the process of learning Stan is no different from using
Stan. In both situations, one is unable to proceed without scaffolds.
Stan gives you full flexibility, and with this modeling power, one
immediately finds out there are infinite ways to go wrong. Simulation is
the only basis that keeps us oriented while navigating through the mess
of modeling. That said, some acquaintance with the structure of Stan
files does help when getting started.&lt;/p>
&lt;h3 id="stan-file-structure">Stan File Structure&lt;/h3>
&lt;p>The template below sketches the most common structure of a Stan program.
A Stan program consists of several “blocks”. Three of the most basic
blocks are shown here: the &lt;em>data&lt;/em>, the &lt;em>parameters&lt;/em>, and the &lt;em>model&lt;/em>
blocks. The data block simply contains variable-declaration code for the
data. The parameters block declares variables that hold the parameters.
The model block is where the data-generating process and the priors get
specified.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>data {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> &lt;span style="color:#75715e">// Declare varaiables for data (passed in from R)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>parameters {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> &lt;span style="color:#75715e">// Declare model parameters
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>model {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span> &lt;span style="color:#75715e">// Describe data-generating process (including priors)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the blocks written up, one then compiles the stan program and feeds
it the data. Stan would then construct and sample from the posterior.
When all of these are done, posterior samples along with diagnostic
information of sample quality are returned.&lt;/p>
&lt;p>That’s it. Don’t read too much while learning Stan. Getting the hands
dirty is a much better way. So now let’s proceed to our massive model.&lt;/p>
&lt;h2 id="stan-model">Stan Model&lt;/h2>
&lt;p>I’ll present our model coded in Stan one block at a time so that we
don’t get overwhelmed. The stan file gets executed from the top to the
bottom, and the blocks have to be defined in the given order as shown in
the above template. Through this structure, the &lt;em>parameters&lt;/em> and the
&lt;em>model&lt;/em> blocks have access to the data variables defined in the &lt;em>data&lt;/em>
block, and the &lt;em>model&lt;/em> block has access to the parameter and data
variables defined in previous blocks. Let’s first look at the &lt;em>data&lt;/em>
block of our model.&lt;/p>
&lt;h3 id="the-data-block">The data block&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>data {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> Ns; &lt;span style="color:#75715e">// num of subjects
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> Ntx; &lt;span style="color:#75715e">// num of treatments
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> Nt; &lt;span style="color:#75715e">// num of time points
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> Nk; &lt;span style="color:#75715e">// num of Likert scale choices
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> Ni; &lt;span style="color:#75715e">// num of items in the self-efficacy scale
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> &lt;span style="color:#75715e">// Item-level responses (NI=Ns*Ni*Nt)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> NI;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> array[NI] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Ns&lt;span style="color:#f92672">&amp;gt;&lt;/span> Sid_I; &lt;span style="color:#75715e">// Subject ID
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NI] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Ni&lt;span style="color:#f92672">&amp;gt;&lt;/span> Iid_I; &lt;span style="color:#75715e">// Item ID
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NI] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Nt&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> time_I; &lt;span style="color:#75715e">// time point of obs.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NI] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Nk&lt;span style="color:#f92672">&amp;gt;&lt;/span> R; &lt;span style="color:#75715e">// Responses on Efficacy scale
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span> &lt;span style="color:#75715e">// Outcome-level responses (NO=Ns*Nt)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> NO;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span> array[NO] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Ns&lt;span style="color:#f92672">&amp;gt;&lt;/span> Sid_O; &lt;span style="color:#75715e">// Subject ID
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NO] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>Nt&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> time_O; &lt;span style="color:#75715e">// time point of obs.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NO] real&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> A; &lt;span style="color:#75715e">// Age scaled: (A-min(A))/10
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NO] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> Tx; &lt;span style="color:#75715e">// Treatment received
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NO] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> G; &lt;span style="color:#75715e">// Gender
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> array[NO] &lt;span style="color:#66d9ef">int&lt;/span>&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>,upper&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">14&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> D; &lt;span style="color:#75715e">// Binomial outcome (heavy-drinking in past 14 days)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In our data block, we simply define all variables that hold information
about the data we pass in through R. Thus, the variables here correspond
exactly to the &lt;code>$dat&lt;/code> element of the simulated data list.&lt;/p>
&lt;p>Stan is a statically typed language, meaning that the type must be
specified for any variables defined. In addition, boundaries could also
be imposed on the typed variables. This is done through the angle
brackets &lt;code>&amp;lt;&amp;gt;&lt;/code> after the type definition, as shown in some of the
variables above. For the &lt;em>data&lt;/em> block, the constraint on the boundaries
guards against unexpected input data format. For the &lt;em>parameters&lt;/em> block,
these constraints are often used to truncate the prior distributions
later assigned to the parameters.&lt;/p>
&lt;h3 id="parameters-and-transformed-parameters">parameters and transformed parameters&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>parameters {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> &lt;span style="color:#75715e">// IRT model params
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> sigma_I;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> vector[Ni&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] zI_raw;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> ordered[Nk&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] kappa;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> &lt;span style="color:#75715e">// E model params
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> matrix[&lt;span style="color:#ae81ff">2&lt;/span>,Ntx] B_TE; &lt;span style="color:#75715e">// Treatment on Efficacy (indirect effect)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real B_AE; &lt;span style="color:#75715e">// Age on Efficacy
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real delta; &lt;span style="color:#75715e">// global intercept (E linear model)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> tau; &lt;span style="color:#75715e">// subj baseline std (E linear model)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[Ns] zSubj; &lt;span style="color:#75715e">// subject baseline
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> &lt;span style="color:#75715e">// D model params
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[Ntx] zB_TD; &lt;span style="color:#75715e">// Treatment on Outcome (direct effect)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real mu_TD; &lt;span style="color:#75715e">// Common mean among direct treatment effects
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real&lt;span style="color:#f92672">&amp;lt;&lt;/span>lower&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span> sigma_TD; &lt;span style="color:#75715e">// Common std among direct treatment effects
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real B_AD; &lt;span style="color:#75715e">// Age on outcome
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real B_ED; &lt;span style="color:#75715e">// Efficacy on outcome
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> real alpha; &lt;span style="color:#75715e">// global intercept (D linear model)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span>transformed parameters {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span> &lt;span style="color:#75715e">// IRT item params (sum-to-zero contrained)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[Ni] I &lt;span style="color:#f92672">=&lt;/span> sigma_I &lt;span style="color:#f92672">*&lt;/span> append_row( &lt;span style="color:#f92672">-&lt;/span>sum(zI_raw), zI_raw );
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26&lt;/span>&lt;span> &lt;span style="color:#75715e">// subject baseline efficacy
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[Ns] subj;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28&lt;/span>&lt;span> subj &lt;span style="color:#f92672">=&lt;/span> tau &lt;span style="color:#f92672">*&lt;/span> zSubj;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30&lt;/span>&lt;span> matrix[Ns,Nt] E;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31&lt;/span>&lt;span> &lt;span style="color:#75715e">// Transformed E
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> ( i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>NO ) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33&lt;/span>&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> sid &lt;span style="color:#f92672">=&lt;/span> Sid_O[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34&lt;/span>&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> time &lt;span style="color:#f92672">=&lt;/span> time_O[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35&lt;/span>&lt;span> E[sid,time&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> delta &lt;span style="color:#f92672">+&lt;/span> subj[sid] &lt;span style="color:#f92672">+&lt;/span> B_AE&lt;span style="color:#f92672">*&lt;/span>A[i] &lt;span style="color:#f92672">+&lt;/span> B_TE[G[i],Tx[i]]&lt;span style="color:#f92672">*&lt;/span>time;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36&lt;/span>&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38&lt;/span>&lt;span> &lt;span style="color:#75715e">// Direct treatment effect
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[Ntx] B_TD &lt;span style="color:#f92672">=&lt;/span> fma(zB_TD, sigma_TD, mu_TD);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;em>parameters&lt;/em> block defines variables for the parameters in a stan
model. This should be enough, theoretically. However, Bayesian models
are often very complex (such as our massive model). This can lead to
complications in posterior geometries such that Stan’s algorithm would
have difficulty exploring the distribution. To deal with this issue, it
is often required to apply mathematical tricks known as
&lt;em>reparameterization&lt;/em> to modify the posterior such that it becomes easier
for Stan to sample from.&lt;/p>
&lt;p>This is where the &lt;em>transformed parameters&lt;/em> block comes into play. In
general, when reparameterizing our model, we define those
reparameterized parameters in the &lt;em>parameters&lt;/em> block. These parameters,
however, are often in forms that are unintuitive for us. We thus utilize
the &lt;em>transformed parameters&lt;/em> block to transform those parameters back
into the forms we are familiar with. The transformed parameters get
returned from Stan as if they are parameters after sampling. But they
are not actual parameters per se but only functions of parameters. That
said, once we have the &lt;em>transformed parameters&lt;/em> block set up and the
sampler working, it’s safe to simply treat them all as parameters.
Understanding the details of the transform block helps when we have
trouble coding the model.&lt;/p>
&lt;p>Here, we utilize the transform block to convert the &lt;a href="https://mc-stan.org/docs/stan-users-guide/reparameterization.html">non-centered
parameters&lt;/a>
(&lt;code>zB_TD&lt;/code> and &lt;code>zE&lt;/code>) back to the familiar centered forms (&lt;code>subj&lt;/code>, &lt;code>B_TD&lt;/code>
and &lt;code>E&lt;/code>). In addition to non-centered reparameterizations, another
transformation is performed here to reconstruct the full vector of the
sum-to-zero constrained item easiness parameters.&lt;/p>
&lt;h3 id="model">model&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>model {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> &lt;span style="color:#75715e">// Priors for IRT parameters
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> zI_raw &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> sigma_I &lt;span style="color:#f92672">~&lt;/span> exponential(&lt;span style="color:#ae81ff">1&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> kappa &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> &lt;span style="color:#75715e">// Priors for causes of E (T -&amp;gt; E)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> to_vector(B_TE) &lt;span style="color:#f92672">~&lt;/span> normal(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1.5&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> B_AE &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> delta &lt;span style="color:#f92672">~&lt;/span> normal(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1.5&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> tau &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> zSubj &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> &lt;span style="color:#75715e">// Priors for causes of D (T -&amp;gt; D &amp;lt;- E)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> zB_TD &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span> mu_TD &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span> sigma_TD &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span> B_AD &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span> alpha &lt;span style="color:#f92672">~&lt;/span> normal(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1.5&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span> B_ED &lt;span style="color:#f92672">~&lt;/span> std_normal();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span> &lt;span style="color:#75715e">// Causes of E (see transformed parameters block)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span> &lt;span style="color:#75715e">// Causes of D
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[NO] mu;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26&lt;/span>&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> ( i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>NO ) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27&lt;/span>&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> sid, time;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28&lt;/span>&lt;span> sid &lt;span style="color:#f92672">=&lt;/span> Sid_O[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29&lt;/span>&lt;span> time &lt;span style="color:#f92672">=&lt;/span> time_O[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30&lt;/span>&lt;span> mu[i] &lt;span style="color:#f92672">=&lt;/span> alpha &lt;span style="color:#f92672">+&lt;/span> B_TD[Tx[i]]&lt;span style="color:#f92672">*&lt;/span>time &lt;span style="color:#f92672">+&lt;/span> B_AD&lt;span style="color:#f92672">*&lt;/span>A[i] &lt;span style="color:#f92672">+&lt;/span> B_ED&lt;span style="color:#f92672">*&lt;/span>E[sid,time&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31&lt;/span>&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32&lt;/span>&lt;span> D &lt;span style="color:#f92672">~&lt;/span> binomial_logit( &lt;span style="color:#ae81ff">14&lt;/span>, &lt;span style="color:#f92672">-&lt;/span>mu );
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34&lt;/span>&lt;span> &lt;span style="color:#75715e">// Measurement model (IRT)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35&lt;/span>&lt;span>&lt;span style="color:#75715e">&lt;/span> vector[NI] phi;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36&lt;/span>&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> ( i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>NI )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37&lt;/span>&lt;span> phi[i] &lt;span style="color:#f92672">=&lt;/span> E[Sid_I[i],time_I[i]&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">+&lt;/span> I[Iid_I[i]];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38&lt;/span>&lt;span> R &lt;span style="color:#f92672">~&lt;/span> ordered_logistic( phi, kappa );
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We are finally at the &lt;em>model&lt;/em> block, often the most terrifying part of a
stan program. It becomes less terrifying if one knows that the code in
the model block can be classified into two functional groups. The first
is the &lt;a href="https://mc-stan.org/docs/reference-manual/sampling-statements.html">&lt;em>sampling
statements&lt;/em>&lt;/a>.
These statements tell Stan to update the posterior distribution, with
information from the priors, the data, or the combination of both. The
posterior gets updated by adding values to itself. This means that the
&lt;em>order&lt;/em> of the sampling statements does not influence the output
(ignoring differences resulting from numerical instability). In general,
although it is legitimate to switch the order of the sampling
statements, we usually organize them in ways that make our modeling
logic easier to grasp.&lt;/p>
&lt;p>The function of the second type of code in the model block is
computation. Often, before reaching a sampling statement, some
intermediate computation is required. For instance,
&lt;code>D ~ binomial_logit( 14, -mu )&lt;/code> depends on the variable &lt;code>mu&lt;/code>. So &lt;code>mu&lt;/code>
has to be computed before one can invoke the sampling statement.&lt;/p>
&lt;p>For our massive model here, I first define the prior distributions in
the first three blocks, with those related grouped into the same block of
code. I then proceed to specify the relations between the parameters and
the data. For efficacy $E$, the relevant relationships are already
defined in the &lt;em>transformed parameters&lt;/em> block, so only a line of comment
is left as a placeholder. Below $E$ is the block for the “causes of D”,
which defines the upstream variables influencing &lt;code>D&lt;/code> (through &lt;code>mu&lt;/code>).
Finally, the last block defines the generative process for the item
responses.&lt;/p>
&lt;h3 id="r-interface">R Interface&lt;/h3>
&lt;p>There are interfaces written in many interpreted languages built to
interact with Stan. In R, the most commonly used are
&lt;a href="https://github.com/stan-dev/rstan">&lt;code>rstan&lt;/code>&lt;/a> and
&lt;a href="https://github.com/stan-dev/cmdstanr">&lt;code>cmdstanr&lt;/code>&lt;/a>. I will be using
&lt;code>cmdstanr&lt;/code>.&lt;/p>
&lt;p>Here’s the code for calling Stan to fit our model. The model takes about
5 minutes to run on an M1 Macbook Air and 16 minutes on a much older
Windows laptop. So it would be wise to save the fitted model once Stan
finishes sampling.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">source&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;simulation.R&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">1977&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sim_data&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e"># Compile stan file&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>m &lt;span style="color:#f92672">=&lt;/span> cmdstanr&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">cmdstan_model&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;m1.stan&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#75715e"># Fit model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>fit &lt;span style="color:#f92672">=&lt;/span> m&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">sample&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> data&lt;span style="color:#f92672">=&lt;/span>d&lt;span style="color:#f92672">$&lt;/span>dat,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> chains&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> parallel_chains&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#75715e"># Save fitted model for later use&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>fit&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">save_object&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;m1.RDS&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I’ve put the fitted model &lt;code>m1.RDS&lt;/code> on &lt;a href="https://drive.google.com/drive/folders/1hCltt7aM4pV2GhFiQQNUhFdTgm09snGB">Google
Drive&lt;/a>
so you don’t have to refit the model. All the scripts in this post are
also available on
&lt;a href="https://github.com/liao961120/stom/tree/main/inst/cases/treatment_dynamics/post">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="model-checking">Model Checking&lt;/h2>
&lt;p>With our model fitted, we can now check if it correctly recovers the
parameter. But before doing so, we need to first assess the quality of
the posterior samples.&lt;/p>
&lt;h3 id="quality-of-posterior-samples">Quality of Posterior Samples&lt;/h3>
&lt;p>When fitting Bayesian models, at least two chains (i.e., two independent
sets of samples) are required to evaluate the quality of the posterior
samples. These chains could be visualized in trace plots, which record
the parameter values in the order they are sampled. Thus, a curve in a
trace plot is essentially the trace of a sampler exploring the posterior
(in one of the dimensions). For high-quality sampling, traces of
different samplers should be independent and fluctuate at similar ranges
of values. This results in chains that mix well, and the trace plots
would then look like caterpillars. Conversely, if the posterior
geometries are difficult to explore, the samplers may be stuck in local
regions. Or, they may explore different regions of the posterior (e.g.,
when the posterior is multi-modal), resulting in a complete separation
of the chains in the trace plots. When these happen, the chains may
disagree with each other, leading to the non-convergence of samples from
different chains.&lt;/p>
&lt;p>Non-convergence indicates that inferences based on the posterior samples
are problematic and shouldn’t be trusted. Upon such circumstances, the
modeler should try to locate the sources of non-convergence and fix the
model. Misspecification of the generative process, loose priors,
unidentifiability, and unexpected data are all potential causes of
non-convergence.&lt;/p>
&lt;p>To evaluate how well the chains mix, we look at a statistic known as
$\hat{R}$ (rhat). Typically, an $\hat{R}$ value less than 1.05 for a
particular parameter indicates a good enough mixing of chains. Let’s
load our fitted model and look at the sampling quality. I will use some
handy wrappers around the functions from &lt;code>cmdstanr&lt;/code> through my package
&lt;code>stom&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">source&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;simulation.R&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">1977&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sim_data&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>m &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">readRDS&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;m1.RDS&amp;#34;&lt;/span>) &lt;span style="color:#75715e"># Load fitted model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> stom&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">precis&lt;/span>(m, &lt;span style="color:#ae81ff">5&lt;/span>) &lt;span style="color:#75715e"># Compute posterior summary&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">head&lt;/span>(s)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 x 8
variable mean sd q5 q95 rhat ess_bulk ess_tail
&amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 sigma_I 2.56 0.379 2.02 3.26 1.01 330. 543.
2 zI_raw[1] -1.68 0.248 -2.11 -1.29 1.01 349. 567.
3 zI_raw[2] -1.43 0.208 -1.79 -1.10 1.01 360. 565.
4 zI_raw[3] -1.17 0.175 -1.48 -0.891 1.01 349. 525.
5 zI_raw[4] -1.05 0.156 -1.31 -0.802 1.01 355. 635.
6 zI_raw[5] -0.826 0.126 -1.04 -0.626 1.01 354. 698.
&lt;/code>&lt;/pre>
&lt;p>The function &lt;code>stom::precis&lt;/code> computes parameter summaries from the
posterior samples, including the $\hat{R}$ statistic. Let’s plot the
$\hat{R}$ values for all of the parameters to see if any of them are
unexpectedly large.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>(s&lt;span style="color:#f92672">$&lt;/span>rhat)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-8-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>The $\hat{R}$ values look fine. The largest one is 1.0106079, which is
much lower than 1.05. Just to be careful, let’s inspect the trace plots
of those parameters with the largest $\hat{R}$ values and the lowest
&lt;a href="https://mc-stan.org/docs/reference-manual/effective-sample-size.html">effective sample size
(ESS)&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(bayesplot)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">color_scheme_set&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;viridis&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>pars &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> s&lt;span style="color:#f92672">$&lt;/span>variable[s&lt;span style="color:#f92672">$&lt;/span>rhat &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">1.006&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> s&lt;span style="color:#f92672">$&lt;/span>variable[s&lt;span style="color:#f92672">$&lt;/span>ess_bulk &lt;span style="color:#f92672">&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">400&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span> s&lt;span style="color:#f92672">$&lt;/span>variable[s&lt;span style="color:#f92672">$&lt;/span>ess_tail &lt;span style="color:#f92672">&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">400&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mcmc_trace&lt;/span>(m&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">draws&lt;/span>(), pars&lt;span style="color:#f92672">=&lt;/span>pars)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-9-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>The trace plots seem fine too. Now we can proceed to check the recovery
of the parameters.&lt;/p>
&lt;h3 id="parameter-recovery">Parameter Recovery&lt;/h3>
&lt;p>Let’s first look at how well the IRT submodel works. I plot the
estimated (y-axis) against the true (x-axis) parameter values. The grey
dashed line shows the identity. Points lying on this line indicate
perfect recovery through the posterior mean. The vertical pink bars
cover 90% of posterior density around the mean.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>( mfrow &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( p in &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;kappa&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;E&amp;#34;&lt;/span>) ) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> &lt;span style="color:#a6e22e">if &lt;/span>( p &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;E&amp;#34;&lt;/span> ) &lt;span style="color:#a6e22e">par&lt;/span>(mfrow &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> mt &lt;span style="color:#f92672">=&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>params[[p]] &lt;span style="color:#75715e"># True value&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> mm &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>mean &lt;span style="color:#75715e"># Posterior mean&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> u &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>q95 &lt;span style="color:#75715e"># Posterior .05 quantile &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> l &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>q5 &lt;span style="color:#75715e"># Posterior .95 quantile &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> &lt;span style="color:#a6e22e">plot&lt;/span>( mt, mm , main&lt;span style="color:#f92672">=&lt;/span>p, ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-4.6&lt;/span>,&lt;span style="color:#ae81ff">4.6&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;True&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Estimated&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span> &lt;span style="color:#a6e22e">for &lt;/span>( i in &lt;span style="color:#a6e22e">seq_along&lt;/span>(mm) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(mt[i],mt[i]), &lt;span style="color:#a6e22e">c&lt;/span>(u[i],l[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-10-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-10-2.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Let’s next look at the parameters for modeling the causes of $E$ and
$D$. This time I plot the true parameter values as solid black dots. The
pink open circles are the posterior mean estimates, and the bars mark
the 90% density around the mean.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>beta &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;B_TE&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B_AE&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B_AD&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B_ED&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B_TD&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;delta&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;alpha&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;tau&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;sigma_I&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>b_true &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">lapply&lt;/span>( beta, &lt;span style="color:#a6e22e">function&lt;/span>(p) d&lt;span style="color:#f92672">$&lt;/span>params[[p]] ) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">unlist&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>b_est &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">lapply&lt;/span>( beta, &lt;span style="color:#a6e22e">function&lt;/span>(p) &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>mean ) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">unlist&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>b_est_upp &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">lapply&lt;/span>( beta, &lt;span style="color:#a6e22e">function&lt;/span>(p) &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>q5 ) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">unlist&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>b_est_low &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">lapply&lt;/span>( beta, &lt;span style="color:#a6e22e">function&lt;/span>(p) &lt;span style="color:#a6e22e">get_pars&lt;/span>(s, p)&lt;span style="color:#f92672">$&lt;/span>q95 ) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">unlist&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( b_true, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.6&lt;/span>, &lt;span style="color:#ae81ff">3.2&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Parameter value&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>(h &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( b_est, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( i in &lt;span style="color:#a6e22e">seq_along&lt;/span>(b_est) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i,i), &lt;span style="color:#a6e22e">c&lt;/span>(b_est_upp[i],b_est_low[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( v in &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">6&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">9&lt;/span>,&lt;span style="color:#ae81ff">12&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span> &lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>v&lt;span style="color:#ae81ff">+.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( v in &lt;span style="color:#ae81ff">7&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">9&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mtext&lt;/span>( beta[v&lt;span style="color:#ae81ff">-5&lt;/span>], at&lt;span style="color:#f92672">=&lt;/span>v )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mtext&lt;/span>( beta[1], at &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">length&lt;/span>(&lt;span style="color:#a6e22e">get_pars&lt;/span>(d,&lt;span style="color:#e6db74">&amp;#34;B_TE&amp;#34;&lt;/span>)) &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">.5&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mtext&lt;/span>( beta[5], at &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">11&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( i in &lt;span style="color:#ae81ff">6&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">9&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mtext&lt;/span>( beta[i], at &lt;span style="color:#f92672">=&lt;/span> i&lt;span style="color:#ae81ff">+7&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>With all these checks, we can see that the model does pretty well in
recovering the parameters! So now, we’re ready to utilize the posterior
samples for inference.&lt;/p>
&lt;h2 id="posterior-predictions">Posterior Predictions&lt;/h2>
&lt;p>Bayesian models are &lt;em>generative&lt;/em> in the sense that they can generate
samples, much like nature generates stochastic observations of various
kinds. This can be done because, instead of providing point estimates, a
Bayesian model returns a full distribution of parameter estimates (i.e.,
the posterior distribution). A distribution contains information of
uncertainty. Thus, the samples computed from the posterior also retain
such uncertainty.&lt;/p>
&lt;p>Given the generative nature of Bayesian models, we can utilize the
posterior distribution to compute various predictions. These predictions
carry forward the uncertainty of the posterior and are thus known as
&lt;em>posterior predictive distributions&lt;/em>. Based on the purpose of the
inference, we can design how the predictions are computed. Here, I
provide two examples. The first computes the predicted &lt;strong>trajectory of
the clinical outcomes&lt;/strong> for all six treatment-gender combinations. The
second compares the effect of the treatments in terms of the &lt;strong>expected
reduction in days of heavy drinking&lt;/strong> relative to the baseline
treatment. Since everything derived from the posterior is a
distribution, we have to plot, rather than tabulate, our predictions.&lt;/p>
&lt;p>Before starting, let me first demonstrate the computation of a single
prediction from the posterior. The rest should be more straightforward
since posterior predictive distributions are simply the generalization
from a single prediction to multiple predictions using all posterior
samples.&lt;/p>
&lt;h3 id="a-single-posterior-prediction">A Single Posterior Prediction&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>post &lt;span style="color:#f92672">=&lt;/span> stom&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">extract2&lt;/span>(m)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">delta&lt;/span>())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> num [1:3000] -2.51 -2.12 -1.76 -2.16 -1.86 ...
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">delta&lt;/span>(idx &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">6&lt;/span>))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -2.11809 -2.15574 -1.69995
&lt;/code>&lt;/pre>
&lt;p>&lt;code>stom::extract2()&lt;/code> extracts the posterior and exposes its parameters as
R6 methods. To access the values of a parameter, we use the syntax
&lt;code>${param}()&lt;/code>. In our example here, doing so would return 3000 samples
since 1000 samples were drawn for each of the 3 chains from our model.
We can control the samples drawn by passing an optional argument &lt;code>idx&lt;/code>
into the parameter method. For instance, &lt;code>post$delta(2)&lt;/code> would return
the second set of posterior samples.&lt;/p>
&lt;p>What I would like to do here is to compute the latent scores $D$
underlying the treatment outcome for a single participant at time 1, 2,
3, and 4. I will use subject 31 (&lt;code>Sid == 31&lt;/code>) as an example here. So
let’s first prepare the data.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Sid &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">31&lt;/span> &lt;span style="color:#75715e"># subject ID&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>idx_subj &lt;span style="color:#f92672">=&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>dat&lt;span style="color:#f92672">$&lt;/span>Sid_O &lt;span style="color:#f92672">==&lt;/span> Sid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>dat&lt;span style="color:#f92672">$&lt;/span>A[idx_subj][1] &lt;span style="color:#75715e"># Age of subject 31&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>G &lt;span style="color:#f92672">=&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>dat&lt;span style="color:#f92672">$&lt;/span>G[idx_subj][1] &lt;span style="color:#75715e"># Gender of subject 31&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>Tid &lt;span style="color:#f92672">=&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>dat&lt;span style="color:#f92672">$&lt;/span>Tx[idx_subj][1] &lt;span style="color:#75715e"># Treatment received by subject 31&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>&lt;span style="color:#a6e22e">c&lt;/span>(A, G, Tid)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.987944 1.000000 2.000000
&lt;/code>&lt;/pre>
&lt;p>Now, let’s extract 1 of the 3000 sets of the posterior samples.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>idx &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2999&lt;/span> &lt;span style="color:#75715e"># posterior sample drawn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>alpha &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">alpha&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>delta &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">delta&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>b_AD &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_AD&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>b_ED &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_ED&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>b_AE &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_AE&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>b_TD &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_TD&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>b_TE &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_TE&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9&lt;/span>&lt;span>subj &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">subj&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the parameters and the data prepared, we can compute our
prediction.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>time &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( time, &lt;span style="color:#a6e22e">function&lt;/span>(t) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> E &lt;span style="color:#f92672">=&lt;/span> delta &lt;span style="color:#f92672">+&lt;/span> subj[Sid] &lt;span style="color:#f92672">+&lt;/span> b_AE&lt;span style="color:#f92672">*&lt;/span>A &lt;span style="color:#f92672">+&lt;/span> b_TE[G,Tid]&lt;span style="color:#f92672">*&lt;/span>(t&lt;span style="color:#ae81ff">-1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> alpha &lt;span style="color:#f92672">+&lt;/span> b_TD[Tid]&lt;span style="color:#f92672">*&lt;/span>(t&lt;span style="color:#ae81ff">-1&lt;/span>) &lt;span style="color:#f92672">+&lt;/span> b_AD&lt;span style="color:#f92672">*&lt;/span>A &lt;span style="color:#f92672">+&lt;/span> b_ED&lt;span style="color:#f92672">*&lt;/span>E
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>D
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -0.8849468 -0.1329965 0.6189538 1.3709041
&lt;/code>&lt;/pre>
&lt;p>Okay, now we have a prediction from a single set of posterior samples.
In general, we would like to have predictions based on &lt;em>all&lt;/em> the
posterior samples. To do this, we simply wrap the code above in a
function and repeat it for all of the posterior samples.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>predict_obs &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(Tid, Sid, A, G, idx) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> alpha &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">alpha&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> delta &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">delta&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> b_AD &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_AD&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> b_ED &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_ED&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> b_AE &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_AE&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> b_TD &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_TD&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> b_TE &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">B_TE&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> subj &lt;span style="color:#f92672">=&lt;/span> post&lt;span style="color:#f92672">$&lt;/span>&lt;span style="color:#a6e22e">subj&lt;/span>(idx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> time &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( time, &lt;span style="color:#a6e22e">function&lt;/span>(t) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span> E &lt;span style="color:#f92672">=&lt;/span> delta &lt;span style="color:#f92672">+&lt;/span> subj[Sid] &lt;span style="color:#f92672">+&lt;/span> b_AE&lt;span style="color:#f92672">*&lt;/span>A &lt;span style="color:#f92672">+&lt;/span> b_TE[G,Tid]&lt;span style="color:#f92672">*&lt;/span>(t&lt;span style="color:#ae81ff">-1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> alpha &lt;span style="color:#f92672">+&lt;/span> b_TD[Tid]&lt;span style="color:#f92672">*&lt;/span>(t&lt;span style="color:#ae81ff">-1&lt;/span>) &lt;span style="color:#f92672">+&lt;/span> b_AD&lt;span style="color:#f92672">*&lt;/span>A &lt;span style="color:#f92672">+&lt;/span> b_ED&lt;span style="color:#f92672">*&lt;/span>E
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span> D
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e"># A single prediction based on the 2999th set of posterior samples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>( &lt;span style="color:#a6e22e">predict_obs&lt;/span>(Tid, Sid, A, G, idx&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2999&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -0.8849468 -0.1329965 0.6189538 1.3709041
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Compute over the full posterior&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">3000&lt;/span>, &lt;span style="color:#a6e22e">function&lt;/span>(i) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#a6e22e">predict_obs&lt;/span>(Tid, Sid, A, G, idx&lt;span style="color:#f92672">=&lt;/span>i)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(D)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> num [1:4, 1:3000] -0.319 0.397 1.114 1.83 -0.891 ...
&lt;/code>&lt;/pre>
&lt;h3 id="counterfactual-predictions">Counterfactual Predictions&lt;/h3>
&lt;p>In the demonstration above, we compute the predictions based on one of
the observations in our data, that is, the data from subject 31. We used
his gender, age, and treatment condition as the data, and his estimated
baseline efficacy &lt;code>subj[Sid]&lt;/code>, to compute the prediction. But we can do
something more interesting by going beyond our current data to compute
&lt;em>counterfactual predictions&lt;/em>. For instance, we can arbitrarily determine
the treatment received and then compute the prediction. Doing so
effectively asks the question: what would the outcome be if subject 31
had been treated with Treatment 3 instead of 2? Likewise, the age and
gender of the participant can be changed to ask similar questions. We
can go even further to simulate a completely new population, with a
different set of baseline subject efficacy parameters, if we have some
idea of what such a population is like (e.g., the degree of variation in
baseline conditions).&lt;/p>
&lt;p>Another thing worth noting is the time variable. In our data and the
above prediction, time is treated as discrete time points (1-4). But we
can treat time as continuous when we compute predictions, simply by
using a smaller time step, such as &lt;code>time = seq(1,4,.01)&lt;/code>. Doing so
results in a smoother trajectory—as we’ll see later—for the predictions
across time.&lt;/p>
&lt;h2 id="visualizing-treatment-effects">Visualizing Treatment Effects&lt;/h2>
&lt;p>Now, let’s use the full posterior to compute the posterior predictive
distributions. The posterior predictive distributions computed here are
&lt;em>counterfactuals&lt;/em>: all 90 subjects are used repeatedly for computing
each of the six treatment-gender combinations. The treatment received
and gender of the subjects are set to match the group’s condition when
computing predictions for that combination.&lt;/p>
&lt;p>The functions for plotting and calculating the relevant quantities are
available in the script
&lt;a href="https://github.com/liao961120/stom/tree/main/inst/cases/treatment_dynamics/post/func_post_predict.R">&lt;code>func_post_predict.R&lt;/code>&lt;/a>.
The manipulation of the posterior samples in the script may seem
sophisticated. But they are all based on the core function
&lt;code>predict_obs()&lt;/code>, which is nearly the same as its simplified version
presented previously.&lt;/p>
&lt;h3 id="trajectories-by-treatment-and-gender">Trajectories By Treatment and Gender&lt;/h3>
&lt;p>The six charts below plot the trajectories of treatment outcomes for
each of the treatment-gender conditions. The charts on the left (blue)
are the trajectories for male participants, and those on the right (red)
are the trajectories for female participants. The three rows counting
from top to bottom correspond to the three treatment conditions,
respectively.&lt;/p>
&lt;p>Note that these trajectories are &lt;strong>expected days of heavy drinking&lt;/strong>.
Since we are modeling the outcomes as counts from the binomial
distribution, the “expectation of the counts” is the product of $N$ and
$P$, i.e., &lt;code>14 * logistic(-D)&lt;/code>, where &lt;code>D&lt;/code> is the latent quantity
computed from &lt;code>predict_obs()&lt;/code>. The means of these expected counts
computed from the full posterior are shown as the thick curves in the
charts below. The shaded regions cover 90% density of the expected
trajectory distributions. The grey lines plot the empirical observation
under that condition&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">source&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;func_post_predict.R&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>post &lt;span style="color:#f92672">=&lt;/span> stom&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">extract2&lt;/span>(m)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>mar &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">par&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;mar&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>mar[1] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>mar[3] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>mar[4] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>( mfrow &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>), mar &lt;span style="color:#f92672">=&lt;/span> mar )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>) &lt;span style="color:#75715e"># Treatment 3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#75715e"># Treatment 3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>) &lt;span style="color:#75715e"># Treatment 2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#75715e"># Treatment 2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>) &lt;span style="color:#75715e"># Treatment 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_model_prediction&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col_main&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#75715e"># Treatment 1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-17-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>From these charts, we can see that Treatment 3 works best for male
participants. For females, however, it is Treatment 2 that is the best.
These differences might not be apparent enough in the form of
trajectories. So next, let’s directly compare the treatments by
inspecting their contrasts.&lt;/p>
&lt;h3 id="treatment-contrasts">Treatment Contrasts&lt;/h3>
&lt;p>The code below computes the relevant contrasts for later uses. Recall
that all 90 participants are used repeatedly for each treatment-gender
condition. Since 3000 is indivisible by 90, I discarded 30 posterior
samples and used the first 2970 samples just to keep things simpler. A
more rigorous but quite computationally expensive alternative is to use
all 3000 instead of 33 (2970/90) posterior samples for computing
predictions for a single participant under each treatment-gender
condition.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>Sids &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rep&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>d&lt;span style="color:#f92672">$&lt;/span>dat&lt;span style="color:#f92672">$&lt;/span>Ns, length&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2970&lt;/span> ) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">sample&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>times &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">.01&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#75715e"># table(Sids)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>cf &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">list&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> T1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">list&lt;/span>( G1&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), G2&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), all&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>() ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> T2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">list&lt;/span>( G1&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), G2&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), all&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>() ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> T3 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">list&lt;/span>( G1&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), G2&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(), all&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>() )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( tx in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> ) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span> &lt;span style="color:#a6e22e">for &lt;/span>( g in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span> cf[[tx]][[g]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">2970&lt;/span>, &lt;span style="color:#a6e22e">function&lt;/span>(i) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span> D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">predict_obs&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>tx, Sid&lt;span style="color:#f92672">=&lt;/span>Sids[i], A&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">NULL&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>g, idx&lt;span style="color:#f92672">=&lt;/span>i)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> &lt;span style="color:#ae81ff">14&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span> cf[[tx]][[3]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">2970&lt;/span>, &lt;span style="color:#a6e22e">function&lt;/span>(i) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span> D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">predict_obs&lt;/span>(Tid&lt;span style="color:#f92672">=&lt;/span>tx, Sid&lt;span style="color:#f92672">=&lt;/span>Sids[i], A&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">NULL&lt;/span>, G&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">NULL&lt;/span>, idx&lt;span style="color:#f92672">=&lt;/span>i)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span> &lt;span style="color:#ae81ff">14&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>D)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the data prepared, we can now directly compare the treatments. For
illustrative purposes, let’s first ignore gender and see how it might
lead us to suboptimal decisions.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>T3T1_noG &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">treatment_contrast&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">3&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>T2T1_noG &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">treatment_contrast&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">3&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>p1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">plot_treatment_contrast_noG&lt;/span>(T3T1_noG, &lt;span style="color:#e6db74">&amp;#34;Treatment 3 - Treatment 1&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>p2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">plot_treatment_contrast_noG&lt;/span>(T2T1_noG, &lt;span style="color:#e6db74">&amp;#34;Treatment 2 - Treatment 1&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_grid&lt;/span>(p1, p2,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> align &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;h&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> axis &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;lb&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> ncol&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-19-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>The figure above compares Treatment 3 to Treatment 2, with Treatment 1
acting as the common baseline. The horizontal axis shows the difference
in the treatment outcomes between the target and baseline treatments. A
negative difference indicates that the target treatment results in fewer
expected days of heavy drinking and is hence better compared to the
baseline.&lt;/p>
&lt;p>Judging from the figure alone, one might conclude that &lt;strong>Treatment 3 is
better than Treatment 2&lt;/strong> and supports prescribing Treatment 3 for all
patients. However, since we simulated the data, we know this is not true
because Treatment 3 and 2 work differentially for males and females. So
let’s replot the figure but consider gender this time.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>T3T1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">treatment_contrast2&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>T2T1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">treatment_contrast2&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>p3 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">plot_treatment_contrast&lt;/span>(T3T1, &lt;span style="color:#e6db74">&amp;#34;Treatment 3 - Treatment 1&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>p4 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">plot_treatment_contrast&lt;/span>(T2T1, &lt;span style="color:#e6db74">&amp;#34;Treatment 2 - Treatment 1&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot_grid&lt;/span>(p3, p4,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> align &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;h&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> axis &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;lb&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> ncol&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part5_files/figure-commonmark/unnamed-chunk-20-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Now, it becomes apparent how the treatments work differentially for
males and females. Treatment 3 seemingly gives better results overall
because it works particularly well for males. However, it is much less
effective for females. Female patients will benefit much more if they
receive Treatment 2.&lt;/p>
&lt;h2 id="can-we-do-better">Can We Do Better?&lt;/h2>
&lt;p>Previously, we picture a rather “static” mechanism that underlies the
working of the treatments (replot in the DAG below). In particular, we
assume that self-efficacy mediates between the treatment and the
clinical outcomes. In other words, it is assumed that the treatments
unidirectionally affect self-efficacy and self-efficacy unidirectionally
affects the treatment outcomes. However, this assumption becomes a bit
oversimplified once we think harder about how people’s lives unfold
during treatments. The treatments may have raised participants’
self-efficacy in alcohol control, leading to improved clinical outcomes.
Yet the improvement in clinical outcomes very likely also raises
subsequent self-efficacy. So instead of a unidirectional influence from
treatment to efficacy to clinical outcomes, a bi-directional interaction
seems to be more plausible, with previous clinical outcomes and
self-efficacy acting as feedback to later statuses.&lt;/p>
&lt;figure>
&lt;img src="dag.svg" style="max-height:210px"
alt="(Simplistic) unidirectional causal assumptions" />
&lt;figcaption aria-hidden="true">(Simplistic) unidirectional causal
assumptions&lt;/figcaption>
&lt;/figure>
&lt;p>Such a scenario might resemble something sketched in the DAG below.
Here, each state of the efficacy ($E_t$) and the clinical outcomes
($D_t$) at time $t$ influences the states at $t+1$ directly. $E_0$ and
$D_0$ represent the initial states before receiving treatments. These
initial states may be influenced by several variables (e.g., age,
gender, ethnicity, etc.), as illustrated by the greyed nodes on the
left-most part of the graph. $T$ represents the treatment and is assumed
to influence only $E$ here for the sake of simplicity.&lt;/p>
&lt;figure>
&lt;img src="dag-feedback-dyn.svg" style="max-height:270px"
alt="Dynamic feedback among variables over time" />
&lt;figcaption aria-hidden="true">Dynamic feedback among variables over
time&lt;/figcaption>
&lt;/figure>
&lt;p>In addition to the variables we have modeled in our unidirectional
causal model, the DAG here introduces additional variables, $S_i$. These
variables indicate extraneous influences on the clinical outcomes at
time $t$. For instance, they might be the stress level of each
participant at time $t$. Stressful life events could worsen clinical
outcomes and often come and go without signs. When these random events
occur and lead to decreases in participants’ clinical outcomes, the
negative effect is likely to carry over to future states, such as
lowering participants’ self-efficacy subsequently. The arrows
$D_t \rightarrow E_{t+1}$ and $E_t \rightarrow D_{t+1}$ would allow one
to model such effects.&lt;/p>
&lt;p>The assumption of this dynamic feedback interaction leads to two
questions. First, such a fine-grained interaction across time requires
denser longitudinal data for the model to make sense. When the dataset
is sparse in terms of time, the link between the current and the next
state might be too weak as multiple events may have occurred in between
and obscure their relationship. Thus, experience sampling methods or
ecological momentary assessments, where data are collected daily, are
preferable for this model. Second, the model is only conceptual at best
so far. To prove that this dynamic feedback model can work, we would
need to run simulations, construct the model, and test it, as always.
This would probably take weeks to months for me. Once I’m done, I might
document the process in another post someday.&lt;/p>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-bowen2014" class="csl-entry">
&lt;p>Bowen, Sarah, Katie Witkiewitz, Seema L. Clifasefi, Joel Grow, Neharika
Chawla, Sharon H. Hsu, Haley A. Carroll, et al. 2014. “Relative Efficacy
of Mindfulness-Based Relapse Prevention, Standard Relapse Prevention,
and Treatment as Usual for Substance Use Disorders: A Randomized
Clinical Trial.” &lt;em>JAMA Psychiatry&lt;/em> 71 (5): 547–56.
&lt;a href="https://doi.org/10.1001/jamapsychiatry.2013.4546">https://doi.org/10.1001/jamapsychiatry.2013.4546&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-moniz-lewis2022" class="csl-entry">
&lt;p>Moniz-Lewis, David I. K., Elena R. Stein, Sarah Bowen, and Katie
Witkiewitz. 2022. “Self-Efficacy as a Potential Mechanism of Behavior
Change in Mindfulness-Based Relapse Prevention.” &lt;em>Mindfulness&lt;/em> 13 (9):
2175–85. &lt;a href="https://doi.org/10.1007/s12671-022-01946-z">https://doi.org/10.1007/s12671-022-01946-z&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The topic of the current post is quite advanced and assumes the
readers have either read the previous four posts or have a solid
understanding of IRT.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The fictitious study here is largely inspired by and modified from
Moniz-Lewis et al. (2022) and Bowen et al. (2014). The details
differ quite a lot, but the main ideas are similar.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>If you are interested in the truncated normal distribution, take a
look at the source code by typing &lt;code>stom::rtnorm&lt;/code>. You will see that
it simply draws samples from a normal distribution and drops out
those beyond the boundaries until the required sample size is
reached.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Note that these empirical observations have a larger variation
than the trajectory distribution under the same condition. This is
because the trajectories are &lt;em>expectations&lt;/em> $NP$. That is, they are
the means of—not samples from—the binomial distributions. Thus,
there is no associated sampling variation.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>stan</category><category>bayes</category><category>stats</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (4/4)</title><link>https://yongfu.name/2023/04/26/irt4/</link><pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/04/26/irt4/</guid><description>&lt;p>Rating scales require special treatments during data analyses. It is
dangerous to treat the choices in a rating scale as simple numerical
values. Nor is it satisfactory to treat them as discrete categories in
which the internal ordering is thrown away. A rating scale is
&lt;strong>ordinal&lt;/strong> in nature, meaning that there is an inherent order among the
choices within. This ordering is different from the ordering in
numerical values such as counts and heights. In such cases, the
differences between numerical values are directly comparable. For
instance, a count of 5 differs from a count of 3 by a count of 2, and so
is the difference between a count of 8 and 6. &lt;strong>Ordinal variables&lt;/strong> are
different. Take for example the subjective rating of happiness. It is
probably easier to move from a rating of 2 to 3 than from a rating of 4
to 5 on a five-point Likert scale, as many people prefer to reserve the
boundary ratings (1 and 5) for extreme cases. Ratings like this are
ubiquitous in the social sciences and particularly in psychology, where
rating scales are deployed to measure unobserved latent psychological
constructs.&lt;/p>
&lt;p>In this post, the final one in the &lt;em>demystifying IRT&lt;/em> series, I will
walk you through the statistical machinery that deals with the rating
scale. Things get a bit complicated in rating scales since the
dimensionality increases, and it is always more challenging to think in
higher dimensions. However, after peeling off the complexity introduced
by the high dimensions, the underlying concept is quite straightforward.
It is again a GLM, just with fancier machinery to map continuous latent
quantities to a vector of probabilities. So don’t be scared off by the
high dimensions. We just have to take one step at a time. Don’t worry if
you run out of working memory. Shift the burden of holding everything in
your brain to a piece of paper. Sketch what you need and proceed slowly.
You will finally get there.&lt;/p>
&lt;h2 id="wine-quality">Wine Quality&lt;/h2>
&lt;p>Before moving on to the details of the statistical machinery behind the
rating scale, let me first provide some context.&lt;/p>
&lt;p>The examples presented in previous posts are classical situations where
IRT is applied and known for—a testing context. In such contexts, there
are testees, test items, and possibly raters, but IRT is much more
general than that. It is well applicable beyond the testing situation.
Let us look at one such example, the &lt;em>rating of wine quality&lt;/em>.&lt;/p>
&lt;p>There are wines, fine wines, premium wines, and judges in a wine
competition. It is a simple twist of the item-testing scenario in which
IRT is often applied. Again, two factors co-determine the rating scores
of the wines here. First, it is the “inherent” property associated with
each wine, the &lt;em>wine quality&lt;/em>. High-quality wines should receive high
ratings for the ratings to make sense at all. The second factor is the
&lt;em>leniency&lt;/em> of a judge in giving out the scores. A lenient judge tends to
give higher ratings to the same wines as compared to stricter judges.
These assumptions are illustrated in the DAG below. Here, $W$ and $J$
represent the latent &lt;strong>wine quality&lt;/strong> and &lt;strong>judge leniency&lt;/strong>,
respectively. $R$ stands for the rating scores. If you will, you could
draw the analogy to the previous IRT context, where $W$ can be thought
of as corresponding to the person ability and $J$ to the item easiness.
The analogy isn’t exact though. It’s equally sensible to think in the
other direction. There’s nothing wrong to think of $W$ as corresponding
to item easiness and $J$ to person ability.&lt;/p>
&lt;img src="dag.svg" style="max-height:140px" />
&lt;p>The only thing new is that instead of a binary response, $R$ can take
more than two values. We need new machinery to map the aggregated
influence from the two factors ($W$ and $J$), which is a latent score in
the real space, to the outcome ordinal scale. Lower latent scores should
give rise to lower ratings, and higher latent scores to higher ratings,
in general. How is this achieved? Let’s dive into the intricacy of this
machinery.&lt;/p>
&lt;h2 id="from-latent-to-rating">From Latent to Rating&lt;/h2>
&lt;p>$$
L ~~ \rightarrow ~~ P_{cum.}
~~ \rightarrow ~~ \begin{bmatrix} P_1 \\ P_2 \\ P_3 \\ P_4 \end{bmatrix}
~~ \rightarrow ~~ R \sim \text{Categorical}( \begin{bmatrix} P_1 \\ P_2 \\ P_3 \\ P_4 \end{bmatrix} )
\tag{1}
$$&lt;/p>
&lt;p>The path along the mapping of the latent scores onto the rating-scale
(ordinal) space is sketched above. The leftmost term $L$ stands for the
latent score, which we have learned to deduce from the simulations in
previous posts. It is also the starting point of this machinery of
converting real-valued scores to ordinal ratings. Things get a bit
complicated in the intermediate steps on the path. Therefore, indulge me
with explaining the path in reverse. I will start with the rightmost
term, which, monstrous as it may seem, is probably the least challenging
concept to be grasped here.&lt;/p>
&lt;h3 id="random-category-generator">Random Category Generator&lt;/h3>
&lt;p>The seemingly monstrous term represents the generation of a rating score
($R$) from a &lt;strong>categorical distribution&lt;/strong>. A categorical distribution
takes &lt;strong>a vector of $k$ probabilities&lt;/strong> as parameters. Each probability
specifies the chance that a particular category (one of the $k$
categories) gets drawn. In essence, a categorical distribution is simply
a bar chart in disguise. Each bar specifies the probability that the
category is sampled. In the example here, I set the number of categories
to $k = 4$, hence the four probability terms $P_1,~P_2, P_3,~P_4$.&lt;/p>
&lt;p>The code below plots a categorical distribution (bar chart) with 4
categories. The first line of the code specifies the relative odds of
producing the 4 categories: Category 2 and 3 are three times more likely
to be drawn than Category 1 and 4. Since the probabilities of all
categories must sum to one in a distribution, the second line of code
normalizes this vector to the correct probability scale.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Category&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">.5&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i,i), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-1-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Now, to sample from this distribution,
$\text{Categorical}( \begin{bmatrix} .125 \\ .375 \\ .375 \\ .125 \end{bmatrix} )$,
we simply use the &lt;code>sample()&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Sample one category from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 3
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Repeatedly sample from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e5&lt;/span>, replace&lt;span style="color:#f92672">=&lt;/span>T, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( P2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">table&lt;/span>(s) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">length&lt;/span>(s) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>s
1 2 3 4
0.12484 0.37579 0.37439 0.12498
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Empirical frequency distibution obtained through sampling&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Category&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">.5&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i,i), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P2[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-3-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>After drawing a large sample from this distribution, we can see that the
frequency distribution of the samples approaches the original
distribution.&lt;/p>
&lt;p>Back to the wine rating scenario. The categories in this context are the
available rating scores. Since I adopted the example of four categories,
in the rating-scale context, it would correspond to a 4-point Likert
scale in which &lt;code>1&lt;/code>, &lt;code>2&lt;/code>, &lt;code>3&lt;/code>, and &lt;code>4&lt;/code> are the four categories. One
crucial part is missing though. The categorical distribution is
order-agnostic: it knows nothing about the order of the categories it
generates. What it does is faithfully produce categories according to
the given probabilities. So, where does the order come from? It’s from
the relationship between rating probabilities and the latent scores.&lt;/p>
&lt;h3 id="enforcing-order-to-categories">Enforcing Order to Categories&lt;/h3>
&lt;p>When a higher latent score tends to give rise to a higher rating, an
order is automatically enforced on the categorical ratings (&lt;code>1&lt;/code>, &lt;code>2&lt;/code>,
&lt;code>3&lt;/code>, and &lt;code>4&lt;/code>). But how is this done? Recall the analogous situation of
the binary regression in the previous posts. Back then, the link between
the responses (&lt;code>0&lt;/code>/&lt;code>1&lt;/code>) and the latent scores is established through the
&lt;strong>probability&lt;/strong>: a higher latent score results in a higher probability
of generating &lt;code>1&lt;/code>. Thus, in general, higher latent scores tend to
produce &lt;code>1&lt;/code>s. A similar strategy can be deployed here: we bridge the
responses and the latent scores through probabilities. The crucial
difference is that we now get multiple, instead of one, probabilities to
deal with. Statisticians came up with a clever solution to this. Instead
of dealing with a vector of fluctuating probabilities, which breaks the
desired monotonically increasing relationship between the probabilities
and the ratings, the probabilities are transformed into a vector of
&lt;strong>cumulative probabilities&lt;/strong>. A nice thing about this vector of
cumulative probabilities is that the probabilities are &lt;strong>ordered&lt;/strong>,
naturally. Larger cumulative probabilities now correspond to higher
rating scores. Sounds confusing? Let me re-describe these more vividly
with some code and plots. I’ll continue to use the four-point rating
example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P) ) &lt;span style="color:#75715e"># Probabilities for R = 1, 2, 3, 4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P) ) &lt;span style="color:#75715e"># Cumulative Probabilities for R = 1, 2, 3, 4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.500 0.875 1.000
&lt;/code>&lt;/pre>
&lt;p>The code above computes the cumulative probabilities (&lt;code>Pc&lt;/code>) from the
vector of rating probabilities (&lt;code>P&lt;/code>) through the function &lt;code>cumsum()&lt;/code>
(cumulative sum). Note that both vectors contain the same information.
The original vector can well be reconstructed from the cumulative
version. In math terms, their relationship is as follows:&lt;/p>
&lt;p>$$
\gdef\Pr{\textrm{Pr}}
\begin{aligned}
\Pr(R=1) = \Pr(R \leq 1)&amp;amp; \\
\Pr(R=2) = \Pr(R \leq 2)&amp;amp; - \Pr(R \leq 1) \\
\Pr(R=3) = \Pr(R \leq 3)&amp;amp; - \Pr(R \leq 2) \\
\Pr(R=4) = \Pr(R \leq 4)&amp;amp; - \Pr(R \leq 3) \\
= \phantom{PPaa} 1 \phantom{aaa}&amp;amp; - \Pr(R \leq 3)
\end{aligned}
\tag{2}
$$&lt;/p>
&lt;p>and in code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>Ps[2&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">5&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[1&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>] &lt;span style="color:#75715e"># or more generally, Ps[-1] - Ps[-length(Ps)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
&lt;/code>&lt;/pre>
&lt;p>The two vectors are visualized as distributions below. The red bars are
the probability distribution we have met in the previous section. The
blue bars plot the cumulative version of it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Rating&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Prob&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">.5&lt;/span>,&lt;span style="color:#ae81ff">4.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i&lt;span style="color:#ae81ff">-.05&lt;/span>,i&lt;span style="color:#ae81ff">-.05&lt;/span>), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,P[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( x&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(i&lt;span style="color:#ae81ff">+.05&lt;/span>,i&lt;span style="color:#ae81ff">+.05&lt;/span>), y&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,Pc[i]), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">10&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-6-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>Once we have an ordered sequence of probabilities, or more precisely,
probabilities with a monotonically increasing relationship to the rating
scores, we’ll be able to introduce latent scores through the &lt;strong>logit
link&lt;/strong>, as we have done in the binary case. We simply pass the
cumulative probabilities to the logit function to map them onto the real
space. To save space, I pack some commonly used functions into my
package &lt;a href="https://yongfu.name/stom/reference">&lt;code>stom&lt;/code>&lt;/a>, which can be
installed through the first two lines of commented code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># install.packages(&amp;#34;remotes&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># remotes::install_github(&amp;#34;liao961120/stom&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(stom)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">logit&lt;/span>(Pc) &lt;span style="color:#75715e"># convert cumulative probabilities to reals&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
&lt;/code>&lt;/pre>
&lt;p>The statistical machinery behind rating scales likely remains elusive
after my wordy explanation. Indeed, since we are only halfway through
the machinery, it would hardly make any sense just by looking at part of
the picture. What I have presented so far is the portion of the
machinery that monotonically aligns the latent scores with the ratings,
through the use of cumulative probabilities. The second part of the
machinery is to allow for the shifting of the entire vector of latent
scores (and thus the probabilities of ratings, through the first part of
the machinery) by a common term, which enables the modeling of
extraneous influences on the ratings (thus the “regression”). Let’s now
look at how this shifting is achieved.&lt;/p>
&lt;h3 id="shifting-latent-scores">Shifting Latent Scores&lt;/h3>
&lt;p>The code below summarizes the first part of the rating-scale machinery:
establishing the link between latent scores and the probabilities of
rating scores.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>P &lt;span style="color:#75715e"># vector of rating probs (starting point)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P) ) &lt;span style="color:#75715e"># vector of rating probs (cumulative)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( L &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logit&lt;/span>(Pc) ) &lt;span style="color:#75715e"># vector of latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.125 0.375 0.375 0.125
[1] 0.125 0.500 0.875 1.000
[1] -1.94591 0.00000 1.94591 Inf
&lt;/code>&lt;/pre>
&lt;p>Since all of the above mappings are one-to-one, we can as well express
the same machinery in reverse:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>L &lt;span style="color:#75715e"># vector of latent scores (starting point)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>( Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L) ) &lt;span style="color:#75715e"># vector of rating probs (cumulative)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>( P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)] ) &lt;span style="color:#75715e"># vector of rating probs &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P ) &lt;span style="color:#75715e"># draw one rating score from the distribution&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
[1] 0.125 0.500 0.875 1.000
[1] 0.125 0.375 0.375 0.125
[1] 2
&lt;/code>&lt;/pre>
&lt;p>This second expression aligns well with the simulation perspective and
precisely lays out the data-generating process of the rating scores. It
also makes it clear that a &lt;em>predetermined&lt;/em> set of latent scores (or
probabilities of ratings) is required for generating the ratings. In a
simulation, these latent scores are determined by us. For a model, they
are a subset of parameters that the model tries to estimate from data.
These latent scores can be thought of as &lt;strong>baselines&lt;/strong> during rating.
That is, the latent scores, or more visually, the shape of the rating
distribution &lt;strong>before any factor has exerted an effect on the ratings&lt;/strong>.&lt;/p>
&lt;p>To model the extraneous influences on the ratings, we utilize an
independent term $\phi$ in the latent score space. The trick is to
&lt;strong>subtract&lt;/strong> this $\phi$ from the vector of the &lt;em>baseline latent
scores&lt;/em>. For instance, if a wine has a better-than-average quality that
raises its quality (latent score) by $1.9$ above the baseline but is
rated by a harsh judge that lowers the quantity by $1.1$, $\phi$ will be
$.8$. Subtracting $\phi=.8$ from the baseline latent scores gives the
shifted latent scores, from which the rating probabilities could then be
derived:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>latent_to_prob &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(L) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#a6e22e">return&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>phi &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.9&lt;/span> &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1.1&lt;/span> &lt;span style="color:#75715e"># wine (1.9) &amp;amp; judge (-1.1) influence on ratings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>L &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>( Ls &lt;span style="color:#f92672">=&lt;/span> L &lt;span style="color:#f92672">-&lt;/span> phi ) &lt;span style="color:#75715e"># latent scores after influences of wine &amp;amp; judge&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#a6e22e">latent_to_prob&lt;/span>(Ls) &lt;span style="color:#75715e"># rating probs after influences of wine &amp;amp; judge&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591 Inf
[1] -2.74591 -0.80000 1.14591 Inf
[1] 0.06031805 0.24970747 0.44873758 0.24123690
&lt;/code>&lt;/pre>
&lt;p>The bar chart below overlays the rating score distribution after
considering $\phi$ (blue bars) on the baseline distribution (red bars).
It can be seen that subtracting $\phi=.8$ from the baseline latent
scores pushes the probability mass toward the right, raising the
expected rating score.&lt;/p>
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>It might seem unintuitive that &lt;em>subtracting&lt;/em> a positive value from the
latent scores &lt;em>raises&lt;/em> the expected rating scores. But it’s simply the
effect of the cumulative probabilities. When the vector of the latent
scores gets shifted, note that the last term doesn’t move since it is
infinity ($logit(1) = \infty$). Thus, the difference between the last
and the second-to-last term, on the cumulative probability scale,
becomes larger after the shift. This difference is essentially the
probability of the largest rating ($P_4$ in our example). Therefore, the
effect of subtracting a positive value from the baseline latent scores
shifts the probability mass toward the larger ratings. For the remaining
ratings, the directions of changes in probability depend on the amount
of shift and the shape of the baseline distribution. It is thus hard to
conceive how these probability bars react to the shift in the latent
scores and how their shifts contribute to the increasing or decreasing
of the expected rating.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="ordlogit.png" alt="Interactive visualization of the rating probability
distribution">
&lt;figcaption>Interactive visualization of the rating probability
distribution&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>To disentangle these intertwined influences on the final distribution,
I’ve built an &lt;a href="https://yongfu.name/ordlogit">interactive
visualization&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> to help. As shown in
the figure above, there are two places where users can tweak to see how
the shape of the rating distribution gets influenced.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The four vertical sliders are there to adjust the baseline
probabilities of the ratings, $Pr(R=1)$, $Pr(R=2)$, $Pr(R=3)$, and
$Pr(R=4)$ (abbreviated as $P_1$ ~ $P_4$ respectively). The numerical
value on top of each bar indicates the probability of that rating.
Note that it is the relative positions between the vertical sliders
that matter, and the four probabilities automatically adjust to
always sum to one.&lt;/p>
&lt;p>The three values, $\kappa_1$, $\kappa_2$, and $\kappa_3$, shown on
top of the four probabilities are the &lt;strong>cumulative logits&lt;/strong>, which
are basically the vector of the cumulative probabilities,
transformed to the logit scale. They are the &lt;strong>baseline latent
scores&lt;/strong> mentioned previously. The last term, $\kappa_4$ is dropped
since it is always infinite.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The horizontal slider above the vertical sliders controls the value
of $\phi$, which gets subtracted from each of the baseline latent
scores to derive the final distribution.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="wheres-the-regression">Where’s the Regression?&lt;/h2>
&lt;p>The previous section demonstrates how the baseline rating distribution
shifts according to an aggregated influence of $\phi$, which is the hard
part of the statistical machinery behind the rating scale IRT model.
Regression is the easy part. Now we have a nice and neat $\phi$ sitting
on the real space&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> to work with. If we zoom in on $\phi$, it’s simply
the summed effect of the predictor variables in a linear regression,
which is similar to $\mu$ in logistic regressions. The only difference
here is that we need a different linking distribution to map the effect
onto the response scale (i.e., discrete ratings). In math terms,
resuming our wine rating example, the distribution is shown in
&lt;a href="#math-eq3">(3)&lt;/a>:&lt;/p>
&lt;p>$$
\begin{aligned}
R_i &amp;amp; \sim \text{OrderedLogit}(\phi_i, ~ \bm{\kappa} = \begin{bmatrix} \kappa_1 \\ \kappa_2 \\ \kappa_3 \end{bmatrix} ) \\
\phi_i &amp;amp; = W_{Wid[i]} + J_{Jid[i]} \\
\tag{3}
\end{aligned}
$$&lt;/p>
&lt;p>The $\text{OrderedLogit}$ expression hides all the details from the
reader. But you’ve already seen the details at work in code form in
previous sections, albeit in a quite scattered manner. Later, I will
collect them into a single function. If you prefer clarity now, the
monstrous expressions in &lt;a href="#math-eq4">(4)&lt;/a> should suffice.&lt;/p>
&lt;p>$$
\newcommand{\logit}{\textrm{logit}}
\begin{aligned}
R_i \sim \text{Categorical} &amp;amp; (
\begin{bmatrix}
\Pr(R_i = 1) = \Pr(R_i \le 1) \phantom{- \Pr(R_i \le 1)} \\
\Pr(R_i = 2) = \Pr(R_i \le 2) - \Pr(R_i \le 1) \\
\Pr(R_i = 3) = \Pr(R_i \le 3) - \Pr(R_i \le 2) \\
\Pr(R_i = 4) = \Pr(R_i \le 4) - \Pr(R_i \le 3) \\
\end{bmatrix}
) \\
\logit[ \Pr(R_i \le 1) ] &amp;amp;= \logit[ Pr(R_i = 1) ] = \kappa_1 - \phi_i \\
\logit[ \Pr(R_i \le 2) ] &amp;amp;= \kappa_2 - \phi_i \\
\logit[ \Pr(R_i \le 3) ] &amp;amp;= \kappa_3 - \phi_i \\
\logit[ \Pr(R_i \le 4) ] &amp;amp;= \logit(1) = \infty \\
\phi_i &amp;amp;= W_{Wid[i]} + J_{Jid[i]}
\tag{4}
\end{aligned}
$$&lt;/p>
&lt;p>Don’t worry if you cannot understand the equations in &lt;a href="#math-eq4">(4)&lt;/a>
right now. After you get accustomed to the logic of the ordered logit,
through coding, the expressions become straightforward. So now let’s
wrap up what we have done so far, in code. I will write down the code
form of the $OrderedLogit$ distribution in the function &lt;code>rOrdLogit()&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>rOrdLogit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(phi, kappa) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span> kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( kappa, &lt;span style="color:#66d9ef">Inf&lt;/span> ) &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> L &lt;span style="color:#f92672">=&lt;/span> kappa &lt;span style="color:#f92672">-&lt;/span> phi &lt;span style="color:#75715e"># latent scores, after shifting&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(L) &lt;span style="color:#75715e"># map latent scores to cumulative probs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span> &lt;span style="color:#75715e"># Compute probs for each rating from Pc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span> Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, Pc )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> P &lt;span style="color:#f92672">=&lt;/span> Ps[&lt;span style="color:#ae81ff">-1&lt;/span>] &lt;span style="color:#f92672">-&lt;/span> Ps[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Ps)] &lt;span style="color:#75715e"># probs of each rating&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> &lt;span style="color:#a6e22e">sample&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(P), size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>P )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e">## Replicate previous example ##&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>P &lt;span style="color:#f92672">=&lt;/span> P &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">sum&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Pc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">cumsum&lt;/span>(P)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>( kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logit&lt;/span>( Pc )[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(Pc)] ) &lt;span style="color:#75715e"># Set up baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#75715e"># 10,000 draws from OrdLogit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>draws &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( &lt;span style="color:#ae81ff">1e4&lt;/span>, &lt;span style="color:#a6e22e">rOrdLogit&lt;/span>(phi&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, kappa&lt;span style="color:#f92672">=&lt;/span>kappa) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e"># should approach P = c(.125, .375, .375, .125)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#a6e22e">table&lt;/span>(draws) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#a6e22e">length&lt;/span>(draws)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] -1.94591 0.00000 1.94591
draws
1 2 3 4
0.1282 0.3788 0.3678 0.1252
&lt;/code>&lt;/pre>
&lt;h2 id="simulating-and-fitting-wine-ratings">Simulating and Fitting Wine Ratings&lt;/h2>
&lt;p>Having all concepts in place, let’s start synthesizing data for our
later model fitting. We will simulate data from the Ordered Logit
distribution. One minor limitation with &lt;code>rOrdLogit()&lt;/code> defined previously
is that it can only take a single value &lt;code>phi&lt;/code>, but it is more desirable
for &lt;code>phi&lt;/code> to be a vector of values. A vectorized version of
&lt;code>rOrdLogit()&lt;/code> is available in the &lt;code>stom&lt;/code> package as &lt;code>rordlogit()&lt;/code>. We
will be using this function for our data simulation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(stom)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">1025&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>Nj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">12&lt;/span> &lt;span style="color:#75715e"># number of judges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>Nw &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of wines&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>J &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>(Nj) &lt;span style="color:#75715e"># judge leniency&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>W &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>(Nw) &lt;span style="color:#75715e"># wine quality&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>J &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">standardize&lt;/span>(J) &lt;span style="color:#75715e"># scale to mean = 0, sd = 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>W &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">standardize&lt;/span>(W) &lt;span style="color:#75715e"># scale to mean = 0, sd = 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>kappa &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">-1.7&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1.7&lt;/span> ) &lt;span style="color:#75715e"># baseline latent scores&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># Create long-form data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Jid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Nj, Wid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>Nw, KEEP.OUT.ATTRS&lt;span style="color:#f92672">=&lt;/span>F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>J &lt;span style="color:#f92672">=&lt;/span> J[d&lt;span style="color:#f92672">$&lt;/span>Jid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>W &lt;span style="color:#f92672">=&lt;/span> W[d&lt;span style="color:#f92672">$&lt;/span>Wid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>phi &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sapply&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#a6e22e">nrow&lt;/span>(d), &lt;span style="color:#a6e22e">function&lt;/span>(i) d&lt;span style="color:#f92672">$&lt;/span>J[i] &lt;span style="color:#f92672">+&lt;/span> d&lt;span style="color:#f92672">$&lt;/span>W[i] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rordlogit&lt;/span>( d&lt;span style="color:#f92672">$&lt;/span>phi, kappa ) &lt;span style="color:#75715e"># simulated rating responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>B &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>phi) ) &lt;span style="color:#75715e"># simulated binary responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>C &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( &lt;span style="color:#a6e22e">nrow&lt;/span>(d), d&lt;span style="color:#f92672">$&lt;/span>phi ) &lt;span style="color:#75715e"># simulated continuous responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span>&lt;span style="color:#75715e"># Conversion of data types to match model-fitting function&amp;#39;s requirements&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>( v in &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Jid&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Wid&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span> d[[v]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d[[v]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ordered&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>R)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 360 obs. of 8 variables:
$ Jid: Factor w/ 12 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
$ Wid: Factor w/ 30 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
$ J : num 0.0187 -0.1794 -1.4372 1.3915 -0.1134 ...
$ W : num 0.236 0.236 0.236 0.236 0.236 ...
$ phi: num 0.2544 0.0564 -1.2014 1.6273 0.1223 ...
$ R : Ord.factor w/ 4 levels &amp;quot;1&amp;quot;&amp;lt;&amp;quot;2&amp;quot;&amp;lt;&amp;quot;3&amp;quot;&amp;lt;&amp;quot;4&amp;quot;: 4 2 2 2 1 1 1 4 2 3 ...
$ B : int 1 0 0 1 1 1 1 1 0 0 ...
$ C : num 0.6142 -1.5915 -0.2754 1.2755 0.0792 ...
&lt;/code>&lt;/pre>
&lt;p>Running the above code will get our data prepared. Two things might be
worth noting. The first is the &lt;code>standardize()&lt;/code> function, which centers
the input vector to zero mean and a standard deviation of one. &lt;code>J&lt;/code> and
&lt;code>W&lt;/code> are centered here to make the parameters later estimated by the
model comparable to the scale of the true values. In our later model, we
will partial-pool both the judges and the wines and hence assume a
zero-meaned distribution for both of them. Since the sample size of our
data isn’t large (12 judges and 30 wines), which will likely cause the
means of the raw &lt;code>J&lt;/code> and &lt;code>W&lt;/code> to have non-minor deviations from zero,
standardization is needed.&lt;/p>
&lt;p>Second, in addition to &lt;code>R&lt;/code>, the rating responses, I also simulate binary
responses &lt;code>B&lt;/code> (&lt;code>0&lt;/code>/&lt;code>1&lt;/code>) from &lt;code>phi&lt;/code>. Indeed, if a model is fitted with
&lt;code>B&lt;/code> as the dependent variable, it will be identical to the logistic
regression models fitted in previous posts. The binary responses are
simulated to demonstrate the parallels between the binary&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> model and
the rating scale model. The two models are highly similar: the linear
effects are aggregated in the same ways (in $\mu$/$\phi$). The only
difference is how these effects are projected onto the response scale:
the binary model does so through the Bernoulli distribution, and the
rating scale model through the Ordered Logit distribution.&lt;/p>
&lt;p>Another reason for simulating binary responses along the rating
responses is for the preparation of model debugging. As we start fitting
more and more complex models, we are bound to find ourselves lost in
situations where we have no idea why the model fails to give the
expected results. In such cases, it helps a lot to check the results
from simpler models, which might hint at where the complex model went
wrong. This is also the reason why I simulate the continuous responses
&lt;code>C&lt;/code>—to prepare data for fitting an even simpler model. By eliminating
the influences arising from nonlinear links in the GLMs, the normal
response model becomes more transparent and hence much easier to debug.&lt;/p>
&lt;p>For our wine rating example here, I’ve deliberately made the
data-generating process simple enough that our rating scale model can
smoothly fit and give us the expected results. To fit ordered logit
regressions with partial pooling structures, we need the &lt;code>clmm()&lt;/code>
function from the package
&lt;a href="https://cran.r-project.org/web/packages/ordinal">&lt;code>ordinal&lt;/code>&lt;/a>. The model
syntax in &lt;code>clmm()&lt;/code> is basically identical to the syntax we used in
&lt;code>lme4::glmer()&lt;/code> back then. As shown in the code below, we model the
rating scores (&lt;code>R&lt;/code>) to be influenced by both the wines and the judges.
By partial pooling wines and judges, the wine effects and the judge
effects are respectively assumed to come from a zero-meaned normal
distribution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(ordinal)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">clmm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Jid) &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Wid), data &lt;span style="color:#f92672">=&lt;/span> d )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">summary&lt;/span>(m)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Cumulative Link Mixed Model fitted with the Laplace approximation
formula: R ~ (1 | Jid) + (1 | Wid)
data: d
link threshold nobs logLik AIC niter max.grad cond.H
logit flexible 360 -455.07 920.14 182(726) 6.96e-06 4.9e+01
Random effects:
Groups Name Variance Std.Dev.
Wid (Intercept) 1.220 1.1046
Jid (Intercept) 0.853 0.9236
Number of groups: Wid 30, Jid 12
No Coefficients
Threshold coefficients:
Estimate Std. Error z value
1|2 -1.42044 0.36491 -3.893
2|3 0.05442 0.35608 0.153
3|4 1.56532 0.36674 4.268
&lt;/code>&lt;/pre>
&lt;p>&lt;code>summary(m)&lt;/code> prints out the model summary along with the estimated
baseline latent scores, which are labeled as &lt;code>Threshold coefficients&lt;/code>
above. You can see that these coefficients (-1.42, 0.054, and 1.565)
align pretty well with the &lt;code>kappa&lt;/code> set in the simulation (-1.7, 0, and
1.7).&lt;/p>
&lt;p>To examine the estimated wine and judge effects, we similarly utilize
the &lt;code>ranef()&lt;/code> function as demonstrated in the previous post:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>est_wine &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m)&lt;span style="color:#f92672">$&lt;/span>Wid[[1]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>est_judge &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m)&lt;span style="color:#f92672">$&lt;/span>Jid[[1]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( est_wine, W ); &lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( est_judge, J ); &lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="two-column">
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-15-1.svg"
data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part4_files/figure-commonmark/unnamed-chunk-15-2.svg"
data-fig-align="center" />&lt;/p>
&lt;/div>
&lt;h2 id="item-response-theory-and-beyond">Item Response Theory and Beyond&lt;/h2>
&lt;p>We have come a long way, from the simplest binary item response model to
models with delicate machinery such as the rating scale model with
partial-pooling structures. The posts in this &lt;em>demystifying&lt;/em> series are
sufficient, I suppose, in providing a solid understanding of and
practical skills for working with item response theory. There are
certainly even more complex IRT models, but I won’t go further in that
direction. No matter how many new and complex models are added to the
toolkit, we are certain to find our tools in shortage when facing
real-world problems.&lt;/p>
&lt;p>Item response models, general as they might seem, quickly run out of
supply. Although binary and rating scale models allow us to deal with
most response types found in the field (such as tests in educational
settings, scales measuring psychological constructs, and various surveys
used in the social sciences), even the slightest complication renders
these models useless. Just consider a mixed-format test consisting of,
for example, multiple-choice items (binary scored) and items of
open-ended questions (rated). Which IRT model can we apply to this
mixed-format test? Not a single one. Instead, we need two separate
models, each independently running on a subset of the test for a
particular item format. A special technique is then required to map the
independently estimated person/item parameters onto a common scale.&lt;/p>
&lt;p>The method works, but it wastes a lot of information. When separately
estimated, information cannot be shared across different item formats to
improve parameter estimation. Item estimates might be fine, as long as
there are many subjects. Person estimates suffer greatly though since,
in practice, the test length is limited and is now further divided up by
two independent models. This is equivalent to estimating person
parameters with fewer items.&lt;/p>
&lt;p>It is always better to incorporate &lt;em>everything&lt;/em> into a &lt;em>&lt;strong>single
comprehensive model&lt;/strong>&lt;/em> instead of separately modeling a subset of
variables in multiple small models. It is better because information
flows smoothly through the variables in a comprehensive model, but the
flow breaks down when the model gets torn apart into several pieces.
However, such comprehensive models are rarely, if not never, available
in the literature. We have to tailor a model ourselves according to what
the current situation demands. Therefore, a &lt;em>&lt;strong>framework&lt;/strong>&lt;/em> is required
to guide us through building up such a model.&lt;/p>
&lt;p>This post marks the end of the &lt;em>demystifying&lt;/em> series. When the thick
cloud of mystery begins to dissolve, we finally get to start solving
real and exciting problems rather than wrangling with mad statistical
models. In my next post, I will move on to Bayesian statistics, a
&lt;em>&lt;strong>unified framework&lt;/strong>&lt;/em> that allows flexibly extending a model to match
the demanded conditions. Bayesian framework is ideal for empirical
research because it is &lt;em>practical&lt;/em>. We do not need to wait for a
statistician to come up with a model for every new situation. In
Bayesian inference, we simply describe the &lt;em>data-generating process&lt;/em> and
the &lt;em>priors&lt;/em>, and the rest is handled by probability theory and an
estimation algorithm. Therefore, we can focus on the scientific problems
at hand instead of fussing around with fancy models and their names. We
will see how item response models can be embedded into a larger network
of causes and effects that represents the assumed interactions
underlying the current problem. Item response models, which are
essentially &lt;strong>methods for handling measurement errors&lt;/strong>, help deal with
the latent constructs measured indirectly through surveys in this
network of interacting variables.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The source code for building the interactive visualization of the
ordered logit distribution can be found on
&lt;a href="https://github.com/liao961120/ordlogit">GitHub&lt;/a>. It is built upon
&lt;a href="https://github.com/probstats/probstats.github.io">this&lt;/a> nice
project for visualizing various probability distributions.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Recall that $\phi$ works in the latent score space by increasing
or decreasing the baseline latent scores.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>In the testing context, a binary dependent variable is often used
for modeling correct/incorrect responses. In the current wine rating
context, a binary dependent variable could also be used for modeling
ratings. In such cases, there must only be two possible ratings,
such as mediocre/premium, on the wines.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>stats</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (3/4)</title><link>https://yongfu.name/2023/03/29/irt3/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/03/29/irt3/</guid><description>&lt;h2 id="fixed-random-and-mixed">Fixed, Random and Mixed&lt;/h2>
&lt;p>Statistics is confusing enough through its massive terminology.
Psychology, which is largely experiment-oriented, further confuses
people by adding in its own flavor. A peek at the definitions of
&lt;a href="https://dictionary.apa.org/fixed-effect">fixed&lt;/a>,
&lt;a href="https://dictionary.apa.org/random-effect">random&lt;/a>, and
&lt;a href="https://dictionary.apa.org/mixed-effects-model">mixed&lt;/a> effects in the
&lt;a href="https://dictionary.apa.org">APA Dictionary of Psychology&lt;/a> exemplifies
this:&lt;/p>
&lt;blockquote>
&lt;p>[A mixed-effect model is] any statistical procedure or experimental
design that uses one or more independent variables whose levels are
&lt;strong>specifically selected by the researcher&lt;/strong> (fixed effects; e.g.,
gender) and one or more additional independent variables whose levels
are &lt;strong>chosen randomly&lt;/strong> from a wide range of possible values (random
effects; e.g., age).&lt;br>
&lt;b>&lt;/b>&lt;br>
—Definition of “mixed-effect model” in the APA Dictionary of
Psychology&lt;/p>
&lt;/blockquote>
&lt;p>The definitions for random and fixed effects above are not only
confusing but also misleading. In principle, whether a categorical
variable is “fixed” or “random” has nothing to do with the nature of the
variable (e.g., &lt;em>gender&lt;/em> doesn’t have to be fixed) or how the levels
within a variable are selected (randomly drawn or chosen by
researchers). Whether a variable is modeled as fixed or random is a
decision to be made by the modeler. And the modeler &lt;strong>should
model variables as random&lt;/strong> if there are no justifiable prohibitive
reasons. Let me explain.&lt;/p>
&lt;h2 id="multilevel-instead-of-mixed">Multilevel Instead of Mixed&lt;/h2>
&lt;p>A better way to understand fixed and random effects is to think
&lt;strong>hierarchically&lt;/strong>. The levels of a random-effect variable are treated
as &lt;strong>related&lt;/strong> by the model, meaning that the effect of each level is
estimated by also considering information from other levels in the
variable. This is known as &lt;strong>partial pooling&lt;/strong>, and it has several
desirable properties. On the other hand, the levels within a
fixed-effect variable are treated as independent: during parameter
estimation, the model considers only information within each level. This
is the &lt;strong>no-pooling&lt;/strong> case. So how does the model incorporate
information from the other levels during estimation in the
partial-pooling case? To explain this, let me start with the no-pooling
case.&lt;/p>
&lt;p>As an example, suppose we have a categorical variable with $n$ levels.
Our goal is to obtain an estimate for each of these levels (and the
variability in the estimates), labeled as
$\alpha_1, \alpha_2, &amp;hellip;, \alpha_n$. To provide some context, we can
think of the categorical variable here as &lt;em>nationality&lt;/em>, and for each
nation (a level), we want to estimate the average height (the parameter)
of its citizens. When we are &lt;strong>not pooling information across the
levels&lt;/strong>, the structure of the data-generating process assumed by the
statistical model is shown in the figure below. Here, the model assumes
that there is a parameter associated with each level that generates the
observations. However, the parameters here are assumed to be
independent. Therefore, the model utilizes only the observations under
each level to estimate its parameter. What has been learned about a
nation is uninformative about another nation for the &lt;strong>no-pooling&lt;/strong>
model.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="tree0.svg" alt="Levels within a categorical variable estimated
independently.">
&lt;figcaption>Levels within a categorical variable estimated
independently.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>But there actually is some information, right? Take the perspective of
an alien, for instance. Knowing the average height of the Americans, say
5′9″, provides quite much information about the heights of the
Japanese—they are unlikely to be 60 feet or 6 inches, agree? This is why
we prefer to partial pool. Partial pooling allows the sharing of
information across different levels, which, as elucidated below, leads
to several desirable properties.&lt;/p>
&lt;p>When we want to incorporate—or partial pool—information from other
levels during estimation, we can utilize models that assume a
hierarchical structure on the levels within a variable. Such a
hierarchical structure is exemplified in the figure below. This
hierarchical structure assumes that all levels, or more precisely all
parameters underlying the levels, come from a common distribution. Here,
this common distribution is assumed to be a normal distribution with
mean $\mu$ and variance $\sigma^2$. The mean and the variance are to be
estimated from the data. When this structure is imposed, the
observations under different levels will be naturally linked since now,
the observations under every level all provide information for
estimating the common distribution.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="tree1.svg" alt="Levels within a categorical variable estimated by incorporating
information from all levels. This is achieved through assuming all
levels to come from a common distribution.">
&lt;figcaption>Levels within a categorical variable estimated by incorporating
information from all levels. This is achieved through assuming all
levels to come from a common distribution.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>What can be gained from partial pooling information across levels? As
mentioned above, when we partial-pool, the information of the
observations across all levels is shared. This means that the model
considers &lt;strong>more information&lt;/strong> for estimating the parameter of each
level. As a direct result of this, the model uses the data
(observations) more efficiently by squeezing out more information.
Secondly, it reduces overfitting and thus provides better
(out-of-sample) estimation. The model is less likely to overfit because
it is more “objective” by considering information across different
levels. Overfitting occurs when data are scarce, which is the case in
the no-pooling case since only data within each level are considered. In
such cases, the model bases the estimations on fewer observations and
hence tends to be overly sensitive to idiosyncratic patterns in the
local data. Another great thing about partial pooling is that it
automatically adjusts according to the sample sizes under each level.
For levels with fewer observations (e.g, North Korea, in which the alien
managed to collect only heights from three of its citizens), the model
places more weight on the overall information provided by other levels,
resulting in larger adjustments of the levels’ estimates. For levels
with abundant data, their estimates are only slightly affected by the
observations from other levels.&lt;/p>
&lt;p>Partial pooling essentially arises from the hierarchical structure
assumed in the models. Therefore, these models are known as
&lt;strong>hierarchical&lt;/strong> or &lt;strong>multilevel&lt;/strong> models. &lt;strong>Mixed (effect)&lt;/strong> models are
another common label for these models, though, as explained above, the
name is quite uninformative. To understand how these models work, it is
better to start with their hierarchical structuring. I will use the term
&lt;strong>multilevel models&lt;/strong> from now on to save ourselves from confusion. This
name is also nice in that it coincides in abbreviation with the mixed
model—both of them are abbreviated as GLMM for Generalized Linear
Multilevel/Mixed Models.&lt;/p>
&lt;h2 id="back-to-irt">Back to IRT&lt;/h2>
&lt;p>Now, we are acquainted with the concept of partial pooling and
multilevel models, let’s apply them to the IRT context to improve our
previous model, which is fitted without partial pooling across levels.
To warm up, let me rephrase the structure of the simulated IRT dataset
in terms of the multilevel terminologies.&lt;/p>
&lt;p>There are two variables at work here—the item variable and the person
(or subject) variable. Within the item variable, there are several
items. In other words, each item acts as a level within the item
variable. Similarly, each person corresponds to a level within the
person variable. For each item, we want to estimate a parameter, the
item’s difficulty. Likewise, for each person, we also want to estimate a
parameter, the person’s ability. To improve our model in estimating the
item/person parameters, we can partial pool information across the
levels &lt;strong>within&lt;/strong> the item and/or the person variable.&lt;/p>
&lt;p>The chunk below copies the data simulation code from the previous post,
with two minor changes. The first is the renaming of the variables for
the item and subject ID as &lt;code>Iid&lt;/code> (originally &lt;code>I&lt;/code>) and &lt;code>Sid&lt;/code> (originally
&lt;code>S&lt;/code>). The second is that, instead of item difficulty (&lt;code>D&lt;/code> in the
previous post), we conceptualize the effect of items as &lt;strong>easiness&lt;/strong>
(&lt;code>E&lt;/code>) here. Item easiness is simply the negative of item difficulty.
This simple switch would allow us to skip the step of reversing the item
effects’ signs returned by the regression model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>logit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p ) &lt;span style="color:#a6e22e">log&lt;/span>( p&lt;span style="color:#f92672">/&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">-&lt;/span>p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) ) &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">12&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>n_item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">60&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>n_resp &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">*&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>n_param &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">+&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( n_subj ) &lt;span style="color:#75715e"># Person ability&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>E &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1.6&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, length&lt;span style="color:#f92672">=&lt;/span>n_item ) &lt;span style="color:#75715e"># Item easiness&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># The data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( Sid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj, Iid&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_item, KEEP.OUT.ATTRS &lt;span style="color:#f92672">=&lt;/span> F )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>mu &lt;span style="color:#f92672">=&lt;/span> A[d&lt;span style="color:#f92672">$&lt;/span>Sid] &lt;span style="color:#f92672">+&lt;/span> E[d&lt;span style="color:#f92672">$&lt;/span>Iid]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>( d&lt;span style="color:#f92672">$&lt;/span>mu ) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>Sid &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>Sid)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>Iid &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>Iid)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="unpooled-model">Unpooled Model&lt;/h3>
&lt;p>With the data prepared, let’s refit model
&lt;a href="https://yongfu.name/2023/03/06/irt2/#coding-models-the-easy-route">&lt;code>m1.2&lt;/code>&lt;/a> from the
previous post. Later, I will fit another model that partial pools the
subject variable (&lt;code>m2&lt;/code>) and compare it to the unpooled model here
(&lt;code>m1&lt;/code>).&lt;/p>
&lt;p>The code below for fitting &lt;code>m1&lt;/code> is identical to those in the previous
post, except that I adopt another method (starting from line 5) to
reconstruct the dropped estimate (forced by the sum-to-zero constraint).
This change is necessary, as it also allows us to reconstruct the
standard errors of the dropped estimate. We will need the standard
errors later to quantify the &lt;em>uncertainty&lt;/em> in the estimates, which are
used for comparing the unpooled and partial-pooled models. In addition,
the method adopted here is more principled and general, which further
consolidates our understanding of contrasts and dummy coding. However,
it takes up some space for the explanation since a little matrix algebra
is involved. I thus leave the details in the &lt;a href="#matrix-algebra">box&lt;/a> at
the end of the post.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>d1 &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>(d1&lt;span style="color:#f92672">$&lt;/span>Sid) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> Sid, data&lt;span style="color:#f92672">=&lt;/span>d1, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> (Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1))[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct std. error of the estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>(&lt;span style="color:#a6e22e">diag&lt;/span>(Vmat))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="partial-pooled-model">Partial-pooled Model&lt;/h3>
&lt;p>To fit the partial-pooled model, &lt;code>glmer()&lt;/code> from the &lt;code>lme4&lt;/code> package is
used.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(lme4)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glmer&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Sid), data&lt;span style="color:#f92672">=&lt;/span>d, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;logit&amp;#39;&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>lme4&lt;/code> provides a syntax for expressing multilevel models of different
structures. For our model here, which is one of the simplest multilevel
models (known as the varying intercept models), we express the partial
pooling of persons with the syntax &lt;code>(1|Sid)&lt;/code>, as shown in the last term
of the model formula. When such a partial pooling structure is
specified, &lt;code>glmer()&lt;/code> automatically imposes a constraint of &lt;strong>zero-meaned
normal distribution&lt;/strong> on the partial-pooled variable. In the case here,
this means that the ability of each person is modeled as being drawn
from a normal distribution with a mean of zero and an unknown standard
deviation to be estimated from the data. This constraint on the
distribution of the person ability naturally resolves the identification
issue of the IRT model. Hence, there is no need to impose an additional
sum-to-zero constraint as we did in &lt;code>m1&lt;/code>. We are only partial-pooling
the person variable here, so except for &lt;code>(1|Sid)&lt;/code>, everything else in
&lt;code>glmer()&lt;/code> is identical to those in &lt;code>m1&lt;/code>.&lt;/p>
&lt;p>After fitting the model, the estimates from &lt;code>m2&lt;/code> can be obtained with
the code below. Unpooled and partial-pooled estimates are extracted
differently in &lt;code>lme4&lt;/code>. To extract the unpooled estimates, one uses the
&lt;code>fixef()&lt;/code> function. These unpooled estimates, along with their standard
errors and other information, are also found in the model summary table
returned by &lt;code>summary()&lt;/code>. The partial-pooled estimates, however, are not
found in the table. To extract them, we need the &lt;code>ranef()&lt;/code> function as
shown below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2_eff.item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">fixef&lt;/span>(m2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2_eff.subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In addition to the estimates, we would also like to retrieve their
standard errors. Similar to the estimates, the standard errors of the
estimates are extracted differently according to whether they are
unpooled (fixed) or partial-pooled (random). We can utilize &lt;code>se.fixef()&lt;/code>
and &lt;code>se.ranef()&lt;/code> from the &lt;code>arm&lt;/code> package to extract these standard
errors:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2_se.item &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.fixef&lt;/span>(m2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>m2_se.subj &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.ranef&lt;/span>(m2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, to compare the estimates of &lt;code>m1&lt;/code> and &lt;code>m2&lt;/code>, I plot them together
in the same figure. I also plot the uncertainty—calculated as
$\pm 2 \times Standard~error$—around each estimate. Estimates and
uncertainties from &lt;code>m1&lt;/code> are plotted as blue, whereas those from &lt;code>m2&lt;/code> are
plotted as pink. The true effects for generating the simulated data are
plotted as solid black points.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Concatenate item &amp;amp; subj effect/std to match m1_eff/m1_se&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>m2_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2_eff.item, m2_eff.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m2_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2_se.item, m2_se.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e">#&amp;#39; Function stolen from `rethinking::col.alpha()`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>col.alpha &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function &lt;/span>(acol, alpha &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.5&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span> acol &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">col2rgb&lt;/span>(acol)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span> acol &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rgb&lt;/span>(acol[1]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, acol[2]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, acol[3]&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">255&lt;/span>, alpha)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span> acol
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># Plot for comparing `m1` &amp;amp; `m2`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, ylim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-4.8&lt;/span>, &lt;span style="color:#ae81ff">4.8&lt;/span>), xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>, n_subj&lt;span style="color:#f92672">+&lt;/span>n_item &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span> ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Effect&amp;#34;&lt;/span>, xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Item Index&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>n_item &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">segments&lt;/span>( &lt;span style="color:#ae81ff">-5&lt;/span>, &lt;span style="color:#a6e22e">mean&lt;/span>(m2_eff.item), n_item&lt;span style="color:#ae81ff">+.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">segments&lt;/span>( n_item&lt;span style="color:#ae81ff">+.5&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1000&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(E, A), pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#75715e"># Uncertainty bars&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#a6e22e">seq_along&lt;/span>(m2_se)) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i,i), m1_eff[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>m1_se[i], col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">4&lt;/span>), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">6&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i,i), m2_eff[i] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)&lt;span style="color:#f92672">*&lt;/span>m2_se[i], col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">col.alpha&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">.7&lt;/span>), lwd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23&lt;/span>&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( m1_eff, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( m2_eff, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part3_files/figure-commonmark/unnamed-chunk-6-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="shrinkage">Shrinkage&lt;/h3>
&lt;p>Some of the benefits of partial pooling &lt;a href="#multilevel-instead-of-mixed">discussed
earlier&lt;/a> are visualized in the comparison
plot above. The most drastic changes from the unpooled to the
partial-pooled model are seen in the ability estimates, which are
exactly the levels that get partially pooled. Two things to notice here.
First, there is less uncertainty in the partial-pooled estimates than in
the unpooled ones (pink bars tend to be shorter than their blue
counterparts). This follows naturally because, through partial pooling,
the model has access to more information (hence less uncertainty) for
each level. Secondly, the partial-pooled estimates tend to get “pulled”
towards the center (i.e., the grand mean of the subject estimates). In
addition, more extreme estimates are further pulled toward the center.
Essentially, this means that the model behaves in a way that is robust
against observations that result in extreme estimates. This is known as
&lt;strong>shrinkage&lt;/strong> and is also a feature that naturally arises from partial
pooling.&lt;/p>
&lt;p>From the figure, we can see that partial pooling improves the estimation
of the person abilities, as most pink circles are found to be much
closer to the solid black dots (true effects) than the blue ones. For
item easiness, which are not partial-pooled, the estimates also improve
slightly. This results from the improvement in estimating abilities.
Since ability and easiness are jointly estimated by the model, the
improvement from ability estimation carries on to easiness estimation.
Given the large improvement in ability estimates, one might consider
also pooling the items. Indeed, there is no reason to not pool.
&lt;strong>Partial pooling should be the default&lt;/strong>.&lt;/p>
&lt;h3 id="partial-pool-items-and-subjects">Partial Pool Items and Subjects&lt;/h3>
&lt;p>To specify the partial pooling of items in the model, we again utilize
the “bar” syntax: &lt;code>(1|Iid)&lt;/code>. This allows &lt;code>glmer()&lt;/code> to also model the
items as coming from a normal distribution with zero mean and unknown
variance. Now, since both the items and the subjects are centered at
zeros, an additional step is needed to reconstruct the zero-centered
item estimates back to their original locations&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This is why the
model formula in &lt;code>m2.2&lt;/code> uses &lt;code>1&lt;/code> instead of &lt;code>-1&lt;/code>. By specifying &lt;code>1&lt;/code>,
&lt;code>glmer()&lt;/code> estimates an independent global intercept. In the case here,
this intercept is identical to the amount subtracted from the item
effects for centering. Hence, to reconstruct the original non-centered
item estimates, we add the global intercept back to the item estimates,
as shown in line 4 in the code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m2.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glmer&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Iid) &lt;span style="color:#f92672">+&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">|&lt;/span>Sid), data&lt;span style="color:#f92672">=&lt;/span>d, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;logit&amp;#39;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>m2.2_eff.subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2.2)&lt;span style="color:#f92672">$&lt;/span>Sid[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>m2.2_eff.item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ranef&lt;/span>(m2.2)&lt;span style="color:#f92672">$&lt;/span>Iid[, &lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">fixef&lt;/span>(m2.2)[[&lt;span style="color:#e6db74">&amp;#34;(Intercept)&amp;#34;&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>m2.2_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2.2_eff.item, m2.2_eff.subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>m2.2_se &lt;span style="color:#f92672">=&lt;/span> arm&lt;span style="color:#f92672">::&lt;/span>&lt;span style="color:#a6e22e">se.ranef&lt;/span>( m2.2 )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>m2.2_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( m2.2_se&lt;span style="color:#f92672">$&lt;/span>Iid, m2.2_se&lt;span style="color:#f92672">$&lt;/span>Sid )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can compare &lt;code>m2.2&lt;/code> to &lt;code>m2&lt;/code> by plotting their estimates with the
plotting code previously shown and see that &lt;code>m2.2&lt;/code> further improves the
estimation (though not large). In the psychometric/measurement
literature, partial pooling both item and person is uncommon. But as
seen in our simulate-and-fit approach, partial pooling results in better
estimation. This approach also refutes the unjustified belief that
“&lt;em>fixed&lt;/em> effects should be used when the levels are &lt;strong>specifically
selected by the researcher&lt;/strong>”. In our simulation, values of the item
easiness are specifically “picked out”. They are deliberately set to be
equally-spaced values. And still, we saw that modeling the item effects
as &lt;em>random&lt;/em> is not only benign but even improves estimation. This is
true in general, and you can change the values of the item easiness in
the simulation to see that partial pooling mostly, if not always, gives
better estimates.&lt;/p>
&lt;h2 id="whats-next">What’s next&lt;/h2>
&lt;p>So far, we have been dealing with item response models with dichotomous
item responses. That is, a response can only either be correct (&lt;code>1&lt;/code>) or
wrong (&lt;code>0&lt;/code>). In &lt;a href="https://yongfu.name/irt4">Part 4&lt;/a>, we move on to item response models for
rating responses. These models are extremely useful since rating scales
are common in the social sciences. The models also allow us to model the
so-called “rater effect”, which quantifies the leniency of the raters.
By incorporating such rater effects, the model corrects for potential
biases introduced by subjective ratings, thereby giving more accurate
person and item estimates.&lt;/p>
&lt;!-- ####################################################################### -->
&lt;div id="matrix-algebra" class="Box"
title="Reconstructing dropped levels with the contrast matrix">
&lt;p>Don’t be intimidated by matrix algebra. It’s simply arithmetics in a
fancy manner, and it looks scary only because it does many things at
once. With some patience, you will be able to break down and understand
the steps involved.&lt;/p>
&lt;h4 id="reconstructing-dropped-estimate">Reconstructing Dropped Estimate&lt;/h4>
&lt;p>Let’s first see how the contrast matrix reconstructs the dropped
estimate from the sum-to-zero constrained model. I’ll start with a toy
example with only three subjects, $S_1, S_2, S_3$. The contrast matrix
for imposing a sum-to-zero constraint on the subjects is shown below.
Recall that the sum-to-zero constraint is coded through the dropping of
the last subject, $S_3$ (hence two columns left in the contrast matrix),
and implicit coding of $S_3$’s information into $S_1$ and $S_2$ by the
&lt;code>-1&lt;/code>s on the third row. Given this coding, the effect of $S_3$ can be
reconstructed from the effects of $S_1$ and $S_2$ by taking the negative
of their sum. This can be done through the code
&lt;code>c(subj_eff.m1.2, -sum(subj_eff.m1.2) )&lt;/code> from the previous post.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">$S_1$&lt;/th>
&lt;th style="text-align:center">$S_2$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">$S_1$&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$S_2$&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$S_3$&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The same thing can be done through &lt;strong>matrix multiplication&lt;/strong>. Simply
take the above 3-by-2 contrast matrix and multiply the 2-by-1 column
vector of the estimated effects for $S_1$ and $S_2$, which I abbreviate
as $E_1$ and $E_2$ here. The last entry of the resulting column vector
would then give what we want.&lt;/p>
&lt;p>$$
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1 \\
-1 &amp;amp; -1
\end{bmatrix}
\begin{bmatrix}
E_1 \\
E_2
\end{bmatrix} =
\begin{bmatrix}
E_1 \\
E_2 \\
-E_1 - E_2
\end{bmatrix}
$$&lt;/p>
&lt;p>Here’s the R code version of the above matrix multiplication:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>) &lt;span style="color:#75715e"># Contrast matrix coding sum-to-zero constraint&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( E1&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.5&lt;/span>, E2&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.7&lt;/span> ) &lt;span style="color:#75715e"># Made-up effect of S1 and S2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">%*%&lt;/span> eff
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [,1]
1 1.5
2 1.7
3 -3.2
&lt;/code>&lt;/pre>
&lt;h4 id="reconstructing-standard-error-of-dropped-estimate">Reconstructing Standard Error of Dropped Estimate&lt;/h4>
&lt;p>The contrast matrix can similarly be applied to reconstruct the variance
( hence the standard error) of the dropped subject’s estimate. The
reconstruction is based on the &lt;strong>variance sum law&lt;/strong>,
$Var(X+Y) = Var(X) + Var(Y) + 2~Cov(X,Y)$, which has a natural
generalization through matrix notations. Hence, given the variance of
the estimates for $S_1$ and $S_2$ and their covariance, we will be able
to reconstruct the variance of $E_3$ as&lt;/p>
&lt;p>$$
\begin{equation}
Var(E_3) = Var(E_1) + Var(E_2) + 2~Cov(E_1,E_2) \tag{1}
\end{equation}
$$&lt;/p>
&lt;p>The variances and covariances of the estimates are given by the
(variance-)covariance matrix of the fitted model. The formula below
shows the matrix generalization to the variance sum law. Note that
through the matrix generalization, we also get the reconstructed
covariances, as shown in the off-diagonal entries in the reconstructed
covariance matrix (the right-most matrix). The variance of $E_1$, $E_2$,
and $E_3$ are found on the diagonal. The standard errors of the
estimates are then obtained by taking the square roots of these diagonal
entries.&lt;/p>
&lt;img src="Cov.svg" style="width:95.0%" />
&lt;p>In R, the same calculation is done with the code below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>) &lt;span style="color:#75715e"># Contrast matrix for coding sum-to-zero constraint&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># Made-up variances-covariances matrix of the estimates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">matrix&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#ae81ff">0.3&lt;/span>, &lt;span style="color:#ae81ff">-0.01&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span> &lt;span style="color:#ae81ff">0.0&lt;/span>, &lt;span style="color:#ae81ff">0.4&lt;/span> ),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> byrow&lt;span style="color:#f92672">=&lt;/span>T, nrow&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">%*%&lt;/span> Vmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat) &lt;span style="color:#75715e"># Reconstructed variance-covariance matrix&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> 1 2 3
1 0.3 -0.01 -0.29
2 0.0 0.40 -0.40
3 -0.3 -0.39 0.69
&lt;/code>&lt;/pre>
&lt;h4 id="back-to-the-code">Back to the Code&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>d1 &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>(d1&lt;span style="color:#f92672">$&lt;/span>Sid) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> Iid &lt;span style="color:#f92672">+&lt;/span> Sid, data&lt;span style="color:#f92672">=&lt;/span>d1, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> (Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1))[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct std. error of the estimates with the constrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>(&lt;span style="color:#a6e22e">diag&lt;/span>(Vmat))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once familiar with the matrix algebra discussed, the above code for
reconstructing the dropped subject’s effect should become
self-explaining. The only complication here is that instead of using the
contrast matrix of the subjects, a larger matrix encompassing the coding
of &lt;strong>all levels of both the item and the subject variables&lt;/strong> is used to
match the covariance matrix given by the model (which also contains all
levels from all variables). This large contrast matrix can be thought of
as the concatenation of two contrast matrices along the diagonal, with
the remaining off-diagonal entries filled in with zeros. To better
explain this, let me go back to our previous example with three
subjects.&lt;/p>
&lt;p>To keep things simple, let’s assume additionally that there are only
three items. Since in the model, the sum-to-zero constraint is only
imposed on the subjects, the contrast matrix for the coding of items
would be a 3-by-3 identity matrix. Concatenating the item and subject
contrast matrices in the way mentioned above results in the matrix:&lt;/p>
&lt;p>&lt;code>$$ \begin{bmatrix} \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}_{3 \times 3} &amp;amp; 0~~~~~ \\ 0 &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}_{3 \times 2} \end{bmatrix}_{6 \times 5} $$&lt;/code>&lt;/p>
&lt;p>In general, with $n_I$ items and $n_S$ subjects, this concatenated
contrast matrix has the form:&lt;/p>
&lt;p>&lt;code>$$ \begin{bmatrix} ~~\mathrm{I}_{n_I \times n_I}~~~ &amp;amp; 0 ~~~~~~~~~~~~~~ \\ \\ 0 &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; &amp;amp; 0 \\ \vdots &amp;amp; &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \\ -1 &amp;amp; -1 &amp;amp; \cdots &amp;amp; -1 \end{bmatrix}_{n_S \times (n_S - 1)} \end{bmatrix}_{ (n_I + n_S) \times (n_I + n_S - 1) } $$&lt;/code>&lt;/p>
&lt;p>This is what the second part of the above code (reproduced below) is
doing. It first sets up the correct shape of this large contrast matrix
according to the number of items and subjects. The trick here is to use
the &lt;code>diag()&lt;/code> function to initialize a square matrix of zeros and drop
one of the columns to match the correct number of dimensions. Then, line
3 of the code sets the upper-left portion of this matrix (the item
sub-matrix) as an identity matrix by filling in the diagonal with ones.
Finally, the lower-right portion of the matrix (the subject sub-matrix)
is replaced with the subject contrast matrix constructed by the
&lt;code>contr.sum()&lt;/code> function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Construct contrast matrix&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>Cmat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">diag&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, nrow&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#f92672">+&lt;/span>n_subj)[, &lt;span style="color:#ae81ff">-1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">diag&lt;/span>(Cmat)[1&lt;span style="color:#f92672">:&lt;/span>n_item] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>idxS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj &lt;span style="color:#f92672">+&lt;/span> n_item
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>Cmat[idxS, idxS[&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(idxS)]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the contrast matrix &lt;code>Cmat&lt;/code> prepared, we can construct what we need
through matrix algebra. The estimates for all levels, including the
dropped one, are reconstructed by multiplying &lt;code>Cmat&lt;/code> with the estimates
returned by the model. This is illustrated in the line below. The
estimates are given by &lt;code>coef(m1)&lt;/code>, and the &lt;code>[, 1]&lt;/code> at the end of the
line forces the resulting one-column matrix into vector form.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1_eff &lt;span style="color:#f92672">=&lt;/span> ( Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1) )[, &lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The full covariance matrix is similarly reconstructed through &lt;code>Cmat&lt;/code> and
the covariance matrix extracted from the model (&lt;code>vcov(m1)&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>Vmat &lt;span style="color:#f92672">=&lt;/span> Cmat &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">vcov&lt;/span>(m1) &lt;span style="color:#f92672">%*%&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>(Cmat)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Since the final products we need are the standard errors, we extract the
diagonal entries of &lt;code>Vmat&lt;/code> and take the square root:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1_se &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">sqrt&lt;/span>( &lt;span style="color:#a6e22e">diag&lt;/span>(Vmat) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/div>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>We don’t touch the subject estimates, though, since we assume them
to be centered at zero in the simulation, remember?&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>stats</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (2/4)</title><link>https://yongfu.name/2023/03/06/irt2/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/03/06/irt2/</guid><description>&lt;p>In &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>, we went through the simplest item response model,
the 1PL model, from the perspective of simulations. Starting with item
difficulty and testee ability, we &lt;strong>worked forward&lt;/strong> to simulate item
responses that mimic real-world data. Back then, we were precisely
laying out the &lt;strong>data generating process&lt;/strong> that is assumed by the item
response theory. In this post, we &lt;strong>work backward&lt;/strong>. We will start with
the item responses and work back toward the unobserved difficulties and
abilities, with the help of statistical models. But first, let’s
simulate the data we will be using!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> (&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) ) &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">12&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>n_item &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># number of items&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>n_subj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">60&lt;/span> &lt;span style="color:#75715e"># number of subjects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_resp &lt;span style="color:#f92672">=&lt;/span> n_subj &lt;span style="color:#f92672">*&lt;/span> n_item &lt;span style="color:#75715e"># number of responses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rnorm&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n_subj, mean&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, sd&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> ) &lt;span style="color:#75715e"># Subjects&amp;#39; ability&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1.6&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, length&lt;span style="color:#f92672">=&lt;/span>n_item ) &lt;span style="color:#75715e"># Items&amp;#39; difficulty&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#75715e"># The data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>d &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>( S&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_subj, I&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_item, KEEP.OUT.ATTRS &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">FALSE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( &lt;span style="color:#a6e22e">logistic&lt;/span>(A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I]) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>S &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>S)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>d&lt;span style="color:#f92672">$&lt;/span>I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">factor&lt;/span>(d&lt;span style="color:#f92672">$&lt;/span>I)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">str&lt;/span>(d)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>'data.frame': 1800 obs. of 3 variables:
$ S: Factor w/ 60 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
$ I: Factor w/ 30 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
$ R: int 0 1 0 0 1 1 1 1 1 1 ...
&lt;/code>&lt;/pre>
&lt;p>In the code above, the first two lines are the definitions for the
logistic and the Bernoulli functions used previously. The second chunk
of code sets the shape of our data. This time, the simulated data is
much larger, with 30 items and 60 testees, or &lt;strong>subjects&lt;/strong> (I will use
the more general term “subject” hereafter). Since we assume here that
each subject responds to every item, this gives us 1800 responses in the
data.&lt;/p>
&lt;p>The subject abilities come from a normal distribution with a zero mean
and a standard deviation of one (the standard normal). The item
difficulties are equally-spaced values that range from -1.6 to 1. A
notable change from the previous post is that &lt;strong>number indices&lt;/strong> are
used here for labeling items (&lt;code>I&lt;/code>) and subjects (&lt;code>S&lt;/code>). For simple
illustrations, letter indices are clearer. But for larger data sets,
number indices are easier to manipulate with code. Now, since &lt;code>S&lt;/code> and
&lt;code>I&lt;/code> are coded as integers, we need to explicitly convert them into
factors. Otherwise, the model will treat the number indices as values in
a continuous variable.&lt;/p>
&lt;h2 id="dags-revisited">DAGs Revisited&lt;/h2>
&lt;p>Before we move on to the statistical model, let me lay out the DAGs
again. The DAG on the right below is identical to the one in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a> (the left DAG here), but with a slight modification that
emphasizes the perspective from the statistical model. Here, the
observed $S$ and $I$ take place of the unobserved $A$ and $D$,
respectively. So why the difference?&lt;/p>
&lt;img src="dag.svg" style="max-height:150px" />
&lt;p>Recall that the nodes $A$ and $D$ represent the joint influences of a
subject’s ability and an item’s difficulty on the probability of success
on that item. However, the statistical model cannot notice $A$ and $D$
since they are &lt;strong>theoretical concepts&lt;/strong> proposed by the IRT. What the
model “sees” is more similar to the DAG on the right. This DAG is
theoretically neutral. All it says is that the probability of success is
influenced by the particular subject and item present in an observation.
It does not further comment on the factors underlying each subject/item
that lead to the results.&lt;/p>
&lt;p>Given the data and the right DAG, the statistical model estimates the
so-called &lt;strong>subject effects&lt;/strong> and &lt;strong>item effects&lt;/strong>. These effects will
be estimates of subject ability and item difficulty &lt;strong>if the IRT
assumptions are met&lt;/strong>: when a subject and an item influence the result
&lt;strong>only through subject ability and item difficulty&lt;/strong>. With the concepts
of &lt;strong>subject/item effects&lt;/strong> in place, we can move on to the formulas of
the statistical model.&lt;/p>
&lt;h2 id="equation-index-and-annoying-things">Equation, Index and Annoying Things&lt;/h2>
&lt;p>The equations in (1) are the formulation of our model. This model is
known as the &lt;a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
regression&lt;/a>, or in
GLM terms, the Generalized Linear Model of the binomial family with the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit link&lt;/a> (more on this later).
Lots of things are going on here. Don’t panic, I’ll walk you through
slowly.&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim \text{Bernoulli}( P_i ) \\
&amp;amp; P_i = \text{logistic}( \mu_i ) \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{1}
$$&lt;/p>
&lt;p>First, note the common subscript $_i$ to the variables above. The
presence of this common $_i$ indicates that &lt;strong>the equations are read at
the &lt;em>observational&lt;/em> level&lt;/strong>. The observational level is easier to think
of with help of the &lt;a href="https://en.wikipedia.org/wiki/Wide_and_narrow_data">long data
format&lt;/a>. In this
long form of data, each row records an observation and is indexed by the
subscript $_i$. So you can think of the set of three equations as
describing the links among the variables for each observation. Note that
the long data format is also the format we have been using for the data
frames.&lt;/p>
&lt;p>The last equation in (1), also related to the reading of the subscript
$_i$, deserves some elaboration, as some might feel confused about the
square brackets after $\alpha$ and $\delta$. Actually, we have already
met these brackets in &lt;a href="https://yongfu.name/irt1">Part 1&lt;/a>. The brackets here serve a similar
function to R’s subset function &lt;code>[]&lt;/code> that we have used for linking
particular ability/difficulty levels of a subject/item to the rows
(observations) of the data frame. So what the square brackets after
$\alpha$ and $\delta$ do exactly, is to “look up” the index of the
subject and item for the $_i$th observation such that the $\alpha$
corresponding to the subject and the $\delta$ corresponding to the item
could be correctly retrieved. The model can thus “know” which $\alpha$
and $\delta$ to update when it encounters an observation. For instance,
suppose we are on the 3rd row (observation) of the data, in which
$S_3 = 5$ and $I_3 = 8$. This tells the model that the observation gives
information about $\alpha_5$ and $\delta_8$. The model thus learns
something about them and updates accordingly.&lt;/p>
&lt;p>I haven’t written about $\alpha$ and $\delta$ yet, but based on the
previous paragraph, you might already know what they are: $\alpha$s are
the subject effects and $\delta$s the item effects to be estimated by
the model.&lt;/p>
&lt;p>Now, let me walk you through the equations from bottom to top:&lt;/p>
&lt;ul>
&lt;li>$\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$&lt;br>
No surprise here. This equation simply illustrates how the model
computes a new variable $\mu$ from $\alpha$ and $\delta$.&lt;/li>
&lt;li>$P_i = \text{logistic}( \mu_i )$&lt;br>
The equation should look familiar. It indicates how the model maps
$\mu$, which can range from $-\infty$ to $\infty$, to probability,
$P$, through the logistic function.&lt;/li>
&lt;li>$R_i \sim \text{Bernoulli}( P_i )$&lt;br>
This equation describes that each observed response is generated from
a Bernoulli distribution with probability $P_i$. Or even simpler,
$R_i$ would be $1$ with probability $P_i$ and $0$ with probability
$1 - P_i$.&lt;/li>
&lt;/ul>
&lt;p>These equations all look familiar because they are essentially
mathematical representations of the simulation we have done. Here, the
model formulation is simply simulation in reverse.&lt;/p>
&lt;h3 id="the-logit-link">The Logit Link&lt;/h3>
&lt;p>The GLM formulation of (1) is often seen in an alternative form in (2).
The only difference between (2) and (1) lies in the second equation.
Instead of the logistic function, the second equation in (2) uses the
&lt;a href="https://en.wikipedia.org/wiki/Logit">logit&lt;/a> function. What is the
logit?&lt;/p>
&lt;p>$$
\begin{align}
&amp;amp; R_i \sim \text{Bernoulli}( P_i ) \\
&amp;amp; \text{logit}(P_i) = \mu_i \\
&amp;amp; \mu_i = \alpha_{[S_i]} + \delta_{[I_i]}
\end{align} \tag{2}
$$&lt;/p>
&lt;p>The logit function is simply the &lt;strong>mirror of the logistic&lt;/strong>. They do the
same mapping but in &lt;strong>reverse directions&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>the logistic function maps real numbers to probabilities&lt;/li>
&lt;li>the logit function maps probabilities to real numbers&lt;/li>
&lt;/ul>
&lt;p>The logistic and the logit are &lt;strong>inverse functions&lt;/strong> to each other. So
if a real number gets converted to the probability by the logistic, the
logit can convert it back to the original real, and vice versa.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>logit &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p ) &lt;span style="color:#a6e22e">log&lt;/span>( p&lt;span style="color:#f92672">/&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">-&lt;/span>p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>( p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>(x) ) &lt;span style="color:#75715e"># Transformed x on probability space&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] 0.2689414 0.2890505 0.3100255 0.3318122 0.3543437 0.3775407 0.4013123
[8] 0.4255575 0.4501660 0.4750208 0.5000000 0.5249792 0.5498340 0.5744425
[15] 0.5986877 0.6224593 0.6456563 0.6681878 0.6899745 0.7109495 0.7310586
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Logit gives x back&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">logit&lt;/span>(p)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4
[16] 0.5 0.6 0.7 0.8 0.9 1.0
&lt;/code>&lt;/pre>
&lt;p>Some arithmetics would get us from the logistic to the logit:&lt;/p>
&lt;p>$$
\begin{aligned}
\text{logistic}(x) &amp;amp;= \frac{1}{1 + e^{-x}} = p \\
&amp;amp; \Rightarrow ~~ e^{-x} = \frac{1-p}{p} \\
&amp;amp; \Rightarrow ~ -x = \text{log}(\frac{1-p}{p}) \\
&amp;amp; \Rightarrow ~~ x = -\text{log}(\frac{1-p}{p}) \\
&amp;amp; \phantom{\Rightarrow ~~ x } = \text{log}(\frac{p}{1-p}) = \text{logit}(p)
\end{aligned}
$$&lt;/p>
&lt;p>There is really nothing special about the logit function. We have
learned all the important things through the logistic back in &lt;a href="https://yongfu.name/irt1">Part
1&lt;/a>. I mention the logit here simply because the term is
frequently used. When people talk about GLMs, they prefer to use the
&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">link
function&lt;/a>
to characterize the model. The link function, in the case of the
logistic regression here, is the logit function. It transforms the
outcome probabilities into real numbers that are modeled linearly. It’s
just the logistic, but works in the reverse direction.&lt;/p>
&lt;h2 id="fitting-glm">Fitting GLM&lt;/h2>
&lt;p>Now, we are packed with the statistical muscles to carry out the
analysis. Let’s fit the model on the data we’ve simulated. In R, this is
done through the function &lt;code>glm()&lt;/code>. The first argument of &lt;code>glm()&lt;/code> is the
formula, in which we specify our linear model with R’s model syntax.
There are in principle two ways, one succinct and the other tedious, to
express the formula &lt;strong>when there are &lt;em>categorical predictors&lt;/em> in the
model&lt;/strong>. I will first demonstrate the tedious one, as it exposes all the
details hidden by the succinct form. Though tedious, it saves us from
confusion.&lt;/p>
&lt;h3 id="dummy-coding">Dummy Coding&lt;/h3>
&lt;p>The formulas we specify in &lt;code>glm()&lt;/code> (and other model fitting functions in
general) correspond pretty well to their mathematical counterparts. So
let me first present the math before we move on to the code. Lots of
things to explain here.&lt;/p>
&lt;p>Equation (3.2) is rewritten from the last two equations,
$logit(P_i) = \mu_i$ and $\mu_i = \alpha_{[S_i]} + \delta_{[I_i]}$ in
(2), which I reproduce here in Equation (3.1) by combining the two
equations.&lt;/p>
&lt;p>Earlier I mentioned that the square brackets after $\alpha$ and $\delta$
serve as a “look up” function to locate the relevant $\alpha$ and
$\delta$ of each subject and item in an observation. There is an
equivalent way to express the same formula without the use of these
“look up” functions, which is shown in equation (3.2). For the sake of
simplicity, let’s assume here that we have only two items (A, B) and
three subjects (J, K, L). For real data, equation (3.2) would be
extremely long.&lt;/p>
&lt;p>$$
\begin{align}
\tag{3.1} \text{logit}(\mu_i) &amp;amp;= \alpha_{[S_i]} + \delta_{[I_i]} \\
\tag{3.2} \text{logit}(\mu_i) &amp;amp;= J_i \alpha_J + K_i \alpha_K + L_i \alpha_L + A_i \delta_A + B_i \delta_B
\end{align}
$$&lt;/p>
&lt;p>The variables ($J_i, K_i, L_i, A_i, B_i$) in front of the $\alpha$s and
$\delta$s have a value of either 0 or 1. Here, they serve as a “switch”
that turns on the relevant $\alpha$ and $\delta$ and turns off the
others in each observation. This is easier to see with the help of the
tables below. Table 3.1 corresponds to Equation (3.1), and Table 3.2
corresponds to Equation (3.2). So, for instance, in row 2 of Table 3.2,
$K$ and $A$ are 1 while the others are 0. This turns on, or picks out,
$\alpha_K$ and $\delta_A$. As such, they would be updated by the model
when it reaches this observation. In row 2 of Table 3.1, $\alpha_K$ and
$\delta_A$ are picked out too, but not by the switches. They are
directly picked out through the &lt;em>K&lt;/em> and &lt;em>A&lt;/em> present in the row.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.1
&lt;/th>
&lt;th>
Table 3.2
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$S$&lt;/th>
&lt;th style="text-align:center">$I$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">J&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">K&lt;/td>
&lt;td style="text-align:center">A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">L&lt;/td>
&lt;td style="text-align:center">B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>The re-expression of Table 3.1 as Table 3.2 by coding the categories
into zeros and ones is known as &lt;strong>dummy coding&lt;/strong>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Why do we need
dummy coding? In short, this is because regression programs do not
“understand” the difference between categorical and continuous
variables. They read only numbers. Dummy coding is essentially
representing categorical variables as continuous ones so that the
program would know how to deal with them. Most programs dummy code for
the users (such as &lt;code>glm()&lt;/code>) if you give them categories. But there are
various ways to dummy code the categories and each of which results in a
different output. The interpretation of the output coefficients depends
on how the categories were coded. This confuses the novice as too much
is happening under the hood.&lt;/p>
&lt;h3 id="coding-models-the-long-route">Coding models: the long route&lt;/h3>
&lt;p>With the concepts of dummy coding in place, let’s code the model. I use
&lt;code>dummy_cols()&lt;/code> from the &lt;code>fastDummies&lt;/code> package to help me with dummy
coding. In the code below, I recode the item and subject variables into
zeros and ones. The result is identical to Table 3.2, except that it now
expands to 30 items and 60 subjects (90 columns in total). I won’t print
out the full dummy-coded data frame to save space. But be sure to take a
look at &lt;code>d_dummy&lt;/code> to see how it corresponds to Table 3.2.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">library&lt;/span>(fastDummies)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>d_dummy &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">dummy_cols&lt;/span>( d, &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>), remove_selected_columns&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">TRUE&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">dim&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 1800 91
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">colnames&lt;/span>(d_dummy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> [1] &amp;quot;R&amp;quot; &amp;quot;I_1&amp;quot; &amp;quot;I_2&amp;quot; &amp;quot;I_3&amp;quot; &amp;quot;I_4&amp;quot; &amp;quot;I_5&amp;quot; &amp;quot;I_6&amp;quot; &amp;quot;I_7&amp;quot; &amp;quot;I_8&amp;quot; &amp;quot;I_9&amp;quot;
[11] &amp;quot;I_10&amp;quot; &amp;quot;I_11&amp;quot; &amp;quot;I_12&amp;quot; &amp;quot;I_13&amp;quot; &amp;quot;I_14&amp;quot; &amp;quot;I_15&amp;quot; &amp;quot;I_16&amp;quot; &amp;quot;I_17&amp;quot; &amp;quot;I_18&amp;quot; &amp;quot;I_19&amp;quot;
[21] &amp;quot;I_20&amp;quot; &amp;quot;I_21&amp;quot; &amp;quot;I_22&amp;quot; &amp;quot;I_23&amp;quot; &amp;quot;I_24&amp;quot; &amp;quot;I_25&amp;quot; &amp;quot;I_26&amp;quot; &amp;quot;I_27&amp;quot; &amp;quot;I_28&amp;quot; &amp;quot;I_29&amp;quot;
[31] &amp;quot;I_30&amp;quot; &amp;quot;S_1&amp;quot; &amp;quot;S_2&amp;quot; &amp;quot;S_3&amp;quot; &amp;quot;S_4&amp;quot; &amp;quot;S_5&amp;quot; &amp;quot;S_6&amp;quot; &amp;quot;S_7&amp;quot; &amp;quot;S_8&amp;quot; &amp;quot;S_9&amp;quot;
[41] &amp;quot;S_10&amp;quot; &amp;quot;S_11&amp;quot; &amp;quot;S_12&amp;quot; &amp;quot;S_13&amp;quot; &amp;quot;S_14&amp;quot; &amp;quot;S_15&amp;quot; &amp;quot;S_16&amp;quot; &amp;quot;S_17&amp;quot; &amp;quot;S_18&amp;quot; &amp;quot;S_19&amp;quot;
[51] &amp;quot;S_20&amp;quot; &amp;quot;S_21&amp;quot; &amp;quot;S_22&amp;quot; &amp;quot;S_23&amp;quot; &amp;quot;S_24&amp;quot; &amp;quot;S_25&amp;quot; &amp;quot;S_26&amp;quot; &amp;quot;S_27&amp;quot; &amp;quot;S_28&amp;quot; &amp;quot;S_29&amp;quot;
[61] &amp;quot;S_30&amp;quot; &amp;quot;S_31&amp;quot; &amp;quot;S_32&amp;quot; &amp;quot;S_33&amp;quot; &amp;quot;S_34&amp;quot; &amp;quot;S_35&amp;quot; &amp;quot;S_36&amp;quot; &amp;quot;S_37&amp;quot; &amp;quot;S_38&amp;quot; &amp;quot;S_39&amp;quot;
[71] &amp;quot;S_40&amp;quot; &amp;quot;S_41&amp;quot; &amp;quot;S_42&amp;quot; &amp;quot;S_43&amp;quot; &amp;quot;S_44&amp;quot; &amp;quot;S_45&amp;quot; &amp;quot;S_46&amp;quot; &amp;quot;S_47&amp;quot; &amp;quot;S_48&amp;quot; &amp;quot;S_49&amp;quot;
[81] &amp;quot;S_50&amp;quot; &amp;quot;S_51&amp;quot; &amp;quot;S_52&amp;quot; &amp;quot;S_53&amp;quot; &amp;quot;S_54&amp;quot; &amp;quot;S_55&amp;quot; &amp;quot;S_56&amp;quot; &amp;quot;S_57&amp;quot; &amp;quot;S_58&amp;quot; &amp;quot;S_59&amp;quot;
[91] &amp;quot;S_60&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now, we can fit the model with the dummy-coded data &lt;code>d_dummy&lt;/code>. The first
argument in &lt;code>glm()&lt;/code> specifies the formula in R’s model syntax. Based on
Equation (3.2), we include all the dummy variables in the table. In R,
this means typing out all the variables as
&lt;code>R ~ S_1 + S_2 + ... + S_80 + I_1 + I_2 + ... + I_20&lt;/code>. That’s a lot of
work!&lt;/p>
&lt;p>Luckily, R provides a handy syntax for this. Since we are including all
the variables except the outcome on the right-hand side of the formula,
we can simply type &lt;code>R ~ .&lt;/code>. Here, the dot serves as a placeholder for
all the remaining variables not specified in the formula. We also need a
&lt;code>-1&lt;/code> in front of the dot: &lt;code>R ~ -1 + .&lt;/code>. The &lt;code>-1&lt;/code> tells the model not to
estimate a global intercept, which is done by default&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. We don’t need
a global intercept here since we want all the effects to be presented in
the items and subjects. If a global intercept is estimated, it will
“suck out” what should have been part of the subject/item effects,
rendering the results hard to interpret.&lt;/p>
&lt;p>The last thing to note in &lt;code>glm()&lt;/code> is the &lt;code>family&lt;/code> argument, which
characterizes the type of GLM used. Since we are fitting the data with
logistic regression, we pass &lt;code>binomial(&amp;quot;logit&amp;quot;)&lt;/code> to &lt;code>family&lt;/code>. The GLM
will then adopt the binomial distribution&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> with the logit link to map
the right-hand linear terms to the outcome space.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">summary&lt;/span>(m1)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Call:
glm(formula = R ~ -1 + ., family = binomial(&amp;quot;logit&amp;quot;), data = d_dummy)
Deviance Residuals:
Min 1Q Median 3Q Max
-2.4291 -0.8850 0.2498 0.8978 2.7509
Coefficients: (1 not defined because of singularities)
Estimate Std. Error z value Pr(&amp;gt;|z|)
I_1 1.926e+00 5.422e-01 3.553 0.000381 ***
I_2 1.798e+00 5.339e-01 3.368 0.000758 ***
I_3 1.154e+00 5.049e-01 2.286 0.022267 *
I_4 1.154e+00 5.049e-01 2.286 0.022267 *
I_5 1.351e+00 5.118e-01 2.639 0.008304 **
I_6 9.686e-01 4.997e-01 1.939 0.052559 .
I_7 1.351e+00 5.118e-01 2.639 0.008304 **
I_8 1.060e+00 5.021e-01 2.111 0.034737 *
I_9 9.686e-01 4.997e-01 1.939 0.052559 .
I_10 1.060e+00 5.021e-01 2.111 0.034737 *
I_11 5.368e-01 4.920e-01 1.091 0.275173
I_12 3.715e-01 4.905e-01 0.757 0.448848
I_13 -3.712e-02 4.901e-01 -0.076 0.939634
I_14 -3.712e-02 4.901e-01 -0.076 0.939634
I_15 1.263e-01 4.897e-01 0.258 0.796423
I_16 2.079e-01 4.898e-01 0.424 0.671244
I_17 -2.857e-01 4.922e-01 -0.581 0.561549
I_18 1.263e-01 4.897e-01 0.258 0.796423
I_19 -3.712e-02 4.901e-01 -0.076 0.939634
I_20 4.473e-02 4.898e-01 0.091 0.927249
I_21 -7.217e-01 5.000e-01 -1.443 0.148885
I_22 -1.194e-01 4.906e-01 -0.243 0.807787
I_23 -6.313e-01 4.979e-01 -1.268 0.204796
I_24 -7.217e-01 5.000e-01 -1.443 0.148885
I_25 -7.217e-01 5.000e-01 -1.443 0.148885
I_26 -7.217e-01 5.000e-01 -1.443 0.148885
I_27 -1.007e+00 5.082e-01 -1.981 0.047587 *
I_28 -1.212e+00 5.159e-01 -2.350 0.018770 *
I_29 -1.212e+00 5.159e-01 -2.350 0.018770 *
I_30 -1.961e+00 5.585e-01 -3.511 0.000447 ***
S_1 -1.800e+00 6.322e-01 -2.847 0.004415 **
S_2 1.732e+00 6.580e-01 2.632 0.008491 **
S_3 -1.575e+00 6.143e-01 -2.564 0.010355 *
S_4 -8.194e-01 5.775e-01 -1.419 0.155962
S_5 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_6 3.276e-01 5.732e-01 0.572 0.567650
S_7 4.966e-01 5.774e-01 0.860 0.389744
S_8 -3.227e-01 5.688e-01 -0.567 0.570509
S_9 -4.853e-01 5.705e-01 -0.851 0.394979
S_10 4.966e-01 5.774e-01 0.860 0.389744
S_11 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_12 -1.369e+00 6.010e-01 -2.277 0.022758 *
S_13 -8.194e-01 5.775e-01 -1.419 0.155962
S_14 -1.613e-01 5.682e-01 -0.284 0.776469
S_15 -1.613e-01 5.682e-01 -0.284 0.776469
S_16 -1.176e+00 5.909e-01 -1.991 0.046527 *
S_17 1.253e+00 6.146e-01 2.039 0.041452 *
S_18 3.276e-01 5.732e-01 0.572 0.567650
S_19 1.046e+00 6.010e-01 1.741 0.081706 .
S_20 -8.194e-01 5.775e-01 -1.419 0.155962
S_21 4.966e-01 5.774e-01 0.860 0.389744
S_22 2.856e+00 8.565e-01 3.334 0.000855 ***
S_23 1.479e+00 6.328e-01 2.337 0.019424 *
S_24 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_25 -8.194e-01 5.775e-01 -1.419 0.155962
S_26 1.625e-01 5.704e-01 0.285 0.775661
S_27 -4.853e-01 5.705e-01 -0.851 0.394979
S_28 -3.227e-01 5.688e-01 -0.567 0.570509
S_29 1.680e-15 5.687e-01 0.000 1.000000
S_30 6.712e-01 5.831e-01 1.151 0.249706
S_31 6.712e-01 5.831e-01 1.151 0.249706
S_32 3.618e+00 1.111e+00 3.256 0.001131 **
S_33 -1.613e-01 5.682e-01 -0.284 0.776469
S_34 -8.194e-01 5.775e-01 -1.419 0.155962
S_35 -4.853e-01 5.705e-01 -0.851 0.394979
S_36 -4.853e-01 5.705e-01 -0.851 0.394979
S_37 1.046e+00 6.010e-01 1.741 0.081706 .
S_38 -6.504e-01 5.734e-01 -1.134 0.256656
S_39 1.046e+00 6.010e-01 1.741 0.081706 .
S_40 -1.575e+00 6.143e-01 -2.564 0.010355 *
S_41 1.625e-01 5.704e-01 0.285 0.775661
S_42 -6.504e-01 5.734e-01 -1.134 0.256656
S_43 1.253e+00 6.146e-01 2.039 0.041452 *
S_44 -1.800e+00 6.322e-01 -2.847 0.004415 **
S_45 -3.227e-01 5.688e-01 -0.567 0.570509
S_46 4.966e-01 5.774e-01 0.860 0.389744
S_47 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_48 3.276e-01 5.732e-01 0.572 0.567650
S_49 8.536e-01 5.908e-01 1.445 0.148546
S_50 -3.227e-01 5.688e-01 -0.567 0.570509
S_51 -4.853e-01 5.705e-01 -0.851 0.394979
S_52 -1.613e-01 5.682e-01 -0.284 0.776469
S_53 6.712e-01 5.831e-01 1.151 0.249706
S_54 2.856e+00 8.565e-01 3.334 0.000855 ***
S_55 -6.504e-01 5.734e-01 -1.134 0.256656
S_56 6.712e-01 5.831e-01 1.151 0.249706
S_57 1.253e+00 6.146e-01 2.039 0.041452 *
S_58 -9.940e-01 5.833e-01 -1.704 0.088343 .
S_59 -4.853e-01 5.705e-01 -0.851 0.394979
S_60 NA NA NA NA
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 2495.3 on 1800 degrees of freedom
Residual deviance: 1927.2 on 1711 degrees of freedom
AIC: 2105.2
Number of Fisher Scoring iterations: 6
&lt;/code>&lt;/pre>
&lt;p>Take a look at the output. Something’s strange. Since there are 30 items
and 60 subjects in the data, we expect the model to return 90
coefficients, one for each subject/item effect. However, the last
coefficient, &lt;code>S_60&lt;/code> in this case, is &lt;code>NA&lt;/code>. The model does not
estimate this coefficient. Why?&lt;/p>
&lt;h4 id="identifiability">Identifiability&lt;/h4>
&lt;p>The reason is that there are &lt;strong>infinite&lt;/strong> sets of parameter combinations
that generate the same probabilities underlying our data. Thus, the
model is unable to work in reverse to infer a unique set of coefficients
from the data. To deal with this issue, R silently sets a constraint on
the parameters: it simply drops one of the parameters and estimates the
rest. When this is done, the remaining parameters become
&lt;a href="https://en.wikipedia.org/wiki/Identifiability">identifiable&lt;/a>, and the
model would be able to estimate them.&lt;/p>
&lt;p>But still, where did the infinity come from? Didn’t we simulate the
data? We didn’t introduce infinity, did we?&lt;/p>
&lt;p>We &lt;em>&lt;strong>did&lt;/strong>&lt;/em> actually, in silence. Recall that the probability of
success on an item is determined by the difference between ability and
difficulty. Since it is the &lt;em>&lt;strong>difference&lt;/strong>&lt;/em> that matters, there could
be an infinite number of ability and difficulty pairs that yield the
same difference. By adding any common value to a pair, we get a new pair
of ability and difficulty that yields the same probability. The code
below demonstrates this. Here, I shift the ability and difficulty levels
by a common value &lt;code>s&lt;/code>. The resulting probabilities should be identical
before and after the shift (except for a tiny floating point
imprecision). You can play with the code below by changing the value of
&lt;code>s&lt;/code>. Identical results should always be yielded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Shift A/D by a common factor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>A2 &lt;span style="color:#f92672">=&lt;/span> A &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>D2 &lt;span style="color:#f92672">=&lt;/span> D &lt;span style="color:#f92672">+&lt;/span> s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>p1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities before shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( A2[d&lt;span style="color:#f92672">$&lt;/span>S] &lt;span style="color:#f92672">-&lt;/span> D2[d&lt;span style="color:#f92672">$&lt;/span>I] ) &lt;span style="color:#75715e"># Probabilities after shift&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">sum&lt;/span>( &lt;span style="color:#a6e22e">abs&lt;/span>(p1 &lt;span style="color:#f92672">-&lt;/span> p2) ) &lt;span style="color:#75715e"># Should be extremely close to zero&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 9.453549e-14
&lt;/code>&lt;/pre>
&lt;p>The way R deals with this issue of identifiability is not preferable
since we want to recover the parameters in our simulation (i.e., the set
of parameters without the shift). To get around R’s default treatment,
we have to impose constraints on the parameters ourselves.&lt;/p>
&lt;p>Recall that subject abilities are generated according to a standard
normal distribution in the simulation. Since the standard normal has a
mean of zero, the &lt;strong>expectation of the sum of subject abilities is
&lt;em>zero&lt;/em>&lt;/strong>. We can use this expectation as a constraint to the parameters
by constraining the subject effects to &lt;strong>sum to zero&lt;/strong>. This constraint,
however, &lt;strong>does not&lt;/strong> scale the model’s estimates to perfectly match the
true parameters since the true subject abilities never exactly sum to
zero in a single run of the simulation. However, the relative scale
would be close enough for the simulated parameters to be comparable to
those recovered by the model. Later in the next post, when the
&lt;strong>generalized linear mixed model&lt;/strong> is introduced, you will see that
there is no need to impose such a constraint. The constraint is
naturally included through the model’s assumptions. The estimated
subject effects then, do not need to sum to zero. Subject effects would
be assumed to result from a normal distribution with a mean of zero.&lt;/p>
&lt;p>Through dummy coding, we can impose the sum-to-zero constraint on the
subject effects. I illustrate this with the example previously presented
in Table 3.2, where there are only 3 subjects and 2 items. Table 3.3 is
re-coded from Table 3.2, in which the sum-to-zero constraint is imposed.&lt;/p>
&lt;p>The sum-to-zero constraint is imposed by dropping one of the subjects
and coding the remaining as &lt;code>-1&lt;/code> for all the observations where the
dropped subject is originally coded as &lt;code>1&lt;/code>. This is shown in Table 3.3,
where I drop subject $L$ (hence the &lt;code>-&lt;/code> in the column) and code the 3rd
row of $J$ and $K$ as &lt;code>-1&lt;/code>.&lt;/p>
&lt;table>
&lt;tr>
&lt;th>
Table 3.2
&lt;/th>
&lt;th>
Table 3.3
&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;td>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$_i$&lt;/th>
&lt;th style="text-align:center">$J$&lt;/th>
&lt;th style="text-align:center">$K$&lt;/th>
&lt;th style="text-align:center">$L$&lt;/th>
&lt;th style="text-align:center">$A$&lt;/th>
&lt;th style="text-align:center">$B$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-1&lt;/td>
&lt;td style="text-align:center">-&lt;/td>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>With the coding scheme in Table 3.3, the estimated effect of subject $L$
can be reconstructed from the remaining subject effects returned by the
model. Since the sum of all subject effects is zero, the effect of
subject $L$ will be the negative of the others’ sum. This might seem a
bit confusing. But notice that the sum-to-zero constraint
&lt;strong>simultaneously applies to all effects in the variable&lt;/strong>. Once the
effect of the dropped category is reconstructed, each effect will also
be the negative sum of the remaining effects.&lt;/p>
&lt;p>Let’s impose this constraint on the data with code. Here, I will drop
the first subject &lt;code>S_1&lt;/code>. You can choose any subject you like to drop,
and the result will be identical. The code from line 3 to 5 below pick
outs the rows where &lt;code>S_1&lt;/code> is coded as &lt;code>1&lt;/code> and recode them as &lt;code>-1&lt;/code> on all
the subject columns. After the re-coding, the final line of code then
drops &lt;code>S_1&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>d_dummy2 &lt;span style="color:#f92672">=&lt;/span> d_dummy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>toDrop &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;S_1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>allCategories &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">startsWith&lt;/span>( &lt;span style="color:#a6e22e">names&lt;/span>(d_dummy2), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>idx_recode &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">which&lt;/span>( d_dummy2[[toDrop]] &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>d_dummy2[idx_recode, allCategories] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>d_dummy2[[toDrop]] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">NULL&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, let’s refit the model with this constraint-coded data. I simply
replace &lt;code>d_dummy&lt;/code> with &lt;code>d_dummy2&lt;/code> in the &lt;code>data&lt;/code> argument. Everything
else is the same.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> ., data&lt;span style="color:#f92672">=&lt;/span>d_dummy2, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># summary(m1.1)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you print out the coefficients of the fitted model, you will see that
the result is what we expected. The model returns 89 coefficients, which
match the number of the predictor variables we passed in. No coefficient
is dropped. We already dropped it for the model. And since we dropped
the predictor in a principled way, we know how to reconstruct it. The
effect of the dropped &lt;code>S_1&lt;/code> will be the negative sum of the remaining.
This is shown in the code below, which reconstructs all the item/subject
effects from the model’s coefficients.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>item_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S_&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#75715e"># Reconstruct S_1 from the remaining subject effects&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>subj_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff), subj_eff )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now plot the estimated effects against the true parameter values
from the simulation. The figures below plot the estimated effects on the
x-axis and the true parameters of the y-axis. The dashed line has a
slope of 1 without an intercept. This line indicates perfect matches
between the truth and the estimates. Notice that for the figure on the
right, I reverse the signs of the item effects to match the scale of
item difficulty. This is necessary since $D$ is subtracted from $A$ in
the simulation. In other words, the effect of difficulty assumed by the
1PL model is &lt;strong>negative&lt;/strong>: the larger the difficulty, the less
probability of success on the item. However, &lt;code>glm()&lt;/code> allows only
additive effects. The effects in the model are summed together to yield
predictions. Hence, the item effects estimated by &lt;code>glm()&lt;/code> will be the
negative of those assumed by the 1PL model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( subj_eff, A, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>item_eff, D, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> ); &lt;span style="color:#a6e22e">abline&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="two-column">
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-1.svg"
data-fig-align="center" />&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-10-2.svg"
data-fig-align="center" />&lt;/p>
&lt;/div>
&lt;p>As seen in both figures, the dots scatter around the lines quite
randomly, which indicates that the model does recover the parameters. To
have a clearer view of the estimates’ accuracy, let me plot some more.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set figure margins&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">1.6&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>true_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(D, A)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>est_eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>item_eff, subj_eff)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>n_param &lt;span style="color:#f92672">=&lt;/span> n_item &lt;span style="color:#f92672">+&lt;/span> n_subj
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>cols &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, n_item), &lt;span style="color:#a6e22e">rep&lt;/span>(&lt;span style="color:#ae81ff">4&lt;/span>, n_subj) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>y_lim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">max&lt;/span>( &lt;span style="color:#a6e22e">abs&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>(true_eff, est_eff)) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11&lt;/span>&lt;span>y_lim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#f92672">-&lt;/span>y_lim&lt;span style="color:#ae81ff">-.1&lt;/span>, y_lim&lt;span style="color:#ae81ff">+.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, ylim&lt;span style="color:#f92672">=&lt;/span>y_lim, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>, n_param&lt;span style="color:#ae81ff">+1&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Effect&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( v&lt;span style="color:#f92672">=&lt;/span>n_item&lt;span style="color:#ae81ff">+0.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( true_eff, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_eff, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17&lt;/span>&lt;span>&lt;span style="color:#a6e22e">for &lt;/span>(i in &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">:&lt;/span>n_param)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18&lt;/span>&lt;span> &lt;span style="color:#a6e22e">lines&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(i, i), &lt;span style="color:#a6e22e">c&lt;/span>(true_eff[i], est_eff[i]), col&lt;span style="color:#f92672">=&lt;/span>cols[i] )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mtext&lt;/span>( &lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Items&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Subjects&amp;#34;&lt;/span>), at&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">9&lt;/span>, &lt;span style="color:#ae81ff">61&lt;/span>), padj &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">-.5&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/eff-est-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>In the plot above, I overlay the estimated effects onto the true
parameters. The dots are the true parameters and the circles are the
model’s estimates. The vertical lines connecting the dots and the
circles show the distances between the truth and the estimates. It is
obvious from the plot that, compared to item difficulties, subject
abilities are harder to estimate, as the distances to the truth are in
general larger for subject estimates. This is apparent in hindsight, as
each item is taken by 60 subjects whereas each subject only takes 30
items. Hence, the estimation for the items is more accurate, compared to
the subjects, as there are more data to estimate each.&lt;/p>
&lt;p>The effect of manipulating the number of subjects and items is revealed
in the plot below. Here, I refit the model with data that have the
subject and the item numbers flipped. The subject abilities are now
estimated more accurately than the item difficulties. You can experiment
with this to see how the estimation accuracy changes with different
combinations of subject/item numbers. The functions &lt;code>sim_data()&lt;/code> and
&lt;code>plot_estimate()&lt;/code> in
&lt;a href="https://github.com/liao961120/stom/blob/main/inst/blog/irt/estimate-acc.R">&lt;code>estimate-acc.R&lt;/code>&lt;/a>
can help you with this.&lt;/p>
&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-11-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="coding-models-the-easy-route">Coding models: the easy route&lt;/h3>
&lt;p>We have gone through a long route, along which we have learned a lot.
Now, you are qualified to take the easy route: to use R’s handy function
for dummy coding. Note that this route won’t be easy at all if you never
went through the longer one. Rather, confusion is all you will get, and
you will have no confidence in the model you coded. Simple code is
simple only for those who are well-trained. So now, let’s fit the model
again. This time, we take the highway.&lt;/p>
&lt;p>The trick for controlling how the model functions dummy code the
categorical variables is to use the &lt;code>contrasts()&lt;/code> function to set up the
preferred coding scheme. In the code below, I pass the number of the
categories in $S$ (i.e., &lt;code>n_subj&lt;/code>) to &lt;code>contr.sum()&lt;/code>, which is a helper
function that codes the subjects in the exact same way as we did in
Table 3.3 (execute &lt;code>contr.sum(3)&lt;/code> and you will see a table that
corresponds exactly to Table 3.3).&lt;/p>
&lt;p>After the coding scheme is set, we can express categorical predictors in
the model formula directly. Everything else is the same except the last
line. Previously, I demonstrated dummy coding by dropping the first
subject in $S$. Here, &lt;code>contr.sum()&lt;/code> drops the last subject by default.
Thus, the code for constructing the dropped subject is slightly
different here.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#75715e"># Drop the last S and impose sum-to-zero constraint on S&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">contrasts&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>S ) &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">contr.sum&lt;/span>( n_subj )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">glm&lt;/span>( R &lt;span style="color:#f92672">~&lt;/span> &lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> I &lt;span style="color:#f92672">+&lt;/span> S, data&lt;span style="color:#f92672">=&lt;/span>dat, family&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">binomial&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;logit&amp;#34;&lt;/span>) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>eff &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">coef&lt;/span>(m1.2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>item_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;I&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> eff[ &lt;span style="color:#a6e22e">startsWith&lt;/span>(&lt;span style="color:#a6e22e">names&lt;/span>(eff), &lt;span style="color:#e6db74">&amp;#34;S&amp;#34;&lt;/span>) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>subj_eff.m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( subj_eff.m1.2, &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#a6e22e">sum&lt;/span>(subj_eff.m1.2) )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we have the estimated effects from the model, let’s check the
results. The figure below plots the current estimates (&lt;code>m1.2&lt;/code>) against
previous ones (&lt;code>m1.1&lt;/code>). The estimates from the two models agree, which
confirms that the second model is correctly coded.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>est_m1.1 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff, subj_eff )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>est_m1.2 &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( item_eff.m1.2, subj_eff.m1.2 )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">-2.5&lt;/span>,&lt;span style="color:#ae81ff">2.5&lt;/span>), xlab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.2&amp;#34;&lt;/span>, ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;m1.1&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( &lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;grey&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( est_m1.2, est_m1.1, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>cols )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part2_files/figure-commonmark/unnamed-chunk-13-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>This post is lengthy, but not because it is hard. Rather, the concepts
presented are fairly simple. The post is lengthy because we got used to
texts that hide details from the readers. The text here does the
opposite: it presents all the &lt;strong>necessary details&lt;/strong> to get the
statistical model working, without confusion. People often assume that
hidden details are trivial. But more often, it is just because writers
are too lazy to present the details. Statistics is hard partly because
it is loaded with details that are hidden and ignored. When details get
ignored long enough, they accumulate to become entangled and
uncrackable. Coding, again, is here to help. It dissolves the fuzziness
that otherwise accumulates and hinders understanding.&lt;/p>
&lt;p>In &lt;a href="https://yongfu.name/irt3">Part 3&lt;/a>, we move on to &lt;strong>Generalized Linear Mixed Models&lt;/strong>,
which are extensions to GLMs that improve estimation and efficiency by
harnessing the information from common group memberships in the data. We
will use the same data, and the text would be much shorter, I hope.
Seeya!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I use &lt;em>dummy coding&lt;/em> as a general umbrella term to cover all
systems for coding categorical variables. In R, what is known as
“treatment coding” (&lt;code>contr.treatment&lt;/code>) is sometimes called “dummy
coding” by others. Here, I follow R’s convention. When “dummy
coding” is used, I always refer to the general sense of re-coding
categories as numbers (not necessarily zeros and ones).&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This default behavior of estimating a global intercept makes sense
in the context of continuous predictors, such as the simple linear
model shown below. In this case, we can succinctly express the
formula as &lt;code>y ~ x&lt;/code> in R’s model syntax. The estimate of the global
intercept $\alpha$ would be given as the intercept coefficient in
the model output.&lt;/p>
&lt;p>$$
\begin{aligned}
y_i &amp;amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;amp;= \alpha + \beta x_i
\end{aligned}
$$&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>I haven’t mentioned the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial
distribution&lt;/a>
before. The binomial distribution is the extension of the Bernoulli
distribution to $n$ independent trials. So if you repeat the
Bernoulli process $n$ times and sum the outcomes, say, you toss the
coin $n=10$ times and record the number of heads observed, the
distribution of outcomes would follow a binomial distribution with
parameters $n$ and $p$. So the Bernoulli distribution is simply a
special case of the binomial distribution where $n=1$.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>stats</category><category>psychology</category></item><item><title>Demystifying Item Response Theory (1/4)</title><link>https://yongfu.name/2023/02/25/irt1/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/25/irt1/</guid><description>&lt;figure>
&lt;img src="irt.jpg" style="width:70.0%"
alt="Okay, so these are the item characteristic curves. What then?" />
&lt;figcaption aria-hidden="true">&lt;em>Okay, so these are the &lt;a
href="https://www.researchgate.net/figure/Item-characteristic-curves-Item-Response-Theory-IRT-1PL-model_fig1_342560715">item
characteristic curves&lt;/a>. What then?&lt;/em>&lt;/figcaption>
&lt;/figure>
&lt;h2 id="mysterious-item-response-theory">Mysterious Item Response Theory&lt;/h2>
&lt;p>&lt;strong>Item response theory is &lt;em>mysterious&lt;/em> and intimidating to students.&lt;/strong>
It is mysterious in the way it is presented in textbooks, at least in
introductory ones. The text often starts with an ambitious conceptual
introduction to IRT, which most students would be able to follow, but
with some confusion. Curious students might bear with the confusion and
expect it to resolve in the following text, only to find themselves
disappointed. At the point where the underlying statistical model should
be further elaborated, the text abruptly stops and tries to convince the
readers to trust the results from black-box IRT software packages.&lt;/p>
&lt;p>It isn’t that I have trust issues with black-box software, and I also
agree that certain details of IRT model estimation should be hidden from
the readers. The problem is that there’s a huge gap here, between where
textbooks stopped explaining and where the confusing details of
statistics should be hidden. Hence, students would be tricked into
believing that they have a &lt;em>sufficient degree of understanding&lt;/em>, but in
reality, it’s just blind faith.&lt;/p>
&lt;p>A sufficient degree of understanding should allow the student to deploy
the learned skills to new situations. Therefore, a sufficient degree of
understanding of IRT models should allow students to extend and apply
the models to analyses of, for instance, &lt;a href="https://en.wikipedia.org/wiki/Differential_item_functioning">differential item functioning
(DIF)&lt;/a> or
&lt;strong>differential rater functioning (DRF)&lt;/strong>.&lt;/p>
&lt;p>I’m arguing here that there is a basic granularity of understanding,
somewhat similar to the concept of &lt;a href="https://en.wikipedia.org/wiki/Basic_category">basic-level
category&lt;/a>, that when
reached, allows a student to smoothly adapt the learned skills to a wide
variety of situations, modifying and extending the skills on demand. And
I believe that item response theory is &lt;em>&lt;strong>too hard&lt;/strong>&lt;/em> for a student to
learn and reach this basic level of understanding, given its
conventional presentation and historical development&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There is still hope however, thanks to the development of a very general
set of statistical models known as the &lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Models
(GLM)&lt;/a>. Item
response models could be understood in terms of the GLM and its
extensions
(&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_mixed_model">GLMM&lt;/a>
and non-linear form of the GLM/GLMM). To be too particular about the
details, the results from IRT software packages and the GLMs/GLMMs would
be very similar but not identical, since they utilize different
estimation methods. The strengths of the GLM, however, lie in its
conceptual simplicity and extensibility. Through GLM, IRT and other
models such as the T-test, ANOVA, and linear regression, are all placed
together into the same conceptual framework. Furthermore, software
packages implementing GLMs are widely available. Users can thus
experiment with them—simulate a set of data based on known parameters,
construct the model and feed it the data, and see if the fitted model
correctly recovers the parameters. This technique of learning statistics
is probably the only effective way for students to understand &lt;em>&lt;strong>a
mysterious statistical model&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>In this series of posts, I will walk you through the path of
understanding item response theory, with the help of simulations and
generalized linear models. No need to worry if you don’t know GLMs yet.
We have another ally—&lt;a href="https://www.r-project.org">R&lt;/a>, in which we will be
simulating artificial data and fitting statistical models along the way.
Although it might seem intimidating at first, coding simulations and
models in fact provide scaffolding for learning. When feeling unsure or
confused, you can always resort to these simulation-based experiments to
resolve the issues at hand. In this very first post, I will start by
teaching you &lt;em>&lt;strong>simulations&lt;/strong>&lt;/em>.&lt;/p>
&lt;h2 id="just-enough-theory-to-get-started">Just Enough Theory to Get Started&lt;/h2>
&lt;p>Jargons aside, the concept behind item response theory is fairly simple.
Consider the case where 80 testees are taking a 20-item English
proficiency test. Under this situation, what are the &lt;em>&lt;strong>factors&lt;/strong>&lt;/em> that
influence whether an item is correctly solved by a testee?
Straightforward right? If an item is easy and if a testee is proficient
in English, he/she would probably get the item correct. Here, &lt;em>&lt;strong>two
factors jointly influence the result&lt;/strong>&lt;/em>:&lt;/p>
&lt;ol>
&lt;li>how difficult (or easy) the item is&lt;/li>
&lt;li>the English ability of the testee&lt;/li>
&lt;/ol>
&lt;p>We can express these variables and the relations between them in the
graphs below. Let’s focus on the left one first. Here, $A$ represents
the &lt;strong>ability&lt;/strong> of the testee, $D$ represents the &lt;strong>difficulty&lt;/strong> of the
item, and $R$ represents the &lt;strong>item response&lt;/strong>, or &lt;strong>score&lt;/strong> on the
item. A response for an item is coded as &lt;code>1&lt;/code> ($R=1$) if it is solved
correctly. Otherwise, it is coded as &lt;code>0&lt;/code> ($R=0$). The arrows
$A \rightarrow R$ and $D \rightarrow R$ indicate the direction of
influence. The arrows enter $R$ since item difficulty and testee ability
influence the score on the item (not the other way around). $A$ and $D$
are drawn as a circled node to indicate that they are &lt;strong>unobserved&lt;/strong> (or
&lt;strong>latent&lt;/strong>, if you prefer a fancier term), whereas uncircled nodes
represent directly observable variables (i.e., stuff that gets recorded
during data collection). This graphical representation of the variables
and their relationships is known as a &lt;a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed Acyclic Graph
(DAG)&lt;/a>.&lt;/p>
&lt;img src="dag.svg" style="max-height:150px" />
&lt;p>The DAGs laid out here represent the concept behind the simplest kind of
item response models, known as the &lt;strong>1-parameter logistic (1PL) model&lt;/strong>
(or the Rasch Model). In more formal terms, this model posits that the
&lt;strong>probability&lt;/strong> of correctly answering an item is determined by the
&lt;strong>difference&lt;/strong> between testee ability and item difficulty. So a more
precise DAG representation for this model would be the one shown on the
right above. Here, $P$ is the probability of correctly answering the
item, which cannot be directly observed. However, $P$ directly
influences the item score $R$, hence the arrow $P \rightarrow R$.&lt;/p>
&lt;p>Believe it or not, the things we have learned so far could get us
started. So let’s simulate some data, based on what we’ve learned about
item response theory!&lt;/p>
&lt;h2 id="simulating-item-responses">Simulating Item Responses&lt;/h2>
&lt;figure>
&lt;img src="tenet2.gif" style="width:85.0%"
alt="Simulation is playing god in a small world. Similar to model fitting, but in reverse direction." />
&lt;figcaption aria-hidden="true">Simulation is &lt;em>playing god&lt;/em> in a
small world. Similar to model fitting, but in &lt;em>reverse&lt;/em>
direction.&lt;/figcaption>
&lt;/figure>
&lt;p>Consider the scenario where 3 students (Rob, Tom, and Joe) took a math
test with 2 items (A and B). Since we play gods during simulations, we
know the math ability of the students and the difficulty of the items.
These ability/difficulty levels can range from positive to negative
numbers, unbounded. Larger numbers indicate higher levels of
difficulty/ability. In addition, the levels of difficulty and ability
sit on a common scale and hence could be directly compared. Also, each
student responds to every item, so we get responses from all 6 (3x2)
combinations of students and items. Let’s code this in R below. The
function &lt;code>expand.grid()&lt;/code> would pair up the 6 combinations for us.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>D &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( A&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.4&lt;/span>, B&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> ) &lt;span style="color:#75715e"># Difficulty of item&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>A &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( R&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, T&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span>, J&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">-0.4&lt;/span> ) &lt;span style="color:#75715e"># Ability of student (R:Rob, T:Tom, J:Joe)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>dat &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">expand.grid&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span> I &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;B&amp;#34;&lt;/span> ), &lt;span style="color:#75715e"># Item index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span> T &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>( &lt;span style="color:#e6db74">&amp;#34;R&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;T&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;J&amp;#34;&lt;/span> ) &lt;span style="color:#75715e"># Testee index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 x 2
I T
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;
1 A R
2 B R
3 A T
4 B T
5 A J
6 B J
&lt;/code>&lt;/pre>
&lt;p>After having all possible combinations of the students and the items, we
could collect the values of student ability and item difficulty into the
data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">=&lt;/span> A[ dat&lt;span style="color:#f92672">$&lt;/span>T ] &lt;span style="color:#75715e"># map ability to df by testee index T&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>D &lt;span style="color:#f92672">=&lt;/span> D[ dat&lt;span style="color:#f92672">$&lt;/span>I ] &lt;span style="color:#75715e"># map difficulty to df by item index I&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 x 4
I T A D
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4
2 B R 0.5 0.1
3 A T 0.1 0.4
4 B T 0.1 0.1
5 A J -0.4 0.4
6 B J -0.4 0.1
&lt;/code>&lt;/pre>
&lt;p>Now, we’ve got all the data needed for simulation, the only thing left
is to precisely lay out the &lt;strong>rules for generating the response data
$R$&lt;/strong>—the scores (zeros and ones) on the items solved by the students.
We are two steps away.&lt;/p>
&lt;h3 id="generating-probabilities">Generating Probabilities&lt;/h3>
&lt;p>When IRT is introduced in the previous section, I mention that the
probability of successfully solving an item is determined by the
&lt;strong>difference between testee ability and item difficulty&lt;/strong>. It is
straightforward to get this difference: simply subtract $D$ from $A$ in
the data. This would give us a new variable $\mu$. I save the values of
$\mu$ to column &lt;code>Mu&lt;/code> in the data frame.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>Mu &lt;span style="color:#f92672">=&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>A &lt;span style="color:#f92672">-&lt;/span> dat&lt;span style="color:#f92672">$&lt;/span>D
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 x 5
I T A D Mu
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 A R 0.5 0.4 0.1
2 B R 0.5 0.1 0.4
3 A T 0.1 0.4 -0.3
4 B T 0.1 0.1 0
5 A J -0.4 0.4 -0.8
6 B J -0.4 0.1 -0.5
&lt;/code>&lt;/pre>
&lt;p>From the way $\mu$ is calculated ($A$ - $D$), we can see that, for a
particular observation, if $\mu$ is positive and large, the testee’s
ability will be much greater than the item’s difficulty, and he would
probably succeed on this item. On the other hand, if $\mu$ is negative
and small, the item difficulty would be much greater in this case, and
the testee would likely fail on this item. Hence, $\mu$ should be
directly related to probability, in that $\mu$ of large values result in
high probabilities of success on the items, whereas $\mu$ of small
values result in low probabilities of success. But how exactly is $\mu$
linked to probability? How can we map $\mu$ to probability in a
principled manner? The solution is to take advantage of the &lt;a href="https://en.wikipedia.org/wiki/Logistic_function">logistic
function&lt;/a>.&lt;/p>
&lt;p>$$
\text{logistic}( x ) = \frac{ 1 }{ 1 + e^{-x} }
$$&lt;/p>
&lt;p>The &lt;em>&lt;strong>logistic&lt;/strong>&lt;/em> is a function that maps a real number $x$ to a
probability $p$. In other words, the logistic function transforms the
input $x$ and constrains it to a value between zero and one. Note that
the transformation is &lt;strong>monotonic increasing&lt;/strong>, meaning that a smaller
$x$ would be mapped onto a smaller $p$, and a larger $x$ would be mapped
onto a larger $p$. The ranks of the values before and after the
transformation stay the same. To have a feel of what the logistic
function does, let’s transform some values with the logistic.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>&lt;span style="color:#75715e"># Set plot margins # (b, l, t, r)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(oma&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">0&lt;/span>)) &lt;span style="color:#75715e"># Outer margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>&lt;span style="color:#a6e22e">par&lt;/span>(mar&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">4.5&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>) ) &lt;span style="color:#75715e"># margin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>logistic &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>(x) &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">/&lt;/span> ( &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#a6e22e">exp&lt;/span>(&lt;span style="color:#f92672">-&lt;/span>x) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">seq&lt;/span>( &lt;span style="color:#ae81ff">-5&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>, by&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.1&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>p &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( x )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( x, p )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-4-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>As the plot shows, the logistic transformation results in an S-shaped
curve. Since the transformed values (p) are bounded by 0 and 1, extreme
values on the poles of the x-axis would be “squeezed” after the
transformation. Real numbers with absolute values greater than 4, after
transformations, would have probabilities very close to the boundaries.&lt;/p>
&lt;h4 id="less-math-less-confusion">Less Math, Less Confusion&lt;/h4>
&lt;p>For many students, the mathematical form of the logistic function leads
to confusion. Staring at the math symbols hardly enables one to arrive
at any insightful interpretation of the logistic. A suggestion here is
to let go of the search for such an interpretation. The logistic
function is introduced not because it is loaded with some crucial
mathematical or statistical meaning. Instead, it is used here solely for
a practical reason: to monotonically map real numbers to probabilities.
You may well use another function here to achieve the same purpose
(e.g., the &lt;strong>cumulative distribution function&lt;/strong> of the standard normal).&lt;/p>
&lt;h3 id="generating-responses">Generating Responses&lt;/h3>
&lt;p>We have gone all the way from ability/difficulty levels to the
probabilities of success on the items. Since we cannot directly observe
probabilities in the real world, the final step is to link these
probabilities to observable outcomes. In the case here, the outcomes are
simply item responses of zeros and ones. How do we map probabilities to
zeros and ones? Coin flips, or &lt;a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distributions&lt;/a>,
will get us there.&lt;/p>
&lt;p>Every time a coin is flipped, either a tail or a head is observed. The
Bernoulli distribution is just a fancy way of describing this process.
Assume that we record tails as &lt;code>0&lt;/code>s and heads as &lt;code>1&lt;/code>s, and suppose that
the probability $p$ of observing a head equals 0.75 (since the coin is
imbalanced in some way that the head is more likely observed and we know
it somehow). Then, the distribution of the outcomes (zero and one) will
be a Bernoulli distribution with parameter $P=0.75$. In graphical terms,
the distribution is just two bars.&lt;/p>
&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-5-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;p>We’ve got all we need by now. Let’s construct the remaining columns to
complete this simulation. In the code below, I compute the probabilities
(&lt;code>P&lt;/code>) from column &lt;code>Mu&lt;/code>. Column &lt;code>P&lt;/code> could then generate column &lt;code>R&lt;/code>, the
item responses, through the Bernoulli distribution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>rbern &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">function&lt;/span>( p, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">length&lt;/span>(p) )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span> &lt;span style="color:#a6e22e">rbinom&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>n, size&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, prob&lt;span style="color:#f92672">=&lt;/span>p )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4&lt;/span>&lt;span>&lt;span style="color:#a6e22e">set.seed&lt;/span>(&lt;span style="color:#ae81ff">13&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>P &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">logistic&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>Mu )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6&lt;/span>&lt;span>dat&lt;span style="color:#f92672">$&lt;/span>R &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( dat&lt;span style="color:#f92672">$&lt;/span>P ) &lt;span style="color:#75715e"># Generate 0/1 from Bernoulli distribution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7&lt;/span>&lt;span>dat
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code># A tibble: 6 x 7
I T A D Mu P R
&amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 A R 0.5 0.4 0.1 0.525 0
2 B R 0.5 0.1 0.4 0.599 1
3 A T 0.1 0.4 -0.3 0.426 0
4 B T 0.1 0.1 0 0.5 0
5 A J -0.4 0.4 -0.8 0.310 1
6 B J -0.4 0.1 -0.5 0.378 0
&lt;/code>&lt;/pre>
&lt;p>Now, we have a complete table of simulated item responses. A few things
to notice here. First, look at the fourth row of the data frame, where
the response of testee T (Tom) on item B is recorded. Column &lt;code>Mu&lt;/code> has a
value of zero since Tom’s ability level is identical to the difficulty
of item B. What does it mean to be “identical”? “Identical” implies that
Tom is neither more likely to succeed nor to fail on item B. Hence, you
can see that Tom has a 50% of getting item B correct in the $P$ column.
This is how the ability/difficulty levels and $\mu$ are interpreted.
They are on an abstract scale of real numbers. We need to convert them
to probabilities to make sense of them.&lt;/p>
&lt;p>The second thing to notice is column &lt;code>R&lt;/code>. This is the only column that
has &lt;em>&lt;strong>randomness&lt;/strong>&lt;/em> introduced. Every run of the simulation would
likely give different values of $R$ (unless a random seed is set, or the
&lt;code>P&lt;/code> column consists solely of zeros and ones). The outcomes are not
guaranteed, probability is at work.&lt;/p>
&lt;p>The presence of such randomness is the gist of simulations and
statistical models. We add uncertainty to the simulation, mimicking the
real world, to know that in the presence of such uncertainty, would it
still be possible to discover targets of interest with a statistical
model. Randomness, however, poses some challenges for coding. We need to
equip ourselves for those challenges.&lt;/p>
&lt;h2 id="coding-randomness">Coding Randomness&lt;/h2>
&lt;p>Randomness is inherent in simulations and statistical models, so it is
impossible to run away from it. It is everywhere. The problem with
randomness is that it introduces uncertainty in the outcome produced.
Thus, it would be hard to spot any errors just by &lt;strong>eyeballing the
results&lt;/strong>.&lt;/p>
&lt;p>Take &lt;code>rbern()&lt;/code> for instance. Given a parameter $P=0.5$, we can
repeatedly run &lt;code>rbern(0.5)&lt;/code> a couple of times to produce zeros and ones.
But these zeros and ones cannot tell us whether &lt;code>rbern(0.5)&lt;/code> is working
properly. &lt;code>rbern(0.5)&lt;/code> might be broken somehow, and instead generates
the ones with, say, $P=0.53$.&lt;/p>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large
numbers&lt;/a> can help us
here. Since &lt;code>rbern()&lt;/code> generates ones with probability $P$, if we run
&lt;code>rbern()&lt;/code> many times, the proportion of the ones in the outcomes should
converge to $P$. To achieve this, take a look at the second argument &lt;code>n&lt;/code>
in &lt;code>rbern()&lt;/code>, which is set here to repeatedly generate outcomes ten
thousand times. You can increase &lt;code>n&lt;/code> to see if the result comes even
closer to $0.5$.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> ) &lt;span style="color:#75715e"># Run rbern with P=0.5 10,000 times&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.4991
&lt;/code>&lt;/pre>
&lt;p>A more general way to rerun a chunk of code is through the for loop or
convenient wrappers such as the &lt;code>replicate()&lt;/code> function. I demonstrate
some of their uses below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1&lt;/span>&lt;span>outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{ &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span> ) } )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2&lt;/span>&lt;span>&lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>[1] 0.5027
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-r" data-lang="r">&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1&lt;/span>&lt;span>&lt;span style="color:#75715e"># See if several runs of rbern( p=0.5, n=1e4 ) give results around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2&lt;/span>&lt;span>Ps &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">replicate&lt;/span>( n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">100&lt;/span>, expr&lt;span style="color:#f92672">=&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3&lt;/span>&lt;span> outcomes &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">rbern&lt;/span>( p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, n&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1e4&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4&lt;/span>&lt;span> &lt;span style="color:#a6e22e">mean&lt;/span>(outcomes)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5&lt;/span>&lt;span>})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6&lt;/span>&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7&lt;/span>&lt;span>&lt;span style="color:#75715e"># Plot Ps to see if they scatter around 0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8&lt;/span>&lt;span>&lt;span style="color:#a6e22e">plot&lt;/span>( &lt;span style="color:#ae81ff">1&lt;/span>, type&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span>, xlim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">100&lt;/span>), ylim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">c&lt;/span>(&lt;span style="color:#ae81ff">0.47&lt;/span>, &lt;span style="color:#ae81ff">0.53&lt;/span>), ylab&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;P&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9&lt;/span>&lt;span>&lt;span style="color:#a6e22e">abline&lt;/span>( h&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.5&lt;/span>, lty&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;dashed&amp;#34;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10&lt;/span>&lt;span>&lt;span style="color:#a6e22e">points&lt;/span>( Ps, pch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">19&lt;/span>, col&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="part1_files/figure-commonmark/unnamed-chunk-9-1.svg"
style="width:100.0%" data-fig-align="center" />&lt;/p>
&lt;h3 id="randomness-in-models">Randomness in Models&lt;/h3>
&lt;p>The method described above works only for simple cases. What about
complex statistical models? How do we test that they are working as they
claim to? Guess what? &lt;em>&lt;strong>Simulation&lt;/strong>&lt;/em> is the key.&lt;/p>
&lt;p>We simulate data based on the assumptions of the statistical model and
see if it indeed returns what it claims to estimate. The simulation can
be repeated several times, each set to different values of parameters.
If the parameters are always recovered by the statistical model, we can
then be confident that the model is properly constructed and correctly
coded. So &lt;strong>simulation is really &lt;em>not an option&lt;/em> when doing
statistics&lt;/strong>. It is the only safety that helps us guard against bugs in
our statistical models, both programmatical and theoretical ones.
Without first testing the statistical model on simulated data, any
inferences about the empirical data are in doubt.&lt;/p>
&lt;h2 id="whats-next">What’s next?&lt;/h2>
&lt;p>In a real-world scenario of the example presented here, we would only
observe the score ($R$) of an item ($I$) taken by a testee ($T$). The
targets of interest are the unobserved item difficulty ($D$) and testee
ability ($A$). In &lt;a href="https://yongfu.name/irt2">Part 2&lt;/a>, we will work in reverse and fit
statistical models on simulated data. We will see how the models
discover the true $A$s and $D$s from the information of $R$, $I$, and
$T$. See you there!&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I mean, what the hack is &lt;em>Joint and Conditional Maximum Likelihood
Estimation&lt;/em>? These are methods developed in the psychometric and
measurement literature and are hardly seen in other fields. Unless
obsessed with psychometrics, I don’t think one would be able to
understand these things.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>r</category><category>stats</category><category>psychology</category><category>reproducibility</category></item><item><title>我的 R 開放課程</title><link>https://yongfu.name/2021/06/03/my-r-course/</link><pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate><guid>https://yongfu.name/2021/06/03/my-r-course/</guid><description>&lt;p>這學期 (109-2) 第二次擔任課程助教，給大學部的同學們上 R 語言。第二次教學，在熱情上減了一半，在教材難度上增加了一半。大概是因為不喜歡重複做一樣的事，這次課程刻意補了上一次 (&lt;a href="https://rlads2019.github.io/lab/">108-1&lt;/a>) 懶得教&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>、沒時間教&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>、沒自信可以教&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>以及沒有能力教&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>的內容。有了這些新的內容，課程準備起來就比較提得起勁，畢竟知道在去年已經教過的內容上，現在的我實在很難超越當時的自己&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>。不過，在準備教材的同時，也可以看到自己這一陣子的轉變：對某些概念的認識更加地完整、思緒變得更複雜迅速有條理。能發現這些真的蠻開心的，至少腦袋還是有長一些。擴增教材的另一個目的，是為了補齊上一次未能 (學會然後) 教的缺憾，大概是一種想把圓畫完整的感覺，當然圓不可能畫得完美&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>，不過整體來說，完整的感覺還是大過缺憾許多，也蠻值得開心的。&lt;/p>
&lt;p>這次的課程全部採取事前錄製影片的方式授課，表面上的目的是擔心遠距教學的情形再度出現 (結果真的出現了&amp;hellip;)，實際上的目的則是跟剛剛一樣：我不想要做同樣的事情 (實體授課)，而且這次實體授課大概也不會講得比上次好，所以不如換一種方式授課&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>。結果無心插柳，最後幾堂課因為突來的疫情改成遠距教學，對我反倒沒造成什麼影響，又是件值得開心的事情。&lt;/p>
&lt;p>在經歷 12 個頗為漫長的週末，終於完成了這個算是完整的 R 語言課程。在這 12 堂課中，或許值得慶幸的一件事情是課程內容主題都不是 state-of-the-art。在能夠選擇時，選擇了去講比較穩定不變的東西、比較不會經過兩三年後就變成歷史名詞的技術或概念。這麼做或許可以讓課程的保鮮期變得比較長，也或許可以讓更多人從這個課程中受惠。這 12 堂課程的完整內容 (影片/簡報/講義/作業/程式碼) 可以在這個&lt;a href="https://rlads2021.github.io/archives/">頁面&lt;/a>取得，歡迎讀者自行使用、分享或是修改並應用於教學之中。&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://img.yongfu.name/posts/rlads2021.png" alt="2021 課程影片">
&lt;figcaption>2021 課程影片&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>整個學期下來，雖然過得平淡、沒有第一次授課時那種短時間內大量成長的感覺，不過反倒是有種&lt;strong>比較完整地完成了一件事情&lt;/strong>的感覺。這種感覺不會讓人大喜大悲、異常緊張或是過度興奮，但會讓心裡變得厚實舒坦，讓內心安穩一些，然後微微的喜悅就會從心底緩慢而持續地湧出。&lt;/p>
&lt;p>最後，還要特別感謝 Andrea、Yulin、Mao-Chang 以及 Amber 在各個方面的協助，這門課的學生能有你們的照顧真的很幸運。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Lab01: &lt;a href="https://rlads2021.github.io/LabBook/ch01">路徑、終端機、R101&lt;/a> (絕對與相對路徑 &amp;amp; Terminal)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Lab12: &lt;a href="https://rlads2021.github.io/LabBook/ch12">專案成果展示&lt;/a> (Shiny)&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Lab09: &lt;a href="https://rlads2021.github.io/LabBook/ch09">文本與詞彙的向量表徵&lt;/a> (document-term matrix &amp;amp; latent semantic analysis)&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Lab06: &lt;a href="https://rlads2021.github.io/LabBook/ch06">Simulating Data with R&lt;/a> (Causal inference 101)&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>那時多有熱情和自信啊！反觀現在真的很難被激勵，不過往好處想或許這代表掌握情緒的能力有所提昇？&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>像是這次在教 Web API 時拿來示範用的 Public API 在我上傳教學影片之後就關閉服務了&amp;hellip;&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>這樣就不用和過去的自己硬碰硬比較，擔心自己退步了 (反正我有錄影片，觸及的人就是比較廣比較潮啦！108-1 的你輸了啦哈哈哈哈哈)。比較不見得會進步，但一定會帶來傷害，所以換個方式讓自己沒辦法去比較或許也不錯。&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description><category>R</category><category>Course</category><category>stats</category><category>linguistics</category><category>中文</category></item></channel></rss>