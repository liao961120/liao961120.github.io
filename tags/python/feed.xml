<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Yongfu's Blog</title><link>https://yongfu.name/tags/python/</link><description>Recent content in Python on Yongfu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/python/feed.xml" rel="self" type="application/rss+xml"/><item><title>A Minimalist Structure for Snakemake</title><link>https://yongfu.name/2023/02/15/snakemake/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><guid>https://yongfu.name/2023/02/15/snakemake/</guid><description>I have heard of the use of GNU Make for enhancing reproducibility for some time. I did not incorporate Make into my work however, since a simple build script written in Bash was sufficient. Everything was well in control, and I could structure the workflow to my will.
It was not until I started working in a company setting that I found most things out of my control. Decades of conventions have been accumulating and passing on, and personal workflows have to fit into existing ones.</description></item><item><title>Searching Interlinear Glosses Written in Word Documents</title><link>https://yongfu.name/2020/04/23/gloss-search/</link><pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/04/23/gloss-search/</guid><description>I am taking the course Linguistic Fieldwork this semester. Each week, we record and transcribe Budai Rukai, an Austronesian language spoken by Rukai people (魯凱族). The resulting data (interlinear glosses) are written in a Word document (.docx) as required by the course instructor. Things get worse as the number of documents accumulates each week, since it becomes harder to locate specific linguistic patterns in the corpus of texts, as they are spread across multiple documents.</description></item><item><title>以 Python 實作 Concordancer</title><link>https://yongfu.name/2020/03/20/building-concordancer/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://yongfu.name/2020/03/20/building-concordancer/</guid><description>每次接近學期末的時候，寫程式癮就會開始發作 (可能是不想面對無趣的期末報告)，這時候腦袋會蹦出許多很有趣的想法，然後就會迫不及待地想將這些想法實作出來。這次(2019 年末) 的程式癮刺激來源是實驗室的雲端硬碟裡的某個 (版權封閉) 中文語料庫，雖然該語料庫已有很好的搜尋界面，但我就是想 reinvent the wheel，自己手刻出一個 concordancer。不為了什麼，就只是因為這件事本身就很有樂趣。
初步嘗試：for loop&amp;hellip; forever 我本來並沒有太大的雄心壯志，就只想快速弄出個程式界面方便我查找 concordance，想說使用 NLTK concordance 應該很快就可以弄出我想的東西。但 NLTK concordance 只能使用 word form (或 pattern) 去搜尋 concordance，我的需求卻是要能使用 word form 或 PoS tag 搜尋語料庫 (類似 Corpus Query Langauge1，但不用這麼複雜)。但要自己用 Python 實作這個功能也頗簡單，於是我就自己手刻了這個功能。然而事實證明我太過天真了。語料庫的大小約 1000 萬個 token，而每次搜尋時，我的程式使用 for 迴圈跑過整個語料，因此要花非常非常非常久的時間才能完成搜尋。對於非資訊背景出生的我，第一次體驗 $O(n)$ 是件不可忽視的問題以及 Database 存在的必要性。
重新規劃： Database + Python + Vue 為了解決上述問題我暫時擱置了這個專案 (寒假開始到春節期間) 去學習必備的一些知識2，最後比較有系統地重新規劃了這個 concordancer 的架構：
這個新的架構分成前3、後端，前端不是本文的重點 (原始碼在此)，就不細談。這邊直接舉一個實例說明這個 concordancer 如何運作：
首先，使用者在前端輸入一個搜尋的字串 (keyword)，這個字串需符特定的格式：[token 1][token 2][token 3]。每對中括號代表一個 token，中括號內則是描述此 token 的特徵，如 word form 與 PoS tag，例如 [word=&amp;quot;打&amp;quot; pos=&amp;quot;V.</description></item></channel></rss>