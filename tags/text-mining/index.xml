<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Text Mining on Yongfu&#39;s Blog</title><link>https://yongfu.name/tags/text-mining/</link><description>Recent content in Text Mining on Yongfu&#39;s Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 31 Jul 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://yongfu.name/tags/text-mining/index.xml" rel="self" type="application/rss+xml"/><item><title>jieba 自訂詞庫斷詞</title><link>https://yongfu.name/2018/07/31/jieba-dict/</link><pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/07/31/jieba-dict/</guid><description>在進行中文 Text Mining 前處理時，必須先經過斷詞處理。社群當中存在相當好的斷詞處理工具，如 jieba。但斷詞時常遇到一個問題：文本中重要的詞彙因為不常見於其它地方而被斷開，像是人物角色名稱。要處理這個問題，需將自訂詞庫提供給斷詞套件，才不會將重要詞彙斷開。 這邊將使用 jiebaR，介紹使用自訂詞庫的斷詞方式，並提供自訂詞庫的製作方式。
示範語料 這裡使用金庸神雕俠侶第三十二回 — 情是何物作為斷詞的文本。武俠小說在此是個很好的例子，因為裡面有許多人物名稱和專有名詞。
因為著作權問題1，語料的原始檔(032.txt)將不會出現在本文的 GitHub repo 中。
製作自訂詞庫 取得小說這類文本的角色名稱與特殊名詞乍看之下可能非常耗工耗時，但有些時候其實相當容易，尤其是著名的小說。這要歸功於維基百科，因為越是著名的小說，其越有可能有詳盡的維基百科頁面，而維基百科對製作詞庫最重要的特色在於其頁面的超連結，因為通常只有專有名詞才會成為一個維基頁面上的超連結。
這邊使用維基百科的神鵰俠侶角色列表作為詞庫的來源。以下使用rvest套件清理此頁面：
library(rvest) library(dplyr) library(magrittr) library(knitr) path &amp;lt;- &amp;quot;神鵰俠侶角色列表.html&amp;quot; # 這裡已先行下載網頁，若無可直接使用網址 data &amp;lt;- read_html(path) %&amp;gt;% html_nodes(&amp;quot;ul&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;li&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text() 觀察頁面後，可發現多數與小說相關的詞彙都位在 unordered list 下的連結內文(&amp;lt;a&amp;gt; tag)，因此透過 3 個html_nodes()取得連結，並用html_text()擷取連結內文。
接著看看擷取的詞彙，可以發現這些詞彙依照順序大致可區分成三個來源：
自維基頁面的目錄擷取之連結 內文的連結(這是我們要的) 其它連結 對應至頁面最下方，與小說有關但並非小說主要內容的連結，如，「射雕英雄传角色列表」。另外，也包含維基百科頁面的固定連結，如「編輯」、「討論」、「下載為PDF」等。 data &amp;lt;- unique(data) data[1:3] [1] &amp;quot;1 主角&amp;quot; &amp;quot;2 桃花島&amp;quot; &amp;quot;2.1 「北丐」門派&amp;quot; data[21:25] [1] &amp;quot;楊過&amp;quot; &amp;quot;射鵰英雄傳&amp;quot; &amp;quot;楊康&amp;quot; &amp;quot;穆念慈&amp;quot; &amp;quot;全真教&amp;quot; data[207:211] [1] &amp;quot;射雕英雄传角色列表&amp;quot; &amp;quot;倚天屠龙记角色列表&amp;quot; &amp;quot;查&amp;quot; [4] &amp;quot;论&amp;quot; &amp;quot;编&amp;quot; 我們要的內容介在data[21](楊過)至data[206](樊一翁)之間。此外，亦可手動加入連結中沒有的詞彙：</description></item><item><title>Text Mining 前處理</title><link>https://yongfu.name/2018/07/28/quanteda-tutorial/</link><pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate><guid>https://yongfu.name/2018/07/28/quanteda-tutorial/</guid><description>中文 Text Mining 的前處理比起其它以拉丁字母為主的文本困難許多，參考資源也相對龐雜不全。 這裡以較晚近出現的quanteda套件為根據，依其需求進行中文文本前處理。
選擇quanteda而非其它較流行的套件如tm的原因是因為其多語言支持較佳，譬如其內建的 tokenizer 能直接對中文進行斷詞。然而，由於 jieba的社群資源以及斷詞效果較佳，此文還是以jiebaR進行斷詞。
此外，因為使用的語料是簡體字，這裡也提到簡體、繁體轉換處理的相關資源。 我希望這篇文章能整理出一套中文文本前處理的架構，試圖減輕未來可能遇到的問題。
流程 graph LR html(&#34;HTML&#34;) html -.-|&#34;rvest&#34;| df0 subgraph 前處理 df1(&#34;斷詞 data_frame&#34;) df0(&#34;data_frame&#34;) df0 -.-|&#34;
jiebaR (保留標點)
&#34;| df1 df1 -.-|&#34;ropencc 簡轉繁&#34;| df1 end corp(&#34;Corpus&#34;) token(&#34;Tokens&#34;) subgraph quanteda df1 -.-|&#34;quanteda corpus()&#34;| corp corp -.-|&#34;quanteda tokenize()&#34;| token end html -.- bls(&#34; &#34;) style bls fill:none,stroke:none style html fill:#ccbdb9 style df1 fill:#92ff7f linkStyle 5 stroke-width:0px,fill:none; 資料爬取 這邊使用 RStudio 軟體工程師 Yihui 的中文部落格文章作為練習素材。首先需要取得文章的網址，因此先到部落格的文章列表頁面(https://yihui.</description></item></channel></rss>