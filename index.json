[{"content":"I recently found myself in a situation similar to many international backpackers in Taiwan. First, I usually travel to small towns or rural areas. Second, I do not have a driver’s license for scooters or motorbikes—public transport is all I can depend on when traveling.\nIn cities other than Taipei (台北), Taichung (台中), and Kaohsiung (高雄) in Taiwan, public transport is limited to railway (台鐵) and buses (公車). In more rural areas, one is only left with buses. Buses arrive infrequently in these areas, and it is quite often to have only 2 or 3 arrivals per day at a particular stop. Consequently, it is necessary to plan ahead when traveling to these places. The hard part though, is that bus schedules are difficult to find on the internet (even for local people), and even if you manage to find one, it might still be outdated.\nLuckily, there is a mobile app Bus+ (https://busplus.app), available in both Andriod and iOS, that integrates public transport information (including bus, MRT, Railway, and Ubike) across all cities and counties in Taiwan. It provides real-time updates for the bus waiting time as well as the route, the stop locations, and the full schedule. In addition, an English interface is available. Personally, I use the app daily and find the bus waiting time highly accurate.\nWhen traveling to unfamiliar places, using Google Maps in combination with Bus+ can be extremely helpful. The route-planning information given by Google is correct for the most part, but the bus arrival time is often inaccurate. Simply search for the bus in Bus+, and you can retrieve more accurate information. Take a while to first explore the app though, there are many hidden gems to be discovered, and it could take some time.\nTaitung’s Bus Travel I often travel to Taitung (台東) and find bus travel there quite convenient. Here, I’ll share some of my experiences.\nThe main traffic routes in Taitung County are primarily divided into two sections by the Coastal Mountain Range (i.e., Hai’an Range 海岸山脈). There is the so-called ‘Mountain Line (山線)’, located in the Huadong Valley (花東縱谷), and the ‘Coastal Line (海線)’ on the other side of the Coastal Mountain Range, facing the Pacific Ocean. The railway traverses the Mountain Line but not the Coastal Line. Thus, individuals relying on public transport can only travel to towns situated along the Coastal Line by bus.\nDespite not being well-known, Taitung possesses a reasonably well-developed bus network. Passengers can conveniently settle their bus fare using an EasyCard, and the cost is notably economical.\nTo travel between Taitung Station (railway station) and Taitung Bus Station (downtown Taitung), one can take: 8101 / 8103 / 8109 / City Tour Circular (市區循環) / Train-Harbor-Airport Line B (陸海空線B) Note that for ‘City Tour Circular’ and ‘Train-Harbor-Airport Line B’, you need to search them by their Chinese names (just copy-and-paste) to find them in Bus+ To travel between Taitung City (railway station or downtown Taitung) and the towns along the Coastal Line (e.g., Fugang Harbor 富岡漁港, Dulan 都蘭, Chenggong 成功), you might take (depending on the destination): 8101 / 8102 / 8103 / 8109 / 8120 ","subtitle":"A Fantastic Bus Travel App for International Backpackers","title":"Navigating Taiwan's Public Transport","uri":"/2023/11/13/taiwan-public-transport/"},{"content":"My love for the base R plotting system has been growing since I started dealing with complicated charts. Complexity in these charts can arise for many reasons. For one, it might simply result from the data structure when working with complex statistical models (e.g., multilevel models). Conventions in the workplace could be another source of complexity. For instance, tweaking a chart in R to replicate the one originally created in Excel could be extremely difficult, and it is nearly impossible to do so with higher-level plotting packages such as ggplot2. To approach the look of a chart created outside of R, life would be much easier if lower-level frameworks such as the base R plotting system were utilized. Below is just a cumulation of charts I’ve created. It is intended to help me search and locate the code for plotting certain features in base R. The remainder of the post is divided into sections by charts, each of which consists of (1) the code, (2) the output chart, and (3) a list of features presented in the chart. Scatter plots Features Text on plot margins: mtext() 1#### Data #### 2set.seed(2023) 3subj_idx = 1:30 4gender = rep(1:2, each=15) 5heights = c(160, 175)[gender] + rnorm(30, sd=15) 6 7#### Plot #### 8ylim = c( min(heights)-5, max(heights)+5 ) 9plot( 1, type=\"n\", xlim=c(.5,30.5), ylim=ylim, 10 xlab=\"Subject Index\", ylab=\"Height (cm)\" ) 11points( 1:15, heights[ 1:15], col=2, pch=19 ) 12points( 16:30, heights[16:30], col=4, pch=19 ) 13abline( v=15.5, col=\"grey\", lty=\"dashed\" ) 14mtext( c(\"Girls\",\"Boys\"), at=c(7,23), col=c(2,4), padj=-.5 ) Bar Charts Features custom axis: axis() axis label rotation (horizontal): las = 1 axis label padding adjustment: hadj, padj number of digits: spintf(\"%.2f\", yseq) Code \u0026 Plot 1yseq = seq(0, 1, .25) 2plot(1, type=\"n\", xaxt='n', yaxt='n', 3 ylab = \"Probability\", xlab=\"\", 4 xlim=c(-.7, 1.7), ylim=c(0, 1) ) 5lines( c(0, 0), c(0, 0.25), lwd=12, col=2 ) 6lines( c(1, 1), c(0, 0.75), lwd=12, col=2 ) 7axis( 2, at=yseq, labels=sprintf(\"%.2f\", yseq), las=1, hadj = .85 ) 8axis( 1, at=0:1, padj = .5, 9 labels = c(\"0\\n(tail)\", \"1\\n(head)\"), 10) Interaction Plots Features Legend outside of plotting region: par(), legend(), mar, xpd, inset Shaded region: polygon() Custom axis (categorical axis): axis() text / label: text() Code \u0026 Plot 1library(stom) 2library(dplyr) 3 4#### Data #### 5d = iris |\u003e 6 group_by(Species) |\u003e 7 summarise( 8 S.L = mean(Sepal.Length), 9 S.W = mean(Sepal.Width), 10 P.L = mean(Petal.Length), 11 P.W = mean(Petal.Width) 12 ) 13d 14 15#### Annotations #### 16(LABELS = colnames(d)[-1]) # A tibble: 3 x 5 Species S.L S.W P.L P.W 1 setosa 5.01 3.43 1.46 0.246 2 versicolor 5.94 2.77 4.26 1.33 3 virginica 6.59 2.97 5.55 2.03 [1] \"S.L\" \"S.W\" \"P.L\" \"P.W\" 1#### Plot #### 2# c( b, l, t, r ) 3par( mar=c(5.1, 4.1, 4.1, 8.1), xpd=F ) # Larger right margin for legend 4plot( 1, type=\"n\", xlim=c(.5, 4.5), ylim=c(0,7), 5 xlab=\"\", ylab=\"Mean\", 6 xaxt=\"n\", yaxt=\"n\") # disable x/y-axis 7# Shaded region (coordinates are specified (counter-)clockwise) 8polygon( c(0,4.9,4.9,0),c(2,2,5,5), col=col.alpha(\"grey\",.15), lty=2, border=\"grey\" ) 9# Auxiliary lines 10for ( h in 0:7 ) 11 abline( h=h, col=col.alpha(\"grey\") ) 12# Lines \u0026 Points 13for ( i in 1:nrow(d) ) { 14 lines ( 1:4, d[i,-1], col=i, lwd=4 ) 15 points( 1:4, d[i,-1], col=i, lwd=4, pch=2+i ) 16} 17# Labels 18for ( i in 1:nrow(d) ) { 19 if ( i != 2 ) next 20 text( 1:4-.03, d[i,-1]-.45, labels = d[i,-1], cex=.9, col=i ) 21} 22# Axis 23axis( 1, at=1:4, tck=-.02, labels=LABELS ) 24axis( 2, at=0:7, labels=sprintf(\"%.1f\", 0:7), las=1 ) # rotated y-axis labels 25# Legend (outside of plotting region) 26legend(\"right\", legend=d$Species, inset=c(-.17,0), xpd=TRUE, 27 col=1:nrow(d), pch=1:nrow(d) + 2, lwd=4, cex=.9, 28 y.intersp=1.8, box.col=\"transparent\", bg=\"transparent\" ) Density Plots Features Line segments: segments() Density: density() text / label: text() Code \u0026 Plot 1library(stom) 2library(dplyr) 3 4#### Data #### 5set.seed(2020) 6d = data.frame( 7 x = rnorm(2000) 8) 9 10#### Annotations #### 11X = d$x 12AVG = mean(X) 13SD = sd(X) 14 15#### Plot #### 16plot( density(X), ylim=c(0,.46), 17 main = \"A Standard Normal\", xlab = \"X\" ) 18# Mean line segment 19segments( x0=AVG, y0=0, y1=.43, col=2, lwd=3 ) 20text( AVG, .46, labels = paste(\"AVG:\",round(AVG,3)), col=2, cex=.8 ) 21# SD line segment 22segments( x0=AVG+.06, x1=AVG+SD, y0=.02, lwd=2, col=4 ) 23segments( x0=AVG+.06, y0=.01, y1=.03, lwd=2, col=4 ) 24segments( x0=AVG+SD, y0=.01, y1=.03, lwd=2, col=4 ) 25text( .5*(AVG+SD+.06), .035, 26 labels = paste(\"SD =\",round(SD,3)), col=4, cex=.8 ) Combining Plots Features Plot panels: par() + mfrow() Code \u0026 Plot 1inner = c(4.1, 4.1, 1.8, .5) 2par( mfrow=c(2,2), oma=c(0,0,0,0), mar=inner ) 3 4for ( i in 1:4 ) { 5 plot( 1, type=\"n\", xlim=0:1, ylim=0:1, xlab=\"\", ylab=\"\" ) 6 text( .5, .5, labels=paste(\"Plot\",i), cex=2 ) 7} ","subtitle":"Some code sketches for Base R plotting","title":"A Diary of Charts","uri":"/2023/10/27/base-plot/"},{"content":"My previous four posts have focused on basic item response models1. These models are commonly found in an educational testing context but are less often seen in clinical research settings where questionnaires are used for the measuring and assessment of individuals’ conditions. Due to the higher level of complexity in the study design of clinical studies, IRT models are often discarded for the analyses of questionnaires. Simple sum scores of the questionnaires, instead of the more robust estimates derived from IRT models, are taken directly to represent the quantity of the latent constructs. This has undesirable effects of introducing bias and overconfidence and ignoring uncertainty in measuring these latent constructs. There is no need to trade measurement inefficiency for study design complexity. Complex models arising from complicated study designs can be naturally extended to incorporate IRT models that deal with latent construct measurement, thus enhancing the quality of the study. In this post, we will walk through such an example by building up a model for analyzing individuals’ changes—in terms of latent variables—over time. A Bayesian framework will be adopted, and we will be working with Stan code directly. As always, we begin with simulations. But since the model we are building is quite complicated—as any model trying to mirror reality is—I’ll first provide some context. Treatment of Alcohol Dependence Suppose some researchers were carrying out a study to examine the effectiveness of treatments on alcohol dependence. Individuals were recruited and randomly assigned to one of the three treatment conditions. All treatments lasted for three months, during which the participants were assessed for the effectiveness of the treatments to track their clinical progress. The assessments included collecting data such as measures of treatment outcomes (e.g., days of heavy drinking in the past 14 days) and mediating factors on treatment effectiveness suggested by previous studies (e.g., self-efficacy). It was hypothesized that treatments for alcohol dependence work partially through raising individuals’ self-efficacy in controlling alcohol use. Hence, during the assessments, questionnaires on self-efficacy in alcohol control were also administered to measure this potential mediating factor. Causal assumptions of the alcohol dependence treatment study The DAG above explicates the causal assumptions of this fictitious study2. Here’s the description of the variables in the DAG. $E$: Participants’ self-efficacy on alcohol use control Since self-efficacy $E$ is not directly observed, it is represented as a circled node in the DAG. $R$: Item responses collected through self-efficacy questionnaires To measure the unobserved self-efficacy $E$, tools like questionnaires are required to measure such a latent construct. $R$ stands for the responses collected through the questionnaire. These responses would allow the estimation of the variable $E$ for each participant. Note that the item parameter $I$ is left out for simplicity. If present, it would point to $R$ as item estimates also affect the responses $R$. $A$: Participants’ age $T$: Treatment condition received by a participant $D$: Latent treatment outcome $D$ is the latent quantity that underlies the (observed) treatment outcome. $D^{\\ast}$: Treatment outcome. Here, it is the days of heavy drinking in the past 14 days. The arrows among the nodes in the DAG indicate the directions of influence. Therefore, the graph is basically saying that the treatments affect the outcomes through two pathways. One direct, and the other indirect, through self-efficacy. Age also has direct influences on self-efficacy and the treatment outcome. The labels on the edges mark the regression coefficients, which are the parameters of interest for our later simulations and model testing. The DAG presented above ignores the time dimension for simplicity. The second DAG below includes it. To avoid cluttering the graph, only three, instead of four, time points are shown. The subscripts on the variables mark the time points. $t=0$ indicates the baseline (i.e., the first) assessment. A cautionary note here is that age only directly influences self-efficacy and the latent treatment outcome at baseline ($A \\rightarrow E_0, D_0$). At subsequent time points, they are influenced by age only indirectly through $E_0$ and $D_0$. This slight complication will become clearer in the following description of the simulation. Causal assumptions of the alcohol dependence treatment study (simplified illustration of three time points). Simulating Treatments With the background provided, let’s now simulate the above scenario. The code for the simulation, as well as later model fitting, will be inevitably long. It’s a natural consequence of realistic stats modeling. This post is thus less of a pedagogical step-by-step guide and more of a conceptual demonstration of practical real-world data analyses. That said, intricate details of code implementation won’t be hidden and are laid out as is. If anything seems mysterious, the key to demystifying it is to run some code. Experimentation is an ally that you should always trust. Efficacy and Treatment Outcome Let’s first sketch out the overall scenario by simulating 30 participants for each treatment condition. The age, gender, and the assigned treatment condition of each participant are saved in the variable A, G, and Tx, respectively. I also simulated a “baseline” for each of the participants. These baselines are the quantification of individual differences and can be more or less thought of as the amount of efficacy possessed by the individuals before receiving any treatment. 1# remotes::install_github(\"liao961120/stom\") 2library(stom) 3set.seed(1977) 4 5Ntx = 3 # number of treatments 6Ns = Ntx * 30 # number of subjects 7Nt = 4 # number of time points 8 9Tx = rep(1:Ntx, each = Ns / Ntx) # Treatment condition for each subj 10G = rep(1:2, times = Ns / 2) # Gender of each subj 11A = rtnorm( Ns, m = 36, lower = 18, upper = 80, s = 20 ) # Age 12tau = 0.8 # std for Subject baseline Efficacy 13subj = rnorm(Ns, 0, tau) # Subject baseline Efficacy (subject intercept) 14 15# Transform Age to a reasonable scale 16minA = min(A) 17A = (A - minA) / 10 # 1 unit = 10 years in original age The simulation of the participants’ age deserves some elaboration. These ages are drawn from a truncated normal distribution, which is essentially a normal distribution with an upper and a lower bound applied. The boundaries are set here such that the participants come from a normal distribution with a mean age of 36 and a standard deviation of 20, with ages below 18 or over 80 left out from the sample3. After drawing samples for the ages, the ages are scaled such that (1) the youngest participant has a scaled age of zero, and (2) a unit of difference in the scaled age corresponds to a 10-year difference in the raw age. This is done to align the scale of the age to the scales of the other variables. Otherwise, the original large scale of the age would make the effects difficult to interpret. Now, let’s lay out the parameters for generating participants’ self-efficacy ($E$) and treatment outcomes ($D$): 1B_AE = .1 # Effect of Age on Efficacy 2B_TE = matrix(c( # Effect of Treatment on Efficacy 3 .3, .7, 1.3, 4 .3, 1, .7 5 ), byrow=T, nrow=2 6) 7B_AD = .2 # Effect of Age on Outcome 8B_ED = 1 # Effect of Efficacy on Outcome 9B_TD = c(0, 0, 0) # Effect of Treatment on Outcome 10 11delta = -1.8 # Global Intercept for \"E model\" 12alpha = 0 # Global Intercept for \"D model\" As mentioned previously, it is assumed that the treatments differentially affect efficacy according to gender (i.e., the treatment effect on efficacy interacts with gender). This is specified in B_TE ($\\beta_{TE}$) as a 2-by-3 matrix. The first row of B_TE corresponds to male and the second to female. The three columns correspond to Treatment 1 (Tx == 1), 2 (Tx == 2), and 3 (Tx == 3) respectively. For the direct treatment effects on the outcomes ($\\beta_{TD}$), I set them all to zero and assume no interaction with the gender, to keep things simple. With all these parameters prepared, we can build up the generative process of efficacy and the outcomes. As shown in the code chunk below, we first set up the time points t. t starts with zero because it is assumed that the first assessment took place right before the treatments started. Hence, any treatment effects at this point of measure should be zero. t = 0 does us the favor as it naturally cancels out the treatment effects $\\beta_{TD}$ and $\\beta_{TE}$. Let’s now simulate self-efficacy $E$. Before receiving the treatments, the efficacy is assumed to be slightly influenced by age, through B_AE. In addition, baseline individual differences are modeled through the random subject intercept subj. After receiving the treatments, treatment effects get added on through the term B_TE. Efficacy is then modeled as the sum of the aforementioned effects. After generating participants’ efficacy, we can again follow the arrows of the DAGs to write down the generative process of $D^{\\ast}$, coded as D_latent below. Finally, the treatment outcomes here, days of heavy drinking in the past 14 days, are assumed to follow a binomial distribution $D \\sim \\text{Binomial}(14, p)$. The latent score D_latent is reversed with the negative sign since we want higher latent scores to result in fewer heavy drinking days. Next, we simply map the latent scores to probabilities with the logistic function. 1t = 0:(Nt - 1) # time points of measure 2 3# E model (causes of E) 4E = sapply(t, function(time) { 5 b_TE = sapply( 1:Ns, function(i) B_TE[ G[i],Tx[i] ] ) 6 delta + subj + B_AE * A + b_TE * time 7}) 8 9# D model (causes of D) 10D_latent = sapply(t, function(time) { 11 alpha + B_TD[Tx]*time + B_AD * A + B_ED * E[, time + 1] 12}) 13D = sapply(t, function(time) { 14 mu = -D_latent[, time+1] 15 rbinom( Ns, size=14, prob=logistic(mu) ) 16}) Now, let’s collect the values we have simulated into a long-form data frame. Each row of the data frame corresponds to a response ($D$) of a participant collected at a specific time point. I will name this data frame the (treatment-)outcome-level data frame. Later, we will see another data frame that records item-level responses. 1# Outcome-level responses (subject-time) 2dO = expand.grid( Sid=1:Ns, time=t, KEEP.OUT.ATTRS=F ) 3dO$A = with( dO, A[Sid] ) 4dO$Tx = with( dO, Tx[Sid] ) 5dO$G = with( dO, G[Sid] ) 6dO$D = NA 7dO$D_latent = NA 8dO$E = NA 9for ( i in 1:nrow(dO) ) { 10 s = dO$Sid[i] 11 t_ = dO$time[i] + 1 12 dO$E[i] = E[s, t_] 13 dO$D_latent[i] = D_latent[s, t_] 14 dO$D[i] = D[s, t_] 15} 16str(dO) 'data.frame': 360 obs. of 8 variables: $ Sid : int 1 2 3 4 5 6 7 8 9 10 ... $ time : int 0 0 0 0 0 0 0 0 0 0 ... $ A : num 1.57 2.58 1.21 0.92 0 ... $ Tx : int 1 1 1 1 1 1 1 1 1 1 ... $ G : int 1 2 1 2 1 2 1 2 1 2 ... $ D : int 13 9 6 11 12 13 7 12 13 12 ... $ D_latent: num -2.534 -1.132 0.286 -0.942 -2.003 ... $ E : num -2.8466 -1.6485 0.0451 -1.1261 -2.0029 ... Item Responses Item responses are generated similarly to $D$ since they both depend on efficacy $E$. Let’s assume the questionnaire for measuring participants’ efficacy contains 20 items (Ni = 20), and that the easiness of the items is equally spaced and ranges from -.6.3 to 6.3. With the item easiness and a baseline choice preference kappa set, we now can generate the item responses $R$ from the participants’ efficacy. 1Ni = 20 # number of items 2I = seq(-4.5, 4.5, length = Ni) # item easiness (sums to zero) 3kappa = logit(cumsum(simplex(c(1, 2, 4, 4, 2, 1)))) 4kappa = kappa[-length(kappa)] 5 6# Item-level responses (subject-item-time) 7dI = expand.grid( Sid=1:Ns, Iid=1:Ni, time=t, KEEP.OUT.ATTRS=F ) 8for (i in 1:nrow(dI)) { 9 dI$R[i] = with(dI, { 10 rordlogit(E[Sid[i], time[i] + 1] + I[Iid[i]], kappa = kappa) 11 }) 12} 13str(dI) 'data.frame': 7200 obs. of 4 variables: $ Sid : int 1 2 3 4 5 6 7 8 9 10 ... $ Iid : int 1 1 1 1 1 1 1 1 1 1 ... $ time: int 0 0 0 0 0 0 0 0 0 0 ... $ R : int 1 1 1 1 1 1 1 1 1 1 ... Wrapping up When a simulation gets massive, it is always better to pack up all the code into a function. There will certainly be times when we have to modify the structure of, or the parameter values in, the simulation. A function helps a lot in these situations. The simulation code we have gone through so far is condensed into the sim_data() function in simulation.R. You can just load it and run the simulation: 1source(\"simulation.R\") 2d = sim_data() # Change param vals by overwriting default args 3str(d) List of 3 $ dat :List of 18 ..$ Ns : num 90 ..$ Ntx : num 3 ..$ Nt : num 4 ..$ Nk : num 6 ..$ Ni : num 20 ..$ NI : num 7200 ..$ Sid_I : int [1:7200] 1 2 3 4 5 6 7 8 9 10 ... ..$ Iid_I : int [1:7200] 1 1 1 1 1 1 1 1 1 1 ... ..$ time_I : int [1:7200] 0 0 0 0 0 0 0 0 0 0 ... ..$ R : int [1:7200] 1 1 1 1 1 1 1 1 1 1 ... ..$ NO : num 360 ..$ Sid_O : int [1:360] 1 2 3 4 5 6 7 8 9 10 ... ..$ time_O : int [1:360] 0 0 0 0 0 0 0 0 0 0 ... ..$ G : int [1:360] 1 2 1 2 1 2 1 2 1 2 ... ..$ A : num [1:360] 1.947 5.137 0.634 2.086 5.84 ... ..$ Tx : int [1:360] 1 1 1 1 1 1 1 1 1 1 ... ..$ D : int [1:360] 5 9 13 6 8 14 6 14 9 11 ... ..$ D_latent: num [1:360] -0.534 -2.038 -1.235 -0.354 -0.725 ... $ params:List of 13 ..$ alpha : num 0 ..$ delta : num -1.8 ..$ B_AE : num 0.1 ..$ B_TE : num [1:2, 1:3] 0.3 0.3 0.7 1 1.3 0.7 .. ..- attr(*, \"dimnames\")=List of 2 .. .. ..$ : chr [1:2] \"male\" \"female\" .. .. ..$ : NULL ..$ B_AD : num 0.2 ..$ B_ED : num 1 ..$ B_TD : num [1:3] 0 0 0 ..$ E : num [1:90, 1:4] -0.923 -3.065 -1.362 -0.772 -1.893 ... .. ..- attr(*, \"dimnames\")=List of 2 .. .. ..$ : chr [1:90] \"male\" \"female\" \"male\" \"female\" ... .. .. ..$ : NULL ..$ I : num [1:20] -4.5 -4.03 -3.55 -3.08 -2.61 ... ..$ sigma_I: num 2.8 ..$ kappa : num [1:5] -2.56 -1.3 0 1.3 2.56 ..$ tau : num 0.8 ..$ subj : num [1:90] 0.682 -1.779 0.374 0.82 -0.677 ... $ others:List of 3 ..$ minA : num 18.5 ..$ D : int [1:90, 1:4] 5 9 13 6 8 14 6 14 9 11 ... ..$ D_latent: num [1:90, 1:4] -0.534 -2.038 -1.235 -0.354 -0.725 ... .. ..- attr(*, \"dimnames\")=List of 2 .. .. ..$ : chr [1:90] \"male\" \"female\" \"male\" \"female\" ... .. .. ..$ : NULL Notice the structure of the returned value by sim_data(). It is a nested named list with three elements at the top level: $dat, $params, and $others. $dat holds the data for model fitting. It has this specific structure to match stan’s data block. It makes more sense later when we see stan’s counterpart of the code. $params are the true parameter values used in the simulation. They are returned so we can later compare the true parameter values to those estimated by the model. $others holds other information that does not belong to the former two but is nevertheless needed. For instance, the minimum age minA is saved in $others so that raw ages in new data can be scaled accordingly if one would like to make predictions with new datasets. Model Formulation Now we have the simulated data, let’s begin constructing the model. In a Bayesian framework, model construction feels pretty much like a simulation (assume you are using Stan, not some wrappers around it) since both are essentially describing the same data-generating process. Bayes, as complicated as it may seem, really just comes down to four simple components: Data-generating process Prior distribution Data Posterior distribution The data-generating process describes the relations between the parameters, that is, how the parameters arise from others. The priors give preliminary information on the parameters (the priors could be thought of as part of the data-generating process as well). With these prepared, the Bayesian machine incorporates the information from the data to update the priors—according to the data-generating process—and arrives at the posterior distribution. The posterior is nothing else but a joint probability distribution of all parameters, after combining the information from the priors, the data-generating process, and the data. It is what we believe the parameters are, given the data and our assumptions. Everything we need for inference derives from the posterior. So we’re now ready to formulate our model (data-generating process) in Bayesian terms. This massive model can be chopped into three parts. Let’s divide and conquer. Causes of Efficacy The first part of the model deals with the generation of Efficacy. The formulation is shown below. The first two lines of the formulation describe how efficacy $E$ is generated from the effects of participant baseline $\\gamma_{Sid}$, age $\\beta_{AE}$, and treatment $\\beta_{TE}$. The last three lines specify the prior distributions for the relevant parameters. The “+” sign on the prior distribution of $\\tau$ is an abbreviation for the truncated normal distribution with negative values removed. Since $\\tau$ is a standard deviation, it must not be negative. The truncation at zero imposes this constraint on $\\tau$. Causes of Treatment Outcome With $E$ generated, we can then describe the process generating the treatment outcome $D^{\\ast}$. The outcomes in the current example are the number of heavy-drinking days in the past 14 days. Hence, it is a count variable with an upper bound of 14, which makes the binomial an ideal distribution for modeling the outcome. The binomial and the logit link are used here to map the count outcomes to the underlying latent variable $D$. As seen in the simulation, a negative sign is needed here to reverse the relation between $D^{\\ast}$ and $D$ such that a higher value of $D$ corresponds to a fewer count of heavy-drinking days. The third formula lays out the generative process of $D$, linking the influences of the treatment, age, and efficacy to $D$. A special kind of prior distribution—exemplified by the distribution of $\\beta_{TD}$ here—deserves some attention. The distribution is assumed to be a normal distribution with an unknown mean $\\mu_{\\beta_{TD}}$ and unknown standard deviation $\\sigma_{\\beta_{TD}}$. Their priors are then given by the next two lines. This nested structure of prior distributions is known as hierarchical priors. By specifying such a hierarchical structure in the priors, one can partial-pool information across the three treatment effects, thus reducing variation among the $\\beta_{TD}$ parameters. Partial pooling is used here for two reasons. First, it makes sense theoretically to partial-pool the direct treatment effects on the outcome since different treatments are still similar in some sense that information across all treatments is useful and should be shared. Second, partial pooling has the effect of reducing variation among the pooled parameters. Without such a soft constraint on the $\\beta_{TD}$ parameters, the model here will be unidentified (i.e., multiple sets of solutions exist). Causes of Item Response The final piece of the model describes the generation of participants’ responses to the items measuring efficacy. The submodel here has a structure nearly identical to the rating scale model described in the previous post. The only difference here is that a sum-to-zero constraint is imposed on the item parameters, which allows the item easiness to be anchored and thus comparable to the true parameter values from the simulation. The item parameters are also partially pooled here, achieved through the hierarchical prior on $\\sigma_I$. Stan: A Brief Introduction We have completed our formal model-building. Now, we have to code it in Stan, a language for implementing Bayesian models. Stan is hard for anyone first meeting it. Sadly, I think there is really no “introductory” material on Stan because Bayesian statistics is already an advanced topic. How, then, could anyone get started with Bayes if everything is advanced and hard? We need scaffolds. The scaffolds for learning statistics are simulations and programming skills that license them. Building up simulations allows one to experiment with stat models and therefore provides opportunities for understanding. So to learn Stan, one should probably begin with examples. The Stan User’s Guide provides tons of example models. Start with simple models or any one that you’re familiar with. Simulate data based on the model’s assumption, construct the model by modifying the example code, and fit the model to see if it correctly recovers the parameters. If succeeded, extend the simulation and the model for the next round of testing. If failed, simplify the model (and the simulation, if needed) to pinpoint the potential problems. Interestingly, the process of learning Stan is no different from using Stan. In both situations, one is unable to proceed without scaffolds. Stan gives you full flexibility, and with this modeling power, one immediately finds out there are infinite ways to go wrong. Simulation is the compass that keeps us oriented while navigating through the mess of modeling. That said, some acquaintance with the structure of Stan files does help when getting started. Stan File Structure The template below sketches the most common structure of a Stan program. A Stan program consists of several “blocks”. Three of the most basic blocks are shown here: the data, the parameters, and the model blocks. The data block simply contains variable-declaration code for the data. The parameters block declares variables that hold the parameters. The model block is where the data-generating process and the priors get specified. 1data { 2 // Declare varaiables for data (passed in from R) 3} 4parameters { 5 // Declare model parameters 6} 7model { 8 // Describe data-generating process (including priors) 9} With the blocks written up, one then compiles the stan program and feeds it the data. Stan would then construct and sample from the posterior. When all of these are done, posterior samples along with diagnostic information of sample quality are returned. That’s it. Don’t read too much while learning Stan. Getting the hands dirty is a much better way. So now let’s proceed to our massive model. Stan Model I’ll present our model coded in Stan one block at a time so that we don’t get overwhelmed. The stan file gets executed from the top to the bottom, and the blocks have to be defined in the given order as shown in the above template. Through this structure, the parameters and the model blocks have access to the data variables defined in the data block, and the model block has access to the parameter and data variables defined in previous blocks. Let’s first look at the data block of our model. The data block 1data { 2 int Ns; // num of subjects 3 int Ntx; // num of treatments 4 int Nt; // num of time points 5 int Nk; // num of Likert scale choices 6 int Ni; // num of items in the self-efficacy scale 7 8 // Item-level responses (NI=Ns*Ni*Nt) 9 int NI; 10 array[NI] int","subtitle":"Growth Curve Modeling of Latent Variables with Bayes","title":"Beyond Item Response Theory","uri":"/2023/06/27/irt5/"},{"content":"Rating scales require special treatments during data analyses. It is dangerous to treat the choices in a rating scale as simple numerical values. Nor is it satisfactory to treat them as discrete categories in which the internal ordering is thrown away. A rating scale is ordinal in nature, meaning that there is an inherent order among the choices within. This ordering is different from the ordering in numerical values such as counts and heights. In such cases, the differences between numerical values are directly comparable. For instance, a count of 5 differs from a count of 3 by a count of 2, and so is the difference between a count of 8 and 6. Ordinal variables are different. Take for example the subjective rating of happiness. It is probably easier to move from a rating of 2 to 3 than from a rating of 4 to 5 on a five-point Likert scale, as many people prefer to reserve the boundary ratings (1 and 5) for extreme cases. Ratings like this are ubiquitous in the social sciences and particularly in psychology, where rating scales are deployed to measure unobserved latent psychological constructs. In this post, the final one in the demystifying IRT series, I will walk you through the statistical machinery that deals with the rating scale. Things get a bit complicated in rating scales since the dimensionality increases, and it is always more challenging to think in higher dimensions. However, after peeling off the complexity introduced by the high dimensions, the underlying concept is quite straightforward. It is again a GLM, just with fancier machinery to map continuous latent quantities to a vector of probabilities. So don’t be scared off by the high dimensions. We just have to take one step at a time. Don’t worry if you run out of working memory. Shift the burden of holding everything in your brain to a piece of paper. Sketch what you need and proceed slowly. You will finally get there. Wine Quality Before moving on to the details of the statistical machinery behind the rating scale, let me first provide some context. The examples presented in previous posts are classical situations where IRT is applied and known for—a testing context. In such contexts, there are testees, test items, and possibly raters, but IRT is much more general than that. It is well applicable beyond the testing situation. Let us look at one such example, the rating of wine quality. There are wines, fine wines, premium wines, and judges in a wine competition. It is a simple twist of the item-testing scenario in which IRT is often applied. Again, two factors co-determine the rating scores of the wines here. First, it is the “inherent” property associated with each wine, the wine quality. High-quality wines should receive high ratings for the ratings to make sense at all. The second factor is the leniency of a judge in giving out the scores. A lenient judge tends to give higher ratings to the same wines as compared to stricter judges. These assumptions are illustrated in the DAG below. Here, $W$ and $J$ represent the latent wine quality and judge leniency, respectively. $R$ stands for the rating scores. If you will, you could draw the analogy to the previous IRT context, where $W$ can be thought of as corresponding to the person ability and $J$ to the item easiness. The analogy isn’t exact though. It’s equally sensible to think in the other direction. There’s nothing wrong to think of $W$ as corresponding to item easiness and $J$ to person ability. The only thing new is that instead of a binary response, $R$ can take more than two values. We need new machinery to map the aggregated influence from the two factors ($W$ and $J$), which is a latent score in the real space, to the outcome ordinal scale. Lower latent scores should give rise to lower ratings, and higher latent scores to higher ratings, in general. How is this achieved? Let’s dive into the intricacy of this machinery. From Latent to Rating $$ L ~~ \\rightarrow ~~ P_{cum.} ~~ \\rightarrow ~~ \\begin{bmatrix} P_1 \\\\ P_2 \\\\ P_3 \\\\ P_4 \\end{bmatrix} ~~ \\rightarrow ~~ R \\sim \\text{Categorical}( \\begin{bmatrix} P_1 \\\\ P_2 \\\\ P_3 \\\\ P_4 \\end{bmatrix} ) \\tag{1} $$ The path along the mapping of the latent scores onto the rating-scale (ordinal) space is sketched above. The leftmost term $L$ stands for the latent score, which we have learned to deduce from the simulations in previous posts. It is also the starting point of this machinery of converting real-valued scores to ordinal ratings. Things get a bit complicated in the intermediate steps on the path. Therefore, indulge me with explaining the path in reverse. I will start with the rightmost term, which, monstrous as it may seem, is probably the least challenging concept to be grasped here. Random Category Generator The seemingly monstrous term represents the generation of a rating score ($R$) from a categorical distribution. A categorical distribution takes a vector of $k$ probabilities as parameters. Each probability specifies the chance that a particular category (one of the $k$ categories) gets drawn. In essence, a categorical distribution is simply a bar chart in disguise. Each bar specifies the probability that the category is sampled. In the example here, I set the number of categories to $k = 4$, hence the four probability terms $P_1,~P_2, P_3,~P_4$. The code below plots a categorical distribution (bar chart) with 4 categories. The first line of the code specifies the relative odds of producing the 4 categories: Category 2 and 3 are three times more likely to be drawn than Category 1 and 4. Since the probabilities of all categories must sum to one in a distribution, the second line of code normalizes this vector to the correct probability scale. 1P = c( 1, 3, 3, 1 ) 2( P = P / sum(P) ) [1] 0.125 0.375 0.375 0.125 1plot( 1, type=\"n\", xlab=\"Category\", ylab=\"Prob\", 2 xlim=c(.5,4.5), ylim=c(0,.5) ) 3for (i in 1:4) 4 lines( x=c(i,i), y=c(0,P[i]), lwd=10, col=2 ) Now, to sample from this distribution, $\\text{Categorical}( \\begin{bmatrix} .125 \\\\ .375 \\\\ .375 \\\\ .125 \\end{bmatrix} )$, we simply use the sample() function: 1# Sample one category from the distribution 2sample( 1:4, size=1, prob=P ) [1] 3 1# Repeatedly sample from the distribution 2s = sample( 1:4, size=1e5, replace=T, prob=P ) 3( P2 = table(s) / length(s) ) s 1 2 3 4 0.12484 0.37579 0.37439 0.12498 1# Empirical frequency distibution obtained through sampling 2plot( 1, type=\"n\", xlab=\"Category\", ylab=\"Prob\", 3 xlim=c(.5,4.5), ylim=c(0,.5) ) 4for (i in 1:4) 5 lines( x=c(i,i), y=c(0,P2[i]), lwd=10, col=2 ) After drawing a large sample from this distribution, we can see that the frequency distribution of the samples approaches the original distribution. Back to the wine rating scenario. The categories in this context are the available rating scores. Since I adopted the example of four categories, in the rating-scale context, it would correspond to a 4-point Likert scale in which 1, 2, 3, and 4 are the four categories. One crucial part is missing though. The categorical distribution is order-agnostic: it knows nothing about the order of the categories it generates. What it does is faithfully produce categories according to the given probabilities. So, where does the order come from? It’s from the relationship between rating probabilities and the latent scores. Enforcing Order to Categories When a higher latent score tends to give rise to a higher rating, an order is automatically enforced on the categorical ratings (1, 2, 3, and 4). But how is this done? Recall the analogous situation of the binary regression in the previous posts. Back then, the link between the responses (0/1) and the latent scores is established through the probability: a higher latent score results in a higher probability of generating 1. Thus, in general, higher latent scores tend to produce 1s. A similar strategy can be deployed here: we bridge the responses and the latent scores through probabilities. The crucial difference is that we now get multiple, instead of one, probabilities to deal with. Statisticians came up with a clever solution to this. Instead of dealing with a vector of fluctuating probabilities, which breaks the desired monotonically increasing relationship between the probabilities and the ratings, the probabilities are transformed into a vector of cumulative probabilities. A nice thing about this vector of cumulative probabilities is that the probabilities are ordered, naturally. Larger cumulative probabilities now correspond to higher rating scores. Sounds confusing? Let me re-describe these more vividly with some code and plots. I’ll continue to use the four-point rating example. 1P = c( 1, 3, 3, 1 ) 2( P = P / sum(P) ) # Probabilities for R = 1, 2, 3, 4 [1] 0.125 0.375 0.375 0.125 1( Pc = cumsum(P) ) # Cumulative Probabilities for R = 1, 2, 3, 4 [1] 0.125 0.500 0.875 1.000 The code above computes the cumulative probabilities (Pc) from the vector of rating probabilities (P) through the function cumsum() (cumulative sum). Note that both vectors contain the same information. The original vector can well be reconstructed from the cumulative version. In math terms, their relationship is as follows: $$ \\gdef\\Pr{\\textrm{Pr}} \\begin{aligned} \\Pr(R=1) = \\Pr(R \\leq 1)\u0026 \\\\ \\Pr(R=2) = \\Pr(R \\leq 2)\u0026 - \\Pr(R \\leq 1) \\\\ \\Pr(R=3) = \\Pr(R \\leq 3)\u0026 - \\Pr(R \\leq 2) \\\\ \\Pr(R=4) = \\Pr(R \\leq 4)\u0026 - \\Pr(R \\leq 3) \\\\ = \\phantom{PPaa} 1 \\phantom{aaa}\u0026 - \\Pr(R \\leq 3) \\end{aligned} \\tag{2} $$ and in code: 1Ps = c( 0, Pc ) 2Ps[2:5] - Ps[1:4] # or more generally, Ps[-1] - Ps[-length(Ps)] [1] 0.125 0.375 0.375 0.125 The two vectors are visualized as distributions below. The red bars are the probability distribution we have met in the previous section. The blue bars plot the cumulative version of it. 1plot( 1, type=\"n\", xlab=\"Rating\", ylab=\"Prob\", xlim=c(.5,4.5), ylim=c(0,1) ) 2for (i in 1:4) { 3 lines( x=c(i-.05,i-.05), y=c(0,P[i]), lwd=10, col=2 ) 4 lines( x=c(i+.05,i+.05), y=c(0,Pc[i]), lwd=10, col=4 ) 5} Once we have an ordered sequence of probabilities, or more precisely, probabilities with a monotonically increasing relationship to the rating scores, we’ll be able to introduce latent scores through the logit link, as we have done in the binary case. We simply pass the cumulative probabilities to the logit function to map them onto the real space. To save space, I pack some commonly used functions into my package stom, which can be installed through the first two lines of commented code below. 1# install.packages(\"remotes\") 2# remotes::install_github(\"liao961120/stom\") 3library(stom) 4logit(Pc) # convert cumulative probabilities to reals [1] -1.94591 0.00000 1.94591 Inf The statistical machinery behind rating scales likely remains elusive after my wordy explanation. Indeed, since we are only halfway through the machinery, it would hardly make any sense just by looking at part of the picture. What I have presented so far is the portion of the machinery that monotonically aligns the latent scores with the ratings, through the use of cumulative probabilities. The second part of the machinery is to allow for the shifting of the entire vector of latent scores (and thus the probabilities of ratings, through the first part of the machinery) by a common term, which enables the modeling of extraneous influences on the ratings (thus the “regression”). Let’s now look at how this shifting is achieved. Shifting Latent Scores The code below summarizes the first part of the rating-scale machinery: establishing the link between latent scores and the probabilities of rating scores. 1P # vector of rating probs (starting point) 2( Pc = cumsum(P) ) # vector of rating probs (cumulative) 3( L = logit(Pc) ) # vector of latent scores [1] 0.125 0.375 0.375 0.125 [1] 0.125 0.500 0.875 1.000 [1] -1.94591 0.00000 1.94591 Inf Since all of the above mappings are one-to-one, we can as well express the same machinery in reverse: 1L # vector of latent scores (starting point) 2( Pc = logistic(L) ) # vector of rating probs (cumulative) 3Ps = c( 0, Pc ) 4( P = Ps[-1] - Ps[-length(Ps)] ) # vector of rating probs 5sample( 1:4, size=1, prob=P ) # draw one rating score from the distribution [1] -1.94591 0.00000 1.94591 Inf [1] 0.125 0.500 0.875 1.000 [1] 0.125 0.375 0.375 0.125 [1] 2 This second expression aligns well with the simulation perspective and precisely lays out the data-generating process of the rating scores. It also makes it clear that a predetermined set of latent scores (or probabilities of ratings) is required for generating the ratings. In a simulation, these latent scores are determined by us. For a model, they are a subset of parameters that the model tries to estimate from data. These latent scores can be thought of as baselines during rating. That is, the latent scores, or more visually, the shape of the rating distribution before any factor has exerted an effect on the ratings. To model the extraneous influences on the ratings, we utilize an independent term $\\phi$ in the latent score space. The trick is to subtract this $\\phi$ from the vector of the baseline latent scores. For instance, if a wine has a better-than-average quality that raises its quality (latent score) by $1.9$ above the baseline but is rated by a harsh judge that lowers the quantity by $1.1$, $\\phi$ will be $.8$. Subtracting $\\phi=.8$ from the baseline latent scores gives the shifted latent scores, from which the rating probabilities could then be derived: 1latent_to_prob = function(L) { 2 Pc = logistic(L) 3 Ps = c( 0, Pc ) 4 P = Ps[-1] - Ps[-length(Ps)] 5 return(P) 6} 7 8phi = 1.9 - 1.1 # wine (1.9) \u0026 judge (-1.1) influence on ratings 9L # baseline latent scores 10( Ls = L - phi ) # latent scores after influences of wine \u0026 judge 11latent_to_prob(Ls) # rating probs after influences of wine \u0026 judge [1] -1.94591 0.00000 1.94591 Inf [1] -2.74591 -0.80000 1.14591 Inf [1] 0.06031805 0.24970747 0.44873758 0.24123690 The bar chart below overlays the rating score distribution after considering $\\phi$ (blue bars) on the baseline distribution (red bars). It can be seen that subtracting $\\phi=.8$ from the baseline latent scores pushes the probability mass toward the right, raising the expected rating score. It might seem unintuitive that subtracting a positive value from the latent scores raises the expected rating scores. But it’s simply the effect of the cumulative probabilities. When the vector of the latent scores gets shifted, note that the last term doesn’t move since it is infinity ($logit(1) = \\infty$). Thus, the difference between the last and the second-to-last term, on the cumulative probability scale, becomes larger after the shift. This difference is essentially the probability of the largest rating ($P_4$ in our example). Therefore, the effect of subtracting a positive value from the baseline latent scores shifts the probability mass toward the larger ratings. For the remaining ratings, the directions of changes in probability depend on the amount of shift and the shape of the baseline distribution. It is thus hard to conceive how these probability bars react to the shift in the latent scores and how their shifts contribute to the increasing or decreasing of the expected rating. Interactive visualization of the rating probability distribution To disentangle these intertwined influences on the final distribution, I’ve built an interactive visualization1 to help. As shown in the figure above, there are two places where users can tweak to see how the shape of the rating distribution gets influenced. The four vertical sliders are there to adjust the baseline probabilities of the ratings, $Pr(R=1)$, $Pr(R=2)$, $Pr(R=3)$, and $Pr(R=4)$ (abbreviated as $P_1$ ~ $P_4$ respectively). The numerical value on top of each bar indicates the probability of that rating. Note that it is the relative positions between the vertical sliders that matter, and the four probabilities automatically adjust to always sum to one. The three values, $\\kappa_1$, $\\kappa_2$, and $\\kappa_3$, shown on top of the four probabilities are the cumulative logits, which are basically the vector of the cumulative probabilities, transformed to the logit scale. They are the baseline latent scores mentioned previously. The last term, $\\kappa_4$ is dropped since it is always infinite. The horizontal slider above the vertical sliders controls the value of $\\phi$, which gets subtracted from each of the baseline latent scores to derive the final distribution. Where’s the Regression? The previous section demonstrates how the baseline rating distribution shifts according to an aggregated influence of $\\phi$, which is the hard part of the statistical machinery behind the rating scale IRT model. Regression is the easy part. Now we have a nice and neat $\\phi$ sitting on the real space2 to work with. If we zoom in on $\\phi$, it’s simply the summed effect of the predictor variables in a linear regression, which is similar to $\\mu$ in logistic regressions. The only difference here is that we need a different linking distribution to map the effect onto the response scale (i.e., discrete ratings). In math terms, resuming our wine rating example, the distribution is shown in (3): $$ \\begin{aligned} R_i \u0026 \\sim \\text{OrderedLogit}(\\phi_i, ~ \\bm{\\kappa} = \\begin{bmatrix} \\kappa_1 \\\\ \\kappa_2 \\\\ \\kappa_3 \\end{bmatrix} ) \\\\ \\phi_i \u0026 = W_{Wid[i]} + J_{Jid[i]} \\\\ \\tag{3} \\end{aligned} $$ The $\\text{OrderedLogit}$ expression hides all the details from the reader. But you’ve already seen the details at work in code form in previous sections, albeit in a quite scattered manner. Later, I will collect them into a single function. If you prefer clarity now, the monstrous expressions in (4) should suffice. $$ \\newcommand{\\logit}{\\textrm{logit}} \\begin{aligned} R_i \\sim \\text{Categorical} \u0026 ( \\begin{bmatrix} \\Pr(R_i = 1) = \\Pr(R_i \\le 1) \\phantom{- \\Pr(R_i \\le 1)} \\\\ \\Pr(R_i = 2) = \\Pr(R_i \\le 2) - \\Pr(R_i \\le 1) \\\\ \\Pr(R_i = 3) = \\Pr(R_i \\le 3) - \\Pr(R_i \\le 2) \\\\ \\Pr(R_i = 4) = \\Pr(R_i \\le 4) - \\Pr(R_i \\le 3) \\\\ \\end{bmatrix} ) \\\\ \\logit[ \\Pr(R_i \\le 1) ] \u0026= \\logit[ Pr(R_i = 1) ] = \\kappa_1 - \\phi_i \\\\ \\logit[ \\Pr(R_i \\le 2) ] \u0026= \\kappa_2 - \\phi_i \\\\ \\logit[ \\Pr(R_i \\le 3) ] \u0026= \\kappa_3 - \\phi_i \\\\ \\logit[ \\Pr(R_i \\le 4) ] \u0026= \\logit(1) = \\infty \\\\ \\phi_i \u0026= W_{Wid[i]} + J_{Jid[i]} \\tag{4} \\end{aligned} $$ Don’t worry if you cannot understand the equations in (4) right now. After you get accustomed to the logic of the ordered logit, through coding, the expressions become straightforward. So now let’s wrap up what we have done so far, in code. I will write down the code form of the $OrderedLogit$ distribution in the function rOrdLogit(). 1rOrdLogit = function(phi, kappa) { 2 kappa = c( kappa, Inf ) # baseline latent scores 3 L = kappa - phi # latent scores, after shifting 4 Pc = logistic(L) # map latent scores to cumulative probs 5 # Compute probs for each rating from Pc 6 Ps = c( 0, Pc ) 7 P = Ps[-1] - Ps[-length(Ps)] # probs of each rating 8 sample( 1:length(P), size=1, prob=P ) 9} 10 11## Replicate previous example ## 12P = c( 1, 3, 3, 1 ) 13P = P / sum(P) 14Pc = cumsum(P) 15( kappa = logit( Pc )[-length(Pc)] ) # Set up baseline latent scores 16 17# 10,000 draws from OrdLogit 18draws = replicate( 1e4, rOrdLogit(phi=0, kappa=kappa) ) 19# should approach P = c(.125, .375, .375, .125) 20table(draws) / length(draws) [1] -1.94591 0.00000 1.94591 draws 1 2 3 4 0.1282 0.3788 0.3678 0.1252 Simulating and Fitting Wine Ratings Having all concepts in place, let’s start synthesizing data for our later model fitting. We will simulate data from the Ordered Logit distribution. One minor limitation with rOrdLogit() defined previously is that it can only take a single value phi, but it is more desirable for phi to be a vector of values. A vectorized version of rOrdLogit() is available in the stom package as rordlogit(). We will be using this function for our data simulation. 1library(stom) 2 3set.seed(1025) 4Nj = 12 # number of judges 5Nw = 30 # number of wines 6J = rnorm(Nj) # judge leniency 7W = rnorm(Nw) # wine quality 8J = standardize(J) # scale to mean = 0, sd = 1 9W = standardize(W) # scale to mean = 0, sd = 1 10kappa = c( -1.7, 0, 1.7 ) # baseline latent scores 11 12# Create long-form data 13d = expand.grid( Jid=1:Nj, Wid=1:Nw, KEEP.OUT.ATTRS=F ) 14d$J = J[d$Jid] 15d$W = W[d$Wid] 16d$phi = sapply( 1:nrow(d), function(i) d$J[i] + d$W[i] ) 17d$R = rordlogit( d$phi, kappa ) # simulated rating responses 18d$B = rbern( logistic(d$phi) ) # simulated binary responses 19d$C = rnorm( nrow(d), d$phi ) # simulated continuous responses 20 21# Conversion of data types to match model-fitting function's requirements 22for ( v in c(\"Jid\", \"Wid\") ) 23 d[[v]] = factor(d[[v]]) 24d$R = ordered(d$R) 25str(d) 'data.frame': 360 obs. of 8 variables: $ Jid: Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ... $ Wid: Factor w/ 30 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ... $ J : num 0.0187 -0.1794 -1.4372 1.3915 -0.1134 ... $ W : num 0.236 0.236 0.236 0.236 0.236 ... $ phi: num 0.2544 0.0564 -1.2014 1.6273 0.1223 ... $ R : Ord.factor w/ 4 levels \"1\"\u003c\"2\"\u003c\"3\"\u003c\"4\": 4 2 2 2 1 1 1 4 2 3 ... $ B : int 1 0 0 1 1 1 1 1 0 0 ... $ C : num 0.6142 -1.5915 -0.2754 1.2755 0.0792 ... Running the above code will get our data prepared. Two things might be worth noting. The first is the standardize() function, which centers the input vector to zero mean and a standard deviation of one. J and W are centered here to make the parameters later estimated by the model comparable to the scale of the true values. In our later model, we will partial-pool both the judges and the wines and hence assume a zero-meaned distribution for both of them. Since the sample size of our data isn’t large (12 judges and 30 wines), which will likely cause the means of the raw J and W to have non-minor deviations from zero, standardization is needed. Second, in addition to R, the rating responses, I also simulate binary responses B (0/1) from phi. Indeed, if a model is fitted with B as the dependent variable, it will be identical to the logistic regression models fitted in previous posts. The binary responses are simulated to demonstrate the parallels between the binary3 model and the rating scale model. The two models are highly similar: the linear effects are aggregated in the same ways (in $\\mu$/$\\phi$). The only difference is how these effects are projected onto the response scale: the binary model does so through the Bernoulli distribution, and the rating scale model through the Ordered Logit distribution. Another reason for simulating binary responses along the rating responses is for the preparation of model debugging. As we start fitting more and more complex models, we are bound to find ourselves lost in situations where we have no idea why the model fails to give the expected results. In such cases, it helps a lot to check the results from simpler models, which might hint at where the complex model went wrong. This is also the reason why I simulate the continuous responses C—to prepare data for fitting an even simpler model. By eliminating the influences arising from nonlinear links in the GLMs, the normal response model becomes more transparent and hence much easier to debug. For our wine rating example here, I’ve deliberately made the data-generating process simple enough that our rating scale model can smoothly fit and give us the expected results. To fit ordered logit regressions with partial pooling structures, we need the clmm() function from the package ordinal. The model syntax in clmm() is basically identical to the syntax we used in lme4::glmer() back then. As shown in the code below, we model the rating scores (R) to be influenced by both the wines and the judges. By partial pooling wines and judges, the wine effects and the judge effects are respectively assumed to come from a zero-meaned normal distribution. 1library(ordinal) 2m = clmm( R ~ (1|Jid) + (1|Wid), data = d ) 3summary(m) Cumulative Link Mixed Model fitted with the Laplace approximation formula: R ~ (1 | Jid) + (1 | Wid) data: d link threshold nobs logLik AIC niter max.grad cond.H logit flexible 360 -455.07 920.14 182(726) 6.96e-06 4.9e+01 Random effects: Groups Name Variance Std.Dev. Wid (Intercept) 1.220 1.1046 Jid (Intercept) 0.853 0.9236 Number of groups: Wid 30, Jid 12 No Coefficients Threshold coefficients: Estimate Std. Error z value 1|2 -1.42044 0.36491 -3.893 2|3 0.05442 0.35608 0.153 3|4 1.56532 0.36674 4.268 summary(m) prints out the model summary along with the estimated baseline latent scores, which are labeled as Threshold coefficients above. You can see that these coefficients (-1.42, 0.054, and 1.565) align pretty well with the kappa set in the simulation (-1.7, 0, and 1.7). To examine the estimated wine and judge effects, we similarly utilize the ranef() function as demonstrated in the previous post: 1est_wine = ranef(m)$Wid[[1]] 2est_judge = ranef(m)$Jid[[1]] 3plot( est_wine, W ); abline( 0, 1 ) 4plot( est_judge, J ); abline( 0, 1 ) Item Response Theory and Beyond We have come a long way, from the simplest binary item response model to models with delicate machinery such as the rating scale model with partial-pooling structures. The posts in this demystifying series are sufficient, I suppose, in providing a solid understanding of and practical skills for working with item response theory. There are certainly even more complex IRT models, but I won’t go further in that direction. No matter how many new and complex models are added to the toolkit, we are certain to find our tools in shortage when facing real-world problems. Item response models, general as they might seem, quickly run out of supply. Although binary and rating scale models allow us to deal with most response types found in the field (such as tests in educational settings, scales measuring psychological constructs, and various surveys used in the social sciences), even the slightest complication renders these models useless. Just consider a mixed-format test consisting of, for example, multiple-choice items (binary scored) and items of open-ended questions (rated). Which IRT model can we apply to this mixed-format test? Not a single one. Instead, we need two separate models, each independently running on a subset of the test for a particular item format. A special technique is then required to map the independently estimated person/item parameters onto a common scale. The method works but is a waste of information. When models are separately estimated, information cannot be shared across different item formats to improve parameter estimation. Item estimates might be fine, as long as there are many subjects. Person estimates suffer greatly though since, in practice, the test length is limited and is now further divided up by two independent models. This is equivalent to estimating person parameters with fewer items. It is always better to incorporate everything into a single comprehensive model instead of separately modeling a subset of variables in multiple small models. It is better because information flows smoothly across the variables in a comprehensive model, but the flow breaks down when the model gets torn apart into several pieces. However, such comprehensive models are rarely, if not never, available in the literature. We have to tailor a model ourselves according to what the current situation demands. Therefore, a framework is required to guide us through building up such a model. This post marks the end of the demystifying series. When the thick cloud of mystery begins to dissolve, we finally get to start solving real and exciting problems rather than wrangling with mad statistical models. In my next post, I will move on to Bayesian statistics, a unified framework that allows flexibly extending a model to match the demanded conditions. Bayesian framework is ideal for empirical research because it is practical. We do not need to wait for a statistician to come up with a model for every new situation. In Bayesian inference, we simply describe the data-generating process and the priors, and the rest is handled by probability theory and an estimation algorithm. Therefore, we can focus on the scientific problems at hand instead of fussing around with fancy models and their names. We will see how item response models can be embedded into a larger network of causes and effects that represents the assumed interactions underlying the current problem. Item response models, which are essentially methods for handling measurement errors, help deal with the latent constructs measured indirectly through surveys in this network of interacting variables. The source code for building the interactive visualization of the ordered logit distribution can be found on GitHub. It is built upon this nice project for visualizing various probability distributions. ↩︎ Recall that $\\phi$ works in the latent score space by increasing or decreasing the baseline latent scores. ↩︎ In the testing context, a binary dependent variable is often used for modeling correct/incorrect responses. In the current wine rating context, a binary dependent variable could also be used for modeling ratings. In such cases, there must only be two possible ratings, such as mediocre/premium, on the wines. ↩︎ ","subtitle":"Rating Scale Models and Ordered Logit Distributions","title":"Demystifying Item Response Theory (4/4)","uri":"/2023/04/26/irt4/"},{"content":"Fixed, Random and Mixed Statistics is confusing enough through its massive terminology. Psychology, which is largely experiment-oriented, further confuses people by adding in its own flavor. A peek at the definitions of fixed, random, and mixed effects in the APA Dictionary of Psychology exemplifies this:\n[A mixed-effect model is] any statistical procedure or experimental design that uses one or more independent variables whose levels are specifically selected by the researcher (fixed effects; e.g., gender) and one or more additional independent variables whose levels are chosen randomly from a wide range of possible values (random effects; e.g., age).\n—Definition of “mixed-effect model” in the APA Dictionary of Psychology\nThe definitions for random and fixed effects above are not only confusing but also misleading. In principle, whether a categorical variable is “fixed” or “random” has nothing to do with the nature of the variable (e.g., gender doesn’t have to be fixed) or how the levels within a variable are selected (randomly drawn or chosen by researchers). Whether a variable is modeled as fixed or random is a decision to be made by the modeler. And the modeler should model variables as random if there are no justifiable prohibitive reasons. Let me explain.\nMultilevel Instead of Mixed A better way to understand fixed and random effects is to think hierarchically. The levels of a random-effect variable are treated as related by the model, meaning that the effect of each level is estimated by also considering information from other levels in the variable. This is known as partial pooling, and it has several desirable properties. On the other hand, the levels within a fixed-effect variable are treated as independent: during parameter estimation, the model considers only information within each level. This is the no-pooling case. So how does the model incorporate information from the other levels during estimation in the partial-pooling case? To explain this, let me start with the no-pooling case.\nAs an example, suppose we have a categorical variable with $n$ levels. Our goal is to obtain an estimate for each of these levels (and the variability in the estimates), labeled as $\\alpha_1, \\alpha_2, …, \\alpha_n$. To provide some context, we can think of the categorical variable here as nationality, and for each nation (a level), we want to estimate the average height (the parameter) of its citizens. When we are not pooling information across the levels, the structure of the data-generating process assumed by the statistical model is shown in the figure below. Here, the model assumes that there is a parameter associated with each level that generates the observations. However, the parameters here are assumed to be independent. Therefore, the model utilizes only the observations under each level to estimate its parameter. What has been learned about a nation is uninformative about another nation for the no-pooling model.\nLevels within a categorical variable estimated independently. But there actually is some information, right? Take the perspective of an alien, for instance. Knowing the average height of the Americans, say 5′9″, provides quite much information about the heights of the Japanese—they are unlikely to be 60 feet or 6 inches, agree? This is why we prefer to partial pool. Partial pooling allows the sharing of information across different levels, which, as elucidated below, leads to several desirable properties.\nWhen we want to incorporate—or partial pool—information from other levels during estimation, we can utilize models that assume a hierarchical structure on the levels within a variable. Such a hierarchical structure is exemplified in the figure below. This hierarchical structure assumes that all levels, or more precisely all parameters underlying the levels, come from a common distribution. Here, this common distribution is assumed to be a normal distribution with mean $\\mu$ and variance $\\sigma^2$. The mean and the variance are to be estimated from the data. When this structure is imposed, the observations under different levels will be naturally linked since now, the observations under every level all provide information for estimating the common distribution.\nLevels within a categorical variable estimated by incorporating information from all levels. This is achieved through assuming all levels to come from a common distribution. What can be gained from partial pooling information across levels? As mentioned above, when we partial-pool, the information of the observations across all levels is shared. This means that the model considers more information for estimating the parameter of each level. As a direct result of this, the model uses the data (observations) more efficiently by squeezing out more information. Secondly, it reduces overfitting and thus provides better (out-of-sample) estimation. The model is less likely to overfit because it is more “objective” by considering information across different levels. Overfitting occurs when data are scarce, which is the case in the no-pooling case since only data within each level are considered. In such cases, the model bases the estimations on fewer observations and hence tends to be overly sensitive to idiosyncratic patterns in the local data. Another great thing about partial pooling is that it automatically adjusts according to the sample sizes under each level. For levels with fewer observations (e.g, North Korea, in which the alien managed to collect only heights from three of its citizens), the model places more weight on the overall information provided by other levels, resulting in larger adjustments of the levels’ estimates. For levels with abundant data, their estimates are only slightly affected by the observations from other levels.\nPartial pooling essentially arises from the hierarchical structure assumed in the models. Therefore, these models are known as hierarchical or multilevel models. Mixed (effect) models are another common label for these models, though, as explained above, the name is quite uninformative. To understand how these models work, it is better to start with their hierarchical structuring. I will use the term multilevel models from now on to save ourselves from confusion. This name is also nice in that it coincides in abbreviation with the mixed model—both of them are abbreviated as GLMM for Generalized Linear Multilevel/Mixed Models.\nBack to IRT Now, we are acquainted with the concept of partial pooling and multilevel models, let’s apply them to the IRT context to improve our previous model, which is fitted without partial pooling across levels. To warm up, let me rephrase the structure of the simulated IRT dataset in terms of the multilevel terminologies.\nThere are two variables at work here—the item variable and the person (or subject) variable. Within the item variable, there are several items. In other words, each item acts as a level within the item variable. Similarly, each person corresponds to a level within the person variable. For each item, we want to estimate a parameter, the item’s difficulty. Likewise, for each person, we also want to estimate a parameter, the person’s ability. To improve our model in estimating the item/person parameters, we can partial pool information across the levels within the item and/or the person variable.\nThe chunk below copies the data simulation code from the previous post, with two minor changes. The first is the renaming of the variables for the item and subject ID as Iid (originally I) and Sid (originally S). The second is that, instead of item difficulty (D in the previous post), we conceptualize the effect of items as easiness (E) here. Item easiness is simply the negative of item difficulty. This simple switch would allow us to skip the step of reversing the item effects’ signs returned by the regression model.\n1logistic = function(x) 1 / (1 + exp(-x)) 2logit = function( p ) log( p/(1-p) ) 3rbern = function( p, n=length(p) ) rbinom( n=n, size=1, prob=p ) 4 5set.seed(12) 6n_item = 30 # number of items 7n_subj = 60 # number of subjects 8n_resp = n_item * n_subj 9n_param = n_item + n_subj 10A = rnorm( n_subj ) # Person ability 11E = seq( -1.6, 1, length=n_item ) # Item easiness 12 13# The data 14d = expand.grid( Sid=1:n_subj, Iid=1:n_item, KEEP.OUT.ATTRS = F ) 15d$mu = A[d$Sid] + E[d$Iid] 16d$R = rbern( logistic( d$mu ) ) 17d$Sid = factor(d$Sid) 18d$Iid = factor(d$Iid) Unpooled Model With the data prepared, let’s refit model m1.2 from the previous post. Later, I will fit another model that partial pools the subject variable (m2) and compare it to the unpooled model here (m1).\nThe code below for fitting m1 is identical to those in the previous post, except that I adopt another method (starting from line 5) to reconstruct the dropped estimate (forced by the sum-to-zero constraint). This change is necessary, as it also allows us to reconstruct the standard errors of the dropped estimate. We will need the standard errors later to quantify the uncertainty in the estimates, which are used for comparing the unpooled and partial-pooled models. In addition, the method adopted here is more principled and general, which further consolidates our understanding of contrasts and dummy coding. However, it takes up some space for the explanation since a little matrix algebra is involved. I thus leave the details in the box at the end of the post.\n1d1 = d 2contrasts(d1$Sid) = contr.sum( n_subj ) 3m1 = glm( R ~ -1 + Iid + Sid, data=d1, family=binomial(\"logit\") ) 4 5# Construct contrast matrix 6Cmat = diag(0, nrow=n_item+n_subj)[, -1] 7diag(Cmat)[1:n_item] = 1 8idxS = 1:n_subj + n_item 9Cmat[idxS, idxS[-length(idxS)]] = contr.sum( n_subj ) 10 11# Reconstruct estimates with the constrast matrix 12m1_eff = (Cmat %*% coef(m1))[, 1] 13# Reconstruct std. error of the estimates with the constrast matrix 14Vmat = Cmat %*% vcov(m1) %*% t(Cmat) 15m1_se = sqrt(diag(Vmat)) Partial-pooled Model To fit the partial-pooled model, glmer() from the lme4 package is used.\n1library(lme4) 2m2 = glmer( R ~ -1 + Iid + (1|Sid), data=d, family=binomial('logit') ) lme4 provides a syntax for expressing multilevel models of different structures. For our model here, which is one of the simplest multilevel models (known as the varying intercept models), we express the partial pooling of persons with the syntax (1|Sid), as shown in the last term of the model formula. When such a partial pooling structure is specified, glmer() automatically imposes a constraint of zero-meaned normal distribution on the partial-pooled variable. In the case here, this means that the ability of each person is modeled as being drawn from a normal distribution with a mean of zero and an unknown standard deviation to be estimated from the data. This constraint on the distribution of the person ability naturally resolves the identification issue of the IRT model. Hence, there is no need to impose an additional sum-to-zero constraint as we did in m1. We are only partial-pooling the person variable here, so except for (1|Sid), everything else in glmer() is identical to those in m1.\nAfter fitting the model, the estimates from m2 can be obtained with the code below. Unpooled and partial-pooled estimates are extracted differently in lme4. To extract the unpooled estimates, one uses the fixef() function. These unpooled estimates, along with their standard errors and other information, are also found in the model summary table returned by summary(). The partial-pooled estimates, however, are not found in the table. To extract them, we need the ranef() function as shown below.\n1m2_eff.item = fixef(m2) 2m2_eff.subj = ranef(m2)$Sid[, 1] In addition to the estimates, we would also like to retrieve their standard errors. Similar to the estimates, the standard errors of the estimates are extracted differently according to whether they are unpooled (fixed) or partial-pooled (random). We can utilize se.fixef() and se.ranef() from the arm package to extract these standard errors:\n1m2_se.item = arm::se.fixef(m2) 2m2_se.subj = arm::se.ranef(m2)$Sid[, 1] Finally, to compare the estimates of m1 and m2, I plot them together in the same figure. I also plot the uncertainty—calculated as $\\pm 2 \\times Standard~error$—around each estimate. Estimates and uncertainties from m1 are plotted as blue, whereas those from m2 are plotted as pink. The true effects for generating the simulated data are plotted as solid black points.\n1# Concatenate item \u0026 subj effect/std to match m1_eff/m1_se 2m2_eff = c( m2_eff.item, m2_eff.subj ) 3m2_se = c( m2_se.item, m2_se.subj ) 4 5#' Function stolen from `rethinking::col.alpha()` 6col.alpha = function (acol, alpha = 0.5) { 7 acol = col2rgb(acol) 8 acol = rgb(acol[1]/255, acol[2]/255, acol[3]/255, alpha) 9 acol 10} 11 12# Plot for comparing `m1` \u0026 `m2` 13plot( 1, type=\"n\", ylim = c(-4.8, 4.8), xlim=c( 0, n_subj+n_item + 1 ), 14 ylab=\"Effect\", xlab=\"Item Index\" ) 15abline( v=n_item + .5, lty=2, col=\"grey\" ) 16segments( -5, mean(m2_eff.item), n_item+.5, lty=2, col=\"grey\" ) 17segments( n_item+.5, 0, 1000, lty=2, col=\"grey\" ) 18points( c(E, A), pch=19 ) 19# Uncertainty bars 20for (i in seq_along(m2_se)) { 21 lines( c(i,i), m1_eff[i] + c(-2,2)*m1_se[i], col=col.alpha(4), lwd=6 ) 22 lines( c(i,i), m2_eff[i] + c(-2,2)*m2_se[i], col=col.alpha(2,.7), lwd=3 ) 23} 24points( m1_eff, col=4 ) 25points( m2_eff, col=2 ) Shrinkage Some of the benefits of partial pooling discussed earlier are visualized in the comparison plot above. The most drastic changes from the unpooled to the partial-pooled model are seen in the ability estimates, which are exactly the levels that get partially pooled. Two things to notice here. First, there is less uncertainty in the partial-pooled estimates than in the unpooled ones (pink bars tend to be shorter than their blue counterparts). This follows naturally because, through partial pooling, the model has access to more information (hence less uncertainty) for each level. Secondly, the partial-pooled estimates tend to get “pulled” towards the center (i.e., the grand mean of the subject estimates). In addition, more extreme estimates are further pulled toward the center. Essentially, this means that the model behaves in a way that is robust against observations that result in extreme estimates. This is known as shrinkage and is also a feature that naturally arises from partial pooling.\nFrom the figure, we can see that partial pooling improves the estimation of the person abilities, as most pink circles are found to be much closer to the solid black dots (true effects) than the blue ones. For item easiness, which are not partial-pooled, the estimates also improve slightly. This results from the improvement in estimating abilities. Since ability and easiness are jointly estimated by the model, the improvement from ability estimation carries on to easiness estimation. Given the large improvement in ability estimates, one might consider also pooling the items. Indeed, there is no reason to not pool. Partial pooling should be the default.\nPartial Pool Items and Subjects To specify the partial pooling of items in the model, we again utilize the “bar” syntax: (1|Iid). This allows glmer() to also model the items as coming from a normal distribution with zero mean and unknown variance. Now, since both the items and the subjects are centered at zeros, an additional step is needed to reconstruct the zero-centered item estimates back to their original locations1. This is why the model formula in m2.2 uses 1 instead of -1. By specifying 1, glmer() estimates an independent global intercept. In the case here, this intercept is identical to the amount subtracted from the item effects for centering. Hence, to reconstruct the original non-centered item estimates, we add the global intercept back to the item estimates, as shown in line 4 in the code below.\n1m2.2 = glmer( R ~ 1 + (1|Iid) + (1|Sid), data=d, family=binomial('logit') ) 2 3m2.2_eff.subj = ranef(m2.2)$Sid[, 1] 4m2.2_eff.item = ranef(m2.2)$Iid[, 1] + fixef(m2.2)[[\"(Intercept)\"]] 5m2.2_eff = c( m2.2_eff.item, m2.2_eff.subj ) 6m2.2_se = arm::se.ranef( m2.2 ) 7m2.2_se = c( m2.2_se$Iid, m2.2_se$Sid ) You can compare m2.2 to m2 by plotting their estimates with the plotting code previously shown and see that m2.2 further improves the estimation (though not large). In the psychometric/measurement literature, partial pooling both item and person is uncommon. But as seen in our simulate-and-fit approach, partial pooling results in better estimation. This approach also refutes the unjustified belief that “fixed effects should be used when the levels are specifically selected by the researcher”. In our simulation, values of the item easiness are specifically “picked out”. They are deliberately set to be equally-spaced values. And still, we saw that modeling the item effects as random is not only benign but even improves estimation. This is true in general, and you can change the values of the item easiness in the simulation to see that partial pooling mostly, if not always, gives better estimates.\nWhat’s next So far, we have been dealing with item response models with dichotomous item responses. That is, a response can only either be correct (1) or wrong (0). In Part 4, we move on to item response models for rating responses. These models are extremely useful since rating scales are common in the social sciences. The models also allow us to model the so-called “rater effect”, which quantifies the leniency of the raters. By incorporating such rater effects, the model corrects for potential biases introduced by subjective ratings, thereby giving more accurate person and item estimates.\nDon’t be intimidated by matrix algebra. It’s simply arithmetics in a fancy manner, and it looks scary only because it does many things at once. With some patience, you will be able to break down and understand the steps involved.\nReconstructing Dropped Estimates Let’s first see how the contrast matrix reconstructs the dropped estimate from the sum-to-zero constrained model. I’ll start with a toy example with only three subjects, $S_1, S_2, S_3$. The contrast matrix for imposing a sum-to-zero constraint on the subjects is shown below. Recall that the sum-to-zero constraint is coded through the dropping of the last subject, $S_3$ (hence two columns left in the contrast matrix), and implicit coding of $S_3$’s information into $S_1$ and $S_2$ by the -1s on the third row. Given this coding, the effect of $S_3$ can be reconstructed from the effects of $S_1$ and $S_2$ by taking the negative of their sum. This can be done through the code c(subj_eff.m1.2, -sum(subj_eff.m1.2) ) from the previous post.\n$S_1$ $S_2$ $S_1$ 1 0 $S_2$ 0 1 $S_3$ -1 -1 The same thing can be done through matrix multiplication. Simply take the above 3-by-2 contrast matrix and multiply the 2-by-1 column vector of the estimated effects for $S_1$ and $S_2$, which I abbreviate as $E_1$ and $E_2$ here. The last entry of the resulting column vector would then give what we want.\n$$ \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{bmatrix} \\begin{bmatrix} E_1 \\\\ E_2 \\end{bmatrix} = \\begin{bmatrix} E_1 \\\\ E_2 \\\\ -E_1 - E_2 \\end{bmatrix} $$\nHere’s the R code version of the above matrix multiplication:\n1Cmat = contr.sum(3) # Contrast matrix coding sum-to-zero constraint 2eff = c( E1=1.5, E2=1.7 ) # Made-up effect of S1 and S2 3Cmat %*% eff [,1] 1 1.5 2 1.7 3 -3.2 Reconstructing Standard Error of Dropped Estimates The contrast matrix can similarly be applied to reconstruct the variance ( hence the standard error) of the dropped subject’s estimate. The reconstruction is based on the variance sum law, $Var(X+Y) = Var(X) + Var(Y) + 2~Cov(X,Y)$, which has a natural generalization through matrix notations. Hence, given the variance of the estimates for $S_1$ and $S_2$ and their covariance, we will be able to reconstruct the variance of $E_3$ as\n$$ \\begin{equation} Var(E_3) = Var(E_1) + Var(E_2) + 2~Cov(E_1,E_2) \\tag{1} \\end{equation} $$\nThe variances and covariances of the estimates are given by the (variance-)covariance matrix of the fitted model. The formula below shows the matrix generalization to the variance sum law. Note that through the matrix generalization, we also get the reconstructed covariances, as shown in the off-diagonal entries in the reconstructed covariance matrix (the right-most matrix). The variance of $E_1$, $E_2$, and $E_3$ are found on the diagonal. The standard errors of the estimates are then obtained by taking the square roots of these diagonal entries.\nIn R, the same calculation is done with the code below.\n1Cmat = contr.sum(3) # Contrast matrix for coding sum-to-zero constraint 2# Made-up variances-covariances matrix of the estimates 3Vmat = matrix(c( 0.3, -0.01, 4 0.0, 0.4 ), 5 byrow=T, nrow=2 ) 6Cmat %*% Vmat %*% t(Cmat) # Reconstructed variance-covariance matrix 1 2 3 1 0.3 -0.01 -0.29 2 0.0 0.40 -0.40 3 -0.3 -0.39 0.69 Back to the Code 1d1 = d 2contrasts(d1$Sid) = contr.sum( n_subj ) 3m1 = glm( R ~ -1 + Iid + Sid, data=d1, family=binomial(\"logit\") ) 4 5# Construct contrast matrix 6Cmat = diag(0, nrow=n_item+n_subj)[, -1] 7diag(Cmat)[1:n_item] = 1 8idxS = 1:n_subj + n_item 9Cmat[idxS, idxS[-length(idxS)]] = contr.sum( n_subj ) 10 11# Reconstruct estimates with the constrast matrix 12m1_eff = (Cmat %*% coef(m1))[, 1] 13# Reconstruct std. error of the estimates with the constrast matrix 14Vmat = Cmat %*% vcov(m1) %*% t(Cmat) 15m1_se = sqrt(diag(Vmat)) Once familiar with the matrix algebra discussed, the above code for reconstructing the dropped subject’s effect should become self-explaining. The only complication here is that instead of using the contrast matrix of the subjects, a larger matrix encompassing the coding of all levels of both the item and the subject variables is used to match the covariance matrix given by the model (which also contains all levels from all variables). This large contrast matrix can be thought of as the concatenation of two contrast matrices along the diagonal, with the remaining off-diagonal entries filled in with zeros. To better explain this, let me go back to our previous example with three subjects.\nTo keep things simple, let’s assume additionally that there are only three items. Since in the model, the sum-to-zero constraint is only imposed on the subjects, the contrast matrix for the coding of items would be a 3-by-3 identity matrix. Concatenating the item and subject contrast matrices in the way mentioned above results in the matrix:\n$$ \\begin{bmatrix} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix}_{3 \\times 3} \u0026 0~~~~~ \\\\ 0 \u0026 \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ -1 \u0026 -1 \\end{bmatrix}_{3 \\times 2} \\end{bmatrix}_{6 \\times 5} $$\nIn general, with $n_I$ items and $n_S$ subjects, this concatenated contrast matrix has the form:\n$$ \\begin{bmatrix} ~~\\mathrm{I}_{n_I \\times n_I}~~~ \u0026 0 ~~~~~~~~~~~~~~ \\\\ \\\\ 0 \u0026 \\begin{bmatrix} 1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\ 0 \u0026 1 \u0026 \u0026 0 \\\\ \\vdots \u0026 \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\cdots \u0026 1 \\\\ -1 \u0026 -1 \u0026 \\cdots \u0026 -1 \\end{bmatrix}_{n_S \\times (n_S - 1)} \\end{bmatrix}_{ (n_I + n_S) \\times (n_I + n_S - 1) } $$\nThis is what the second part of the above code (reproduced below) is doing. It first sets up the correct shape of this large contrast matrix according to the number of items and subjects. The trick here is to use the diag() function to initialize a square matrix of zeros and drop one of the columns to match the correct number of dimensions. Then, line 3 of the code sets the upper-left portion of this matrix (the item sub-matrix) as an identity matrix by filling in the diagonal with ones. Finally, the lower-right portion of the matrix (the subject sub-matrix) is replaced with the subject contrast matrix constructed by the contr.sum() function.\n1# Construct contrast matrix 2Cmat = diag(0, nrow=n_item+n_subj)[, -1] 3diag(Cmat)[1:n_item] = 1 4idxS = 1:n_subj + n_item 5Cmat[idxS, idxS[-length(idxS)]] = contr.sum( n_subj ) With the contrast matrix Cmat prepared, we can construct what we need through matrix algebra. The estimates for all levels, including the dropped one, are reconstructed by multiplying Cmat with the estimates returned by the model. This is illustrated in the line below. The estimates are given by coef(m1), and the [, 1] at the end of the line forces the resulting one-column matrix into vector form.\n1m1_eff = ( Cmat %*% coef(m1) )[, 1] The full covariance matrix is similarly reconstructed through Cmat and the covariance matrix extracted from the model (vcov(m1)):\n1Vmat = Cmat %*% vcov(m1) %*% t(Cmat) Since the final products we need are the standard errors, we extract the diagonal entries of Vmat and take the square root:\n1m1_se = sqrt( diag(Vmat) ) We don’t touch the subject estimates, though, since we assume them to be centered at zero in the simulation, remember? ↩︎\n","subtitle":"Improving Estimation through Partial Pooling","title":"Demystifying Item Response Theory (3/4)","uri":"/2023/03/29/irt3/"},{"content":"In Part 1, we went through the simplest item response model, the 1PL model, from the perspective of simulations. Starting with item difficulty and testee ability, we worked forward to simulate item responses that mimic real-world data. Back then, we were precisely laying out the data generating process that is assumed by the item response theory. In this post, we work backward. We will start with the item responses and work back toward the unobserved difficulties and abilities, with the help of statistical models. But first, let’s simulate the data we will be using! 1logistic = function(x) 1 / (1 + exp(-x)) 2rbern = function( p, n=length(p) ) rbinom( n=n, size=1, prob=p ) 3 4set.seed(12) 5n_item = 30 # number of items 6n_subj = 60 # number of subjects 7n_resp = n_subj * n_item # number of responses 8 9A = rnorm( n=n_subj, mean=0, sd=1 ) # Subjects' ability 10D = seq( -1.6, 1, length=n_item ) # Items' difficulty 11 12# The data 13d = expand.grid( S=1:n_subj, I=1:n_item, KEEP.OUT.ATTRS = FALSE ) 14d$R = rbern( logistic(A[d$S] - D[d$I]) ) 15d$S = factor(d$S) 16d$I = factor(d$I) 17str(d) 'data.frame': 1800 obs. of 3 variables: $ S: Factor w/ 60 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ... $ I: Factor w/ 30 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ... $ R: int 0 1 0 0 1 1 1 1 1 1 ... In the code above, the first two lines are the definitions for the logistic and the Bernoulli functions used previously. The second chunk of code sets the shape of our data. This time, the simulated data is much larger, with 30 items and 60 testees, or subjects (I will use the more general term “subject” hereafter). Since we assume here that each subject responds to every item, this gives us 1800 responses in the data. The subject abilities come from a normal distribution with a zero mean and a standard deviation of one (the standard normal). The item difficulties are equally-spaced values that range from -1.6 to 1. A notable change from the previous post is that number indices are used here for labeling items (I) and subjects (S). For simple illustrations, letter indices are clearer. But for larger data sets, number indices are easier to manipulate with code. Now, since S and I are coded as integers, we need to explicitly convert them into factors. Otherwise, the model will treat the number indices as values in a continuous variable. DAGs Revisited Before we move on to the statistical model, let me lay out the DAGs again. The DAG on the right below is identical to the one in Part 1 (the left DAG here), but with a slight modification that emphasizes the perspective from the statistical model. Here, the observed $S$ and $I$ take place of the unobserved $A$ and $D$, respectively. So why the difference? Recall that the nodes $A$ and $D$ represent the joint influences of a subject’s ability and an item’s difficulty on the probability of success on that item. However, the statistical model cannot notice $A$ and $D$ since they are theoretical concepts proposed by the IRT. What the model “sees” is more similar to the DAG on the right. This DAG is theoretically neutral. All it says is that the probability of success is influenced by the particular subject and item present in an observation. It does not further comment on the factors underlying each subject/item that lead to the results. Given the data and the right DAG, the statistical model estimates the so-called subject effects and item effects. These effects will be estimates of subject ability and item difficulty if the IRT assumptions are met: when a subject and an item influence the result only through subject ability and item difficulty. With the concepts of subject/item effects in place, we can move on to the formulas of the statistical model. Equation, Index and Annoying Things The equations in (1) are the formulation of our model. This model is known as the logistic regression, or in GLM terms, the Generalized Linear Model of the binomial family with the logit link (more on this later). Lots of things are going on here. Don’t panic, I’ll walk you through slowly. $$ \\begin{align} \u0026 R_i \\sim \\text{Bernoulli}( P_i ) \\\\ \u0026 P_i = \\text{logistic}( \\mu_i ) \\\\ \u0026 \\mu_i = \\alpha_{[S_i]} + \\delta_{[I_i]} \\end{align} \\tag{1} $$ First, note the common subscript $_i$ to the variables above. The presence of this common $_i$ indicates that the equations are read at the observational level. The observational level is easier to think of with help of the long data format. In this long form of data, each row records an observation and is indexed by the subscript $_i$. So you can think of the set of three equations as describing the links among the variables for each observation. Note that the long data format is also the format we have been using for the data frames. The last equation in (1), also related to the reading of the subscript $_i$, deserves some elaboration, as some might feel confused about the square brackets after $\\alpha$ and $\\delta$. Actually, we have already met these brackets in Part 1. The brackets here serve a similar function to R’s subset function [] that we have used for linking particular ability/difficulty levels of a subject/item to the rows (observations) of the data frame. So what the square brackets after $\\alpha$ and $\\delta$ do exactly, is to “look up” the index of the subject and item for the $_i$th observation such that the $\\alpha$ corresponding to the subject and the $\\delta$ corresponding to the item could be correctly retrieved. The model can thus “know” which $\\alpha$ and $\\delta$ to update when it encounters an observation. For instance, suppose we are on the 3rd row (observation) of the data, in which $S_3 = 5$ and $I_3 = 8$. This tells the model that the observation gives information about $\\alpha_5$ and $\\delta_8$. The model thus learns something about them and updates accordingly. I haven’t written about $\\alpha$ and $\\delta$ yet, but based on the previous paragraph, you might already know what they are: $\\alpha$s are the subject effects and $\\delta$s the item effects to be estimated by the model. Now, let me walk you through the equations from bottom to top: $\\mu_i = \\alpha_{[S_i]} + \\delta_{[I_i]}$ No surprise here. This equation simply illustrates how the model computes a new variable $\\mu$ from $\\alpha$ and $\\delta$. $P_i = \\text{logistic}( \\mu_i )$ The equation should look familiar. It indicates how the model maps $\\mu$, which can range from $-\\infty$ to $\\infty$, to probability, $P$, through the logistic function. $R_i \\sim \\text{Bernoulli}( P_i )$ This equation describes that each observed response is generated from a Bernoulli distribution with probability $P_i$. Or even simpler, $R_i$ would be $1$ with probability $P_i$ and $0$ with probability $1 - P_i$. These equations all look familiar because they are essentially mathematical representations of the simulation we have done. Here, the model formulation is simply simulation in reverse. The Logit Link The GLM formulation of (1) is often seen in an alternative form in (2). The only difference between (2) and (1) lies in the second equation. Instead of the logistic function, the second equation in (2) uses the logit function. What is the logit? $$ \\begin{align} \u0026 R_i \\sim \\text{Bernoulli}( P_i ) \\\\ \u0026 \\text{logit}(P_i) = \\mu_i \\\\ \u0026 \\mu_i = \\alpha_{[S_i]} + \\delta_{[I_i]} \\end{align} \\tag{2} $$ The logit function is simply the mirror of the logistic. They do the same mapping but in reverse directions: the logistic function maps real numbers to probabilities the logit function maps probabilities to real numbers The logistic and the logit are inverse functions to each other. So if a real number gets converted to the probability by the logistic, the logit can convert it back to the original real, and vice versa. 1logit = function( p ) log( p/(1-p) ) 2x = seq( -1, 1, by=0.1 ) 3( p = logistic(x) ) # Transformed x on probability space [1] 0.2689414 0.2890505 0.3100255 0.3318122 0.3543437 0.3775407 0.4013123 [8] 0.4255575 0.4501660 0.4750208 0.5000000 0.5249792 0.5498340 0.5744425 [15] 0.5986877 0.6224593 0.6456563 0.6681878 0.6899745 0.7109495 0.7310586 1# Logit gives x back 2logit(p) [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 [16] 0.5 0.6 0.7 0.8 0.9 1.0 Some arithmetics would get us from the logistic to the logit: $$ \\begin{aligned} \\text{logistic}(x) \u0026= \\frac{1}{1 + e^{-x}} = p \\\\ \u0026 \\Rightarrow ~~ e^{-x} = \\frac{1-p}{p} \\\\ \u0026 \\Rightarrow ~ -x = \\text{log}(\\frac{1-p}{p}) \\\\ \u0026 \\Rightarrow ~~ x = -\\text{log}(\\frac{1-p}{p}) \\\\ \u0026 \\phantom{\\Rightarrow ~~ x } = \\text{log}(\\frac{p}{1-p}) = \\text{logit}(p) \\end{aligned} $$ There is really nothing special about the logit function. We have learned all the important things through the logistic back in Part 1. I mention the logit here simply because the term is frequently used. When people talk about GLMs, they prefer to use the link function to characterize the model. The link function, in the case of the logistic regression here, is the logit function. It transforms the outcome probabilities into real numbers that are modeled linearly. It’s just the logistic, but works in the reverse direction. Fitting GLM Now, we are packed with the statistical muscles to carry out the analysis. Let’s fit the model on the data we’ve simulated. In R, this is done through the function glm(). The first argument of glm() is the formula, in which we specify our linear model with R’s model syntax. There are in principle two ways, one succinct and the other tedious, to express the formula when there are categorical predictors in the model. I will first demonstrate the tedious one, as it exposes all the details hidden by the succinct form. Though tedious, it saves us from confusion. Dummy Coding The formulas we specify in glm() (and other model fitting functions in general) correspond pretty well to their mathematical counterparts. So let me first present the math before we move on to the code. Lots of things to explain here. Equation (3.2) is rewritten from the last two equations, $logit(P_i) = \\mu_i$ and $\\mu_i = \\alpha_{[S_i]} + \\delta_{[I_i]}$ in (2), which I reproduce here in Equation (3.1) by combining the two equations. Earlier I mentioned that the square brackets after $\\alpha$ and $\\delta$ serve as a “look up” function to locate the relevant $\\alpha$ and $\\delta$ of each subject and item in an observation. There is an equivalent way to express the same formula without the use of these “look up” functions, which is shown in equation (3.2). For the sake of simplicity, let’s assume here that we have only two items (A, B) and three subjects (J, K, L). For real data, equation (3.2) would be extremely long. $$ \\begin{align} \\tag{3.1} \\text{logit}(\\mu_i) \u0026= \\alpha_{[S_i]} + \\delta_{[I_i]} \\\\ \\tag{3.2} \\text{logit}(\\mu_i) \u0026= J_i \\alpha_J + K_i \\alpha_K + L_i \\alpha_L + A_i \\delta_A + B_i \\delta_B \\end{align} $$ The variables ($J_i, K_i, L_i, A_i, B_i$) in front of the $\\alpha$s and $\\delta$s have a value of either 0 or 1. Here, they serve as a “switch” that turns on the relevant $\\alpha$ and $\\delta$ and turns off the others in each observation. This is easier to see with the help of the tables below. Table 3.1 corresponds to Equation (3.1), and Table 3.2 corresponds to Equation (3.2). So, for instance, in row 2 of Table 3.2, $K$ and $A$ are 1 while the others are 0. This turns on, or picks out, $\\alpha_K$ and $\\delta_A$. As such, they would be updated by the model when it reaches this observation. In row 2 of Table 3.1, $\\alpha_K$ and $\\delta_A$ are picked out too, but not by the switches. They are directly picked out through the K and A present in the row. Table 3.1 Table 3.2 $_i$ $S$ $I$ 1 J A 2 K A 3 L B $_i$ $J$ $K$ $L$ $A$ $B$ 1 1 0 0 1 0 2 0 1 0 1 0 3 0 0 1 0 1 The re-expression of Table 3.1 as Table 3.2 by coding the categories into zeros and ones is known as dummy coding1. Why do we need dummy coding? In short, this is because regression programs do not “understand” the difference between categorical and continuous variables. They read only numbers. Dummy coding is essentially representing categorical variables as continuous ones so that the program would know how to deal with them. Most programs dummy code for the users (such as glm()) if you give them categories. But there are various ways to dummy code the categories and each of which results in a different output. The interpretation of the output coefficients depends on how the categories were coded. This confuses the novice as too much is happening under the hood. Coding models: the long route With the concepts of dummy coding in place, let’s code the model. I use dummy_cols() from the fastDummies package to help me with dummy coding. In the code below, I recode the item and subject variables into zeros and ones. The result is identical to Table 3.2, except that it now expands to 30 items and 60 subjects (90 columns in total). I won’t print out the full dummy-coded data frame to save space. But be sure to take a look at d_dummy to see how it corresponds to Table 3.2. 1library(fastDummies) 2d_dummy = dummy_cols( d, c(\"I\", \"S\"), remove_selected_columns=TRUE ) 3dim(d_dummy) [1] 1800 91 1colnames(d_dummy) [1] \"R\" \"I_1\" \"I_2\" \"I_3\" \"I_4\" \"I_5\" \"I_6\" \"I_7\" \"I_8\" \"I_9\" [11] \"I_10\" \"I_11\" \"I_12\" \"I_13\" \"I_14\" \"I_15\" \"I_16\" \"I_17\" \"I_18\" \"I_19\" [21] \"I_20\" \"I_21\" \"I_22\" \"I_23\" \"I_24\" \"I_25\" \"I_26\" \"I_27\" \"I_28\" \"I_29\" [31] \"I_30\" \"S_1\" \"S_2\" \"S_3\" \"S_4\" \"S_5\" \"S_6\" \"S_7\" \"S_8\" \"S_9\" [41] \"S_10\" \"S_11\" \"S_12\" \"S_13\" \"S_14\" \"S_15\" \"S_16\" \"S_17\" \"S_18\" \"S_19\" [51] \"S_20\" \"S_21\" \"S_22\" \"S_23\" \"S_24\" \"S_25\" \"S_26\" \"S_27\" \"S_28\" \"S_29\" [61] \"S_30\" \"S_31\" \"S_32\" \"S_33\" \"S_34\" \"S_35\" \"S_36\" \"S_37\" \"S_38\" \"S_39\" [71] \"S_40\" \"S_41\" \"S_42\" \"S_43\" \"S_44\" \"S_45\" \"S_46\" \"S_47\" \"S_48\" \"S_49\" [81] \"S_50\" \"S_51\" \"S_52\" \"S_53\" \"S_54\" \"S_55\" \"S_56\" \"S_57\" \"S_58\" \"S_59\" [91] \"S_60\" Now, we can fit the model with the dummy-coded data d_dummy. The first argument in glm() specifies the formula in R’s model syntax. Based on Equation (3.2), we include all the dummy variables in the table. In R, this means typing out all the variables as R ~ S_1 + S_2 + ... + S_80 + I_1 + I_2 + ... + I_20. That’s a lot of work! Luckily, R provides a handy syntax for this. Since we are including all the variables except the outcome on the right-hand side of the formula, we can simply type R ~ .. Here, the dot serves as a placeholder for all the remaining variables not specified in the formula. We also need a -1 in front of the dot: R ~ -1 + .. The -1 tells the model not to estimate a global intercept, which is done by default2. We don’t need a global intercept here since we want all the effects to be presented in the items and subjects. If a global intercept is estimated, it will “suck out” what should have been part of the subject/item effects, rendering the results hard to interpret. The last thing to note in glm() is the family argument, which characterizes the type of GLM used. Since we are fitting the data with logistic regression, we pass binomial(\"logit\") to family. The GLM will then adopt the binomial distribution3 with the logit link to map the right-hand linear terms to the outcome space. 1m1 = glm( R ~ -1 + ., data=d_dummy, family=binomial(\"logit\") ) 2summary(m1) Call: glm(formula = R ~ -1 + ., family = binomial(\"logit\"), data = d_dummy) Deviance Residuals: Min 1Q Median 3Q Max -2.4291 -0.8850 0.2498 0.8978 2.7509 Coefficients: (1 not defined because of singularities) Estimate Std. Error z value Pr(\u003e|z|) I_1 1.926e+00 5.422e-01 3.553 0.000381 *** I_2 1.798e+00 5.339e-01 3.368 0.000758 *** I_3 1.154e+00 5.049e-01 2.286 0.022267 * I_4 1.154e+00 5.049e-01 2.286 0.022267 * I_5 1.351e+00 5.118e-01 2.639 0.008304 ** I_6 9.686e-01 4.997e-01 1.939 0.052559 . I_7 1.351e+00 5.118e-01 2.639 0.008304 ** I_8 1.060e+00 5.021e-01 2.111 0.034737 * I_9 9.686e-01 4.997e-01 1.939 0.052559 . I_10 1.060e+00 5.021e-01 2.111 0.034737 * I_11 5.368e-01 4.920e-01 1.091 0.275173 I_12 3.715e-01 4.905e-01 0.757 0.448848 I_13 -3.712e-02 4.901e-01 -0.076 0.939634 I_14 -3.712e-02 4.901e-01 -0.076 0.939634 I_15 1.263e-01 4.897e-01 0.258 0.796423 I_16 2.079e-01 4.898e-01 0.424 0.671244 I_17 -2.857e-01 4.922e-01 -0.581 0.561549 I_18 1.263e-01 4.897e-01 0.258 0.796423 I_19 -3.712e-02 4.901e-01 -0.076 0.939634 I_20 4.473e-02 4.898e-01 0.091 0.927249 I_21 -7.217e-01 5.000e-01 -1.443 0.148885 I_22 -1.194e-01 4.906e-01 -0.243 0.807787 I_23 -6.313e-01 4.979e-01 -1.268 0.204796 I_24 -7.217e-01 5.000e-01 -1.443 0.148885 I_25 -7.217e-01 5.000e-01 -1.443 0.148885 I_26 -7.217e-01 5.000e-01 -1.443 0.148885 I_27 -1.007e+00 5.082e-01 -1.981 0.047587 * I_28 -1.212e+00 5.159e-01 -2.350 0.018770 * I_29 -1.212e+00 5.159e-01 -2.350 0.018770 * I_30 -1.961e+00 5.585e-01 -3.511 0.000447 *** S_1 -1.800e+00 6.322e-01 -2.847 0.004415 ** S_2 1.732e+00 6.580e-01 2.632 0.008491 ** S_3 -1.575e+00 6.143e-01 -2.564 0.010355 * S_4 -8.194e-01 5.775e-01 -1.419 0.155962 S_5 -1.176e+00 5.909e-01 -1.991 0.046527 * S_6 3.276e-01 5.732e-01 0.572 0.567650 S_7 4.966e-01 5.774e-01 0.860 0.389744 S_8 -3.227e-01 5.688e-01 -0.567 0.570509 S_9 -4.853e-01 5.705e-01 -0.851 0.394979 S_10 4.966e-01 5.774e-01 0.860 0.389744 S_11 -1.176e+00 5.909e-01 -1.991 0.046527 * S_12 -1.369e+00 6.010e-01 -2.277 0.022758 * S_13 -8.194e-01 5.775e-01 -1.419 0.155962 S_14 -1.613e-01 5.682e-01 -0.284 0.776469 S_15 -1.613e-01 5.682e-01 -0.284 0.776469 S_16 -1.176e+00 5.909e-01 -1.991 0.046527 * S_17 1.253e+00 6.146e-01 2.039 0.041452 * S_18 3.276e-01 5.732e-01 0.572 0.567650 S_19 1.046e+00 6.010e-01 1.741 0.081706 . S_20 -8.194e-01 5.775e-01 -1.419 0.155962 S_21 4.966e-01 5.774e-01 0.860 0.389744 S_22 2.856e+00 8.565e-01 3.334 0.000855 *** S_23 1.479e+00 6.328e-01 2.337 0.019424 * S_24 -9.940e-01 5.833e-01 -1.704 0.088343 . S_25 -8.194e-01 5.775e-01 -1.419 0.155962 S_26 1.625e-01 5.704e-01 0.285 0.775661 S_27 -4.853e-01 5.705e-01 -0.851 0.394979 S_28 -3.227e-01 5.688e-01 -0.567 0.570509 S_29 1.680e-15 5.687e-01 0.000 1.000000 S_30 6.712e-01 5.831e-01 1.151 0.249706 S_31 6.712e-01 5.831e-01 1.151 0.249706 S_32 3.618e+00 1.111e+00 3.256 0.001131 ** S_33 -1.613e-01 5.682e-01 -0.284 0.776469 S_34 -8.194e-01 5.775e-01 -1.419 0.155962 S_35 -4.853e-01 5.705e-01 -0.851 0.394979 S_36 -4.853e-01 5.705e-01 -0.851 0.394979 S_37 1.046e+00 6.010e-01 1.741 0.081706 . S_38 -6.504e-01 5.734e-01 -1.134 0.256656 S_39 1.046e+00 6.010e-01 1.741 0.081706 . S_40 -1.575e+00 6.143e-01 -2.564 0.010355 * S_41 1.625e-01 5.704e-01 0.285 0.775661 S_42 -6.504e-01 5.734e-01 -1.134 0.256656 S_43 1.253e+00 6.146e-01 2.039 0.041452 * S_44 -1.800e+00 6.322e-01 -2.847 0.004415 ** S_45 -3.227e-01 5.688e-01 -0.567 0.570509 S_46 4.966e-01 5.774e-01 0.860 0.389744 S_47 -9.940e-01 5.833e-01 -1.704 0.088343 . S_48 3.276e-01 5.732e-01 0.572 0.567650 S_49 8.536e-01 5.908e-01 1.445 0.148546 S_50 -3.227e-01 5.688e-01 -0.567 0.570509 S_51 -4.853e-01 5.705e-01 -0.851 0.394979 S_52 -1.613e-01 5.682e-01 -0.284 0.776469 S_53 6.712e-01 5.831e-01 1.151 0.249706 S_54 2.856e+00 8.565e-01 3.334 0.000855 *** S_55 -6.504e-01 5.734e-01 -1.134 0.256656 S_56 6.712e-01 5.831e-01 1.151 0.249706 S_57 1.253e+00 6.146e-01 2.039 0.041452 * S_58 -9.940e-01 5.833e-01 -1.704 0.088343 . S_59 -4.853e-01 5.705e-01 -0.851 0.394979 S_60 NA NA NA NA --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2495.3 on 1800 degrees of freedom Residual deviance: 1927.2 on 1711 degrees of freedom AIC: 2105.2 Number of Fisher Scoring iterations: 6 Take a look at the output. Something’s strange. Since there are 30 items and 60 subjects in the data, we expect the model to return 90 coefficients, one for each subject/item effect. However, the last coefficient, S_60 in this case, is NA. The model does not estimate this coefficient. Why? Identifiability The reason is that there are infinite sets of parameter combinations that generate the same probabilities underlying our data. Thus, the model is unable to work in reverse to infer a unique set of coefficients from the data. To deal with this issue, R silently sets a constraint on the parameters: it simply drops one of the parameters and estimates the rest. When this is done, the remaining parameters become identifiable, and the model would be able to estimate them. But still, where did the infinity come from? Didn’t we simulate the data? We didn’t introduce infinity, did we? We did actually, in silence. Recall that the probability of success on an item is determined by the difference between ability and difficulty. Since it is the difference that matters, there could be an infinite number of ability and difficulty pairs that yield the same difference. By adding any common value to a pair, we get a new pair of ability and difficulty that yields the same probability. The code below demonstrates this. Here, I shift the ability and difficulty levels by a common value s. The resulting probabilities should be identical before and after the shift (except for a tiny floating point imprecision). You can play with the code below by changing the value of s. Identical results should always be yielded. 1# Shift A/D by a common factor 2s = 5 3A2 = A + s 4D2 = D + s 5 6p1 = logistic( A[d$S] - D[d$I] ) # Probabilities before shift 7p2 = logistic( A2[d$S] - D2[d$I] ) # Probabilities after shift 8sum( abs(p1 - p2) ) # Should be extremely close to zero [1] 9.453549e-14 The way R deals with this issue of identifiability is not preferable since we want to recover the parameters in our simulation (i.e., the set of parameters without the shift). To get around R’s default treatment, we have to impose constraints on the parameters ourselves. Recall that subject abilities are generated according to a standard normal distribution in the simulation. Since the standard normal has a mean of zero, the expectation of the sum of subject abilities is zero. We can use this expectation as a constraint to the parameters by constraining the subject effects to sum to zero. This constraint, however, does not scale the model’s estimates to perfectly match the true parameters since the true subject abilities never exactly sum to zero in a single run of the simulation. However, the relative scale would be close enough for the simulated parameters to be comparable to those recovered by the model. Later in the next post, when the generalized linear mixed model is introduced, you will see that there is no need to impose such a constraint. The constraint is naturally included through the model’s assumptions. The estimated subject effects then, do not need to sum to zero. Subject effects would be assumed to result from a normal distribution with a mean of zero. Through dummy coding, we can impose the sum-to-zero constraint on the subject effects. I illustrate this with the example previously presented in Table 3.2, where there are only 3 subjects and 2 items. Table 3.3 is re-coded from Table 3.2, in which the sum-to-zero constraint is imposed. The sum-to-zero constraint is imposed by dropping one of the subjects and coding the remaining as -1 for all the observations where the dropped subject is originally coded as 1. This is shown in Table 3.3, where I drop subject $L$ (hence the - in the column) and code the 3rd row of $J$ and $K$ as -1. Table 3.2 Table 3.3 $_i$ $J$ $K$ $L$ $A$ $B$ 1 1 0 0 1 0 2 0 1 0 1 0 3 0 0 1 0 1 $_i$ $J$ $K$ $L$ $A$ $B$ 1 1 0 - 1 0 2 0 1 - 1 0 3 -1 -1 - 0 1 With the coding scheme in Table 3.3, the estimated effect of subject $L$ can be reconstructed from the remaining subject effects returned by the model. Since the sum of all subject effects is zero, the effect of subject $L$ will be the negative of the others’ sum. This might seem a bit confusing. But notice that the sum-to-zero constraint simultaneously applies to all effects in the variable. Once the effect of the dropped category is reconstructed, each effect will also be the negative sum of the remaining effects. Let’s impose this constraint on the data with code. Here, I will drop the first subject S_1. You can choose any subject you like to drop, and the result will be identical. The code from line 3 to 5 below pick outs the rows where S_1 is coded as 1 and recode them as -1 on all the subject columns. After the re-coding, the final line of code then drops S_1. 1d_dummy2 = d_dummy 2toDrop = \"S_1\" 3allCategories = startsWith( names(d_dummy2), \"S_\" ) 4idx_recode = which( d_dummy2[[toDrop]] == 1 ) 5d_dummy2[idx_recode, allCategories] = -1 6d_dummy2[[toDrop]] = NULL Now, let’s refit the model with this constraint-coded data. I simply replace d_dummy with d_dummy2 in the data argument. Everything else is the same. 1m1.1 = glm( R ~ -1 + ., data=d_dummy2, family=binomial(\"logit\") ) 2# summary(m1.1) If you print out the coefficients of the fitted model, you will see that the result is what we expected. The model returns 89 coefficients, which match the number of the predictor variables we passed in. No coefficient is dropped. We already dropped it for the model. And since we dropped the predictor in a principled way, we know how to reconstruct it. The effect of the dropped S_1 will be the negative sum of the remaining. This is shown in the code below, which reconstructs all the item/subject effects from the model’s coefficients. 1eff = coef(m1.1) 2item_eff = eff[ startsWith(names(eff), \"I_\") ] 3subj_eff = eff[ startsWith(names(eff), \"S_\") ] 4# Reconstruct S_1 from the remaining subject effects 5subj_eff = c( -sum(subj_eff), subj_eff ) We can now plot the estimated effects against the true parameter values from the simulation. The figures below plot the estimated effects on the x-axis and the true parameters of the y-axis. The dashed line has a slope of 1 without an intercept. This line indicates perfect matches between the truth and the estimates. Notice that for the figure on the right, I reverse the signs of the item effects to match the scale of item difficulty. This is necessary since $D$ is subtracted from $A$ in the simulation. In other words, the effect of difficulty assumed by the 1PL model is negative: the larger the difficulty, the less probability of success on the item. However, glm() allows only additive effects. The effects in the model are summed together to yield predictions. Hence, the item effects estimated by glm() will be the negative of those assumed by the 1PL model. 1plot( subj_eff, A, pch=19, col=4 ); abline(0, 1, lty=\"dashed\" ) 2plot( -item_eff, D, pch=19, col=2 ); abline(0, 1, lty=\"dashed\" ) As seen in both figures, the dots scatter around the lines quite randomly, which indicates that the model does recover the parameters. To have a clearer view of the estimates’ accuracy, let me plot some more. 1# Set figure margins 2par(oma=c(0,0,0,0)) # outer margin 3par(mar=c(3, 4, 3, 1.6) ) # margin 4 5true_eff = c(D, A) 6est_eff = c(-item_eff, subj_eff) 7n_param = n_item + n_subj 8cols = c( rep(2, n_item), rep(4, n_subj) ) 9 10y_lim = max( abs(c(true_eff, est_eff)) ) 11y_lim = c( -y_lim-.1, y_lim+.1 ) 12plot( 1, type=\"n\", ylim=y_lim, xlim=c(1, n_param+1), ylab=\"Effect\" ) 13abline( h=0, lty=\"dashed\", col=\"grey\" ) 14abline( v=n_item+0.5, col=\"grey\") 15points( true_eff, pch=19, col=cols ) 16points( est_eff, col=cols ) 17for (i in 1:n_param) 18 lines( c(i, i), c(true_eff[i], est_eff[i]), col=cols[i] ) 19mtext( c(\"Items\", \"Subjects\"), at=c(9, 61), padj = -.5, col=c(2, 4) ) In the plot above, I overlay the estimated effects onto the true parameters. The dots are the true parameters and the circles are the model’s estimates. The vertical lines connecting the dots and the circles show the distances between the truth and the estimates. It is obvious from the plot that, compared to item difficulties, subject abilities are harder to estimate, as the distances to the truth are in general larger for subject estimates. This is apparent in hindsight, as each item is taken by 60 subjects whereas each subject only takes 30 items. Hence, the estimation for the items is more accurate, compared to the subjects, as there are more data to estimate each. The effect of manipulating the number of subjects and items is revealed in the plot below. Here, I refit the model with data that have the subject and the item numbers flipped. The subject abilities are now estimated more accurately than the item difficulties. You can experiment with this to see how the estimation accuracy changes with different combinations of subject/item numbers. The functions sim_data() and plot_estimate() in estimate-acc.R can help you with this. Coding models: the easy route We have gone through a long route, along which we have learned a lot. Now, you are qualified to take the easy route: to use R’s handy function for dummy coding. Note that this route won’t be easy at all if you never went through the longer one. Rather, confusion is all you will get, and you will have no confidence in the model you coded. Simple code is simple only for those who are well-trained. So now, let’s fit the model again. This time, we take the highway. The trick for controlling how the model functions dummy code the categorical variables is to use the contrasts() function to set up the preferred coding scheme. In the code below, I pass the number of the categories in $S$ (i.e., n_subj) to contr.sum(), which is a helper function that codes the subjects in the exact same way as we did in Table 3.3 (execute contr.sum(3) and you will see a table that corresponds exactly to Table 3.3). After the coding scheme is set, we can express categorical predictors in the model formula directly. Everything else is the same except the last line. Previously, I demonstrated dummy coding by dropping the first subject in $S$. Here, contr.sum() drops the last subject by default. Thus, the code for constructing the dropped subject is slightly different here. 1dat = d 2# Drop the last S and impose sum-to-zero constraint on S 3contrasts( dat$S ) = contr.sum( n_subj ) 4m1.2 = glm( R ~ -1 + I + S, data=dat, family=binomial(\"logit\") ) 5eff = coef(m1.2) 6item_eff.m1.2 = eff[ startsWith(names(eff), \"I\") ] 7subj_eff.m1.2 = eff[ startsWith(names(eff), \"S\") ] 8subj_eff.m1.2 = c( subj_eff.m1.2, -sum(subj_eff.m1.2) ) Now we have the estimated effects from the model, let’s check the results. The figure below plots the current estimates (m1.2) against previous ones (m1.1). The estimates from the two models agree, which confirms that the second model is correctly coded. 1est_m1.1 = c( item_eff, subj_eff ) 2est_m1.2 = c( item_eff.m1.2, subj_eff.m1.2 ) 3plot( 1, type=\"n\", xlim=c(-2.5,2.5), ylim=c(-2.5,2.5), xlab=\"m1.2\", ylab=\"m1.1\" ) 4abline( 0,1, lty=\"dashed\", col=\"grey\" ) 5points( est_m1.2, est_m1.1, pch=19, col=cols ) What’s next? This post is lengthy, but not because it is hard. Rather, the concepts presented are fairly simple. The post is lengthy because we got used to texts that hide details from the readers. The text here does the opposite: it presents all the necessary details to get the statistical model working, without confusion. People often assume that hidden details are trivial. But more often, it is just because writers are too lazy to present the details. Statistics is hard partly because it is loaded with details that are hidden and ignored. When details get ignored long enough, they accumulate to become entangled and uncrackable. Coding, again, is here to help. It dissolves the fuzziness that otherwise accumulates and hinders understanding. In Part 3, we move on to Generalized Linear Mixed Models, which are extensions to GLMs that improve estimation and efficiency by harnessing the information from common group memberships in the data. We will use the same data, and the text would be much shorter, I hope. Seeya! I use dummy coding as a general umbrella term to cover all systems for coding categorical variables. In R, what is known as “treatment coding” (contr.treatment) is sometimes called “dummy coding” by others. Here, I follow R’s convention. When “dummy coding” is used, I always refer to the general sense of re-coding categories as numbers (not necessarily zeros and ones). ↩︎ This default behavior of estimating a global intercept makes sense in the context of continuous predictors, such as the simple linear model shown below. In this case, we can succinctly express the formula as y ~ x in R’s model syntax. The estimate of the global intercept $\\alpha$ would be given as the intercept coefficient in the model output. $$ \\begin{aligned} y_i \u0026\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i \u0026= \\alpha + \\beta x_i \\end{aligned} $$ ↩︎ I haven’t mentioned the binomial distribution before. The binomial distribution is the extension of the Bernoulli distribution to $n$ independent trials. So if you repeat the Bernoulli process $n$ times and sum the outcomes, say, you toss the coin $n=10$ times and record the number of heads observed, the distribution of outcomes would follow a binomial distribution with parameters $n$ and $p$. So the Bernoulli distribution is simply a special case of the binomial distribution where $n=1$. ↩︎ ","subtitle":"IRT as Generalized Linear Models","title":"Demystifying Item Response Theory (2/4)","uri":"/2023/03/06/irt2/"},{"content":" Okay, so these are the item characteristic curves. What then? Mysterious Item Response Theory Item response theory is mysterious and intimidating to students. It is mysterious in the way it is presented in textbooks, at least in introductory ones. The text often starts with an ambitious conceptual introduction to IRT, which most students would be able to follow, but with some confusion. Curious students might bear with the confusion and expect it to resolve in the following text, only to find themselves disappointed. At the point where the underlying statistical model should be further elaborated, the text abruptly stops and tries to convince the readers to trust the results from black-box IRT software packages. It isn’t that I have trust issues with black-box software, and I also agree that certain details of IRT model estimation should be hidden from the readers. The problem is that there’s a huge gap here, between where textbooks stopped explaining and where the confusing details of statistics should be hidden. Hence, students would be tricked into believing that they have a sufficient degree of understanding, but in reality, it’s just blind faith. A sufficient degree of understanding should allow the student to deploy the learned skills to new situations. Therefore, a sufficient degree of understanding of IRT models should allow students to extend and apply the models to analyses of, for instance, differential item functioning (DIF) or differential rater functioning (DRF). I’m arguing here that there is a basic granularity of understanding, somewhat similar to the concept of basic-level category, that when reached, allows a student to smoothly adapt the learned skills to a wide variety of situations, modifying and extending the skills on demand. And I believe that item response theory is too hard for a student to learn and reach this basic level of understanding, given its conventional presentation and historical development1. There is still hope however, thanks to the development of a very general set of statistical models known as the Generalized Linear Models (GLM). Item response models could be understood in terms of the GLM and its extensions (GLMM and non-linear form of the GLM/GLMM). To be too particular about the details, the results from IRT software packages and the GLMs/GLMMs would be very similar but not identical, since they utilize different estimation methods. The strengths of the GLM, however, lie in its conceptual simplicity and extensibility. Through GLM, IRT and other models such as the T-test, ANOVA, and linear regression, are all placed together into the same conceptual framework. Furthermore, software packages implementing GLMs are widely available. Users can thus experiment with them—simulate a set of data based on known parameters, construct the model and feed it the data, and see if the fitted model correctly recovers the parameters. This technique of learning statistics is probably the only effective way for students to understand a mysterious statistical model. In this series of posts, I will walk you through the path of understanding item response theory, with the help of simulations and generalized linear models. No need to worry if you don’t know GLMs yet. We have another ally—R, in which we will be simulating artificial data and fitting statistical models along the way. Although it might seem intimidating at first, coding simulations and models in fact provide scaffolding for learning. When feeling unsure or confused, you can always resort to these simulation-based experiments to resolve the issues at hand. In this very first post, I will start by teaching you simulations. Just Enough Theory to Get Started Jargons aside, the concept behind item response theory is fairly simple. Consider the case where 80 testees are taking a 20-item English proficiency test. Under this situation, what are the factors that influence whether an item is correctly solved by a testee? Straightforward right? If an item is easy and if a testee is proficient in English, he/she would probably get the item correct. Here, two factors jointly influence the result: how difficult (or easy) the item is the English ability of the testee We can express these variables and the relations between them in the graphs below. Let’s focus on the left one first. Here, $A$ represents the ability of the testee, $D$ represents the difficulty of the item, and $R$ represents the item response, or score on the item. A response for an item is coded as 1 ($R=1$) if it is solved correctly. Otherwise, it is coded as 0 ($R=0$). The arrows $A \\rightarrow R$ and $D \\rightarrow R$ indicate the direction of influence. The arrows enter $R$ since item difficulty and testee ability influence the score on the item (not the other way around). $A$ and $D$ are drawn as a circled node to indicate that they are unobserved (or latent, if you prefer a fancier term), whereas uncircled nodes represent directly observable variables (i.e., stuff that gets recorded during data collection). This graphical representation of the variables and their relationships is known as a Directed Acyclic Graph (DAG). The DAGs laid out here represent the concept behind the simplest kind of item response models, known as the 1-parameter logistic (1PL) model (or the Rasch Model). In more formal terms, this model posits that the probability of correctly answering an item is determined by the difference between testee ability and item difficulty. So a more precise DAG representation for this model would be the one shown on the right above. Here, $P$ is the probability of correctly answering the item, which cannot be directly observed. However, $P$ directly influences the item score $R$, hence the arrow $P \\rightarrow R$. Believe it or not, the things we have learned so far could get us started. So let’s simulate some data, based on what we’ve learned about item response theory! Simulating Item Responses Simulation is playing god in a small world. Similar to model fitting, but in reverse direction. Consider the scenario where 3 students (Rob, Tom, and Joe) took a math test with 2 items (A and B). Since we play gods during simulations, we know the math ability of the students and the difficulty of the items. These ability/difficulty levels can range from positive to negative numbers, unbounded. Larger numbers indicate higher levels of difficulty/ability. In addition, the levels of difficulty and ability sit on a common scale and hence could be directly compared. Also, each student responds to every item, so we get responses from all 6 (3x2) combinations of students and items. Let’s code this in R below. The function expand.grid() would pair up the 6 combinations for us. 1D = c( A=0.4, B=0.1 ) # Difficulty of item 2A = c( R=0.5, T=0.1, J=-0.4 ) # Ability of student (R:Rob, T:Tom, J:Joe) 3 4dat = expand.grid( 5 I = c( \"A\", \"B\" ), # Item index 6 T = c( \"R\", \"T\", \"J\" ) # Testee index 7) 8dat # A tibble: 6 x 2 I T 1 A R 2 B R 3 A T 4 B T 5 A J 6 B J After having all possible combinations of the students and the items, we could collect the values of student ability and item difficulty into the data frame. 1dat$A = A[ dat$T ] # map ability to df by testee index T 2dat$D = D[ dat$I ] # map difficulty to df by item index I 3dat # A tibble: 6 x 4 I T A D 1 A R 0.5 0.4 2 B R 0.5 0.1 3 A T 0.1 0.4 4 B T 0.1 0.1 5 A J -0.4 0.4 6 B J -0.4 0.1 Now, we’ve got all the data needed for simulation, the only thing left is to precisely lay out the rules for generating the response data $R$—the scores (zeros and ones) on the items solved by the students. We are two steps away. Generating Probabilities When IRT is introduced in the previous section, I mention that the probability of successfully solving an item is determined by the difference between testee ability and item difficulty. It is straightforward to get this difference: simply subtract $D$ from $A$ in the data. This would give us a new variable $\\mu$. I save the values of $\\mu$ to column Mu in the data frame. 1dat$Mu = dat$A - dat$D 2dat # A tibble: 6 x 5 I T A D Mu 1 A R 0.5 0.4 0.1 2 B R 0.5 0.1 0.4 3 A T 0.1 0.4 -0.3 4 B T 0.1 0.1 0 5 A J -0.4 0.4 -0.8 6 B J -0.4 0.1 -0.5 From the way $\\mu$ is calculated ($A$ - $D$), we can see that, for a particular observation, if $\\mu$ is positive and large, the testee’s ability will be much greater than the item’s difficulty, and he would probably succeed on this item. On the other hand, if $\\mu$ is negative and small, the item difficulty would be much greater in this case, and the testee would likely fail on this item. Hence, $\\mu$ should be directly related to probability, in that $\\mu$ of large values result in high probabilities of success on the items, whereas $\\mu$ of small values result in low probabilities of success. But how exactly is $\\mu$ linked to probability? How can we map $\\mu$ to probability in a principled manner? The solution is to take advantage of the logistic function. $$ \\text{logistic}( x ) = \\frac{ 1 }{ 1 + e^{-x} } $$ The logistic is a function that maps a real number $x$ to a probability $p$. In other words, the logistic function transforms the input $x$ and constrains it to a value between zero and one. Note that the transformation is monotonic increasing, meaning that a smaller $x$ would be mapped onto a smaller $p$, and a larger $x$ would be mapped onto a larger $p$. The ranks of the values before and after the transformation stay the same. To have a feel of what the logistic function does, let’s transform some values with the logistic. 1# Set plot margins # (b, l, t, r) 2par(oma=c(0,0,0,0)) # Outer margin 3par(mar=c(4.5, 4.5, 1, 3) ) # margin 4 5logistic = function(x) 1 / ( 1 + exp(-x) ) 6x = seq( -5, 5, by=0.1 ) 7p = logistic( x ) 8plot( x, p ) As the plot shows, the logistic transformation results in an S-shaped curve. Since the transformed values (p) are bounded by 0 and 1, extreme values on the poles of the x-axis would be “squeezed” after the transformation. Real numbers with absolute values greater than 4, after transformations, would have probabilities very close to the boundaries. Less Math, Less Confusion For many students, the mathematical form of the logistic function leads to confusion. Staring at the math symbols hardly enables one to arrive at any insightful interpretation of the logistic. A suggestion here is to let go of the search for such an interpretation. The logistic function is introduced not because it is loaded with some crucial mathematical or statistical meaning. Instead, it is used here solely for a practical reason: to monotonically map real numbers to probabilities. You may well use another function here to achieve the same purpose (e.g., the cumulative distribution function of the standard normal). Generating Responses We have gone all the way from ability/difficulty levels to the probabilities of success on the items. Since we cannot directly observe probabilities in the real world, the final step is to link these probabilities to observable outcomes. In the case here, the outcomes are simply item responses of zeros and ones. How do we map probabilities to zeros and ones? Coin flips, or Bernoulli distributions, will get us there. Every time a coin is flipped, either a tail or a head is observed. The Bernoulli distribution is just a fancy way of describing this process. Assume that we record tails as 0s and heads as 1s, and suppose that the probability $p$ of observing a head equals 0.75 (since the coin is imbalanced in some way that the head is more likely observed and we know it somehow). Then, the distribution of the outcomes (zero and one) will be a Bernoulli distribution with parameter $P=0.75$. In graphical terms, the distribution is just two bars. We’ve got all we need by now. Let’s construct the remaining columns to complete this simulation. In the code below, I compute the probabilities (P) from column Mu. Column P could then generate column R, the item responses, through the Bernoulli distribution. 1rbern = function( p, n=length(p) ) 2 rbinom( n=n, size=1, prob=p ) 3 4set.seed(13) 5dat$P = logistic( dat$Mu ) 6dat$R = rbern( dat$P ) # Generate 0/1 from Bernoulli distribution 7dat # A tibble: 6 x 7 I T A D Mu P R 1 A R 0.5 0.4 0.1 0.525 0 2 B R 0.5 0.1 0.4 0.599 1 3 A T 0.1 0.4 -0.3 0.426 0 4 B T 0.1 0.1 0 0.5 0 5 A J -0.4 0.4 -0.8 0.310 1 6 B J -0.4 0.1 -0.5 0.378 0 Now, we have a complete table of simulated item responses. A few things to notice here. First, look at the fourth row of the data frame, where the response of testee T (Tom) on item B is recorded. Column Mu has a value of zero since Tom’s ability level is identical to the difficulty of item B. What does it mean to be “identical”? “Identical” implies that Tom is neither more likely to succeed nor to fail on item B. Hence, you can see that Tom has a 50% of getting item B correct in the $P$ column. This is how the ability/difficulty levels and $\\mu$ are interpreted. They are on an abstract scale of real numbers. We need to convert them to probabilities to make sense of them. The second thing to notice is column R. This is the only column that has randomness introduced. Every run of the simulation would likely give different values of $R$ (unless a random seed is set, or the P column consists solely of zeros and ones). The outcomes are not guaranteed, probability is at work. The presence of such randomness is the gist of simulations and statistical models. We add uncertainty to the simulation, mimicking the real world, to know that in the presence of such uncertainty, would it still be possible to discover targets of interest with a statistical model. Randomness, however, poses some challenges for coding. We need to equip ourselves for those challenges. Coding Randomness Randomness is inherent in simulations and statistical models, so it is impossible to run away from it. It is everywhere. The problem with randomness is that it introduces uncertainty in the outcome produced. Thus, it would be hard to spot any errors just by eyeballing the results. Take rbern() for instance. Given a parameter $P=0.5$, we can repeatedly run rbern(0.5) a couple of times to produce zeros and ones. But these zeros and ones cannot tell us whether rbern(0.5) is working properly. rbern(0.5) might be broken somehow, and instead generates the ones with, say, $P=0.53$. The law of large numbers can help us here. Since rbern() generates ones with probability $P$, if we run rbern() many times, the proportion of the ones in the outcomes should converge to $P$. To achieve this, take a look at the second argument n in rbern(), which is set here to repeatedly generate outcomes ten thousand times. You can increase n to see if the result comes even closer to $0.5$. 1outcomes = rbern( p=0.5, n=1e4 ) # Run rbern with P=0.5 10,000 times 2mean(outcomes) [1] 0.4991 A more general way to rerun a chunk of code is through the for loop or convenient wrappers such as the replicate() function. I demonstrate some of their uses below. 1outcomes = replicate( n=1e4, expr={ rbern( p=0.5 ) } ) 2mean(outcomes) [1] 0.5027 1# See if several runs of rbern( p=0.5, n=1e4 ) give results around 0.5 2Ps = replicate( n=100, expr={ 3 outcomes = rbern( p=0.5, n=1e4 ) 4 mean(outcomes) 5}) 6 7# Plot Ps to see if they scatter around 0.5 8plot( 1, type=\"n\", xlim=c(0, 100), ylim=c(0.47, 0.53), ylab=\"P\" ) 9abline( h=0.5, lty=\"dashed\" ) 10points( Ps, pch=19, col=2 ) Randomness in Models The method described above works only for simple cases. What about complex statistical models? How do we test that they are working as they claim to? Guess what? Simulation is the key. We simulate data based on the assumptions of the statistical model and see if it indeed returns what it claims to estimate. The simulation can be repeated several times, each set to different values of parameters. If the parameters are always recovered by the statistical model, we can then be confident that the model is properly constructed and correctly coded. So simulation is really not an option when doing statistics. It is the only safety that helps us guard against bugs in our statistical models, both programmatical and theoretical ones. Without first testing the statistical model on simulated data, any inferences about the empirical data are in doubt. What’s next? In a real-world scenario of the example presented here, we would only observe the score ($R$) of an item ($I$) taken by a testee ($T$). The targets of interest are the unobserved item difficulty ($D$) and testee ability ($A$). In Part 2, we will work in reverse and fit statistical models on simulated data. We will see how the models discover the true $A$s and $D$s from the information of $R$, $I$, and $T$. See you there! I mean, what the hack is Joint and Conditional Maximum Likelihood Estimation? These are methods developed in the psychometric and measurement literature and are hardly seen in other fields. Unless obsessed with psychometrics, I don’t think one would be able to understand these things. ↩︎ ","subtitle":"Playing God through Simulations","title":"Demystifying Item Response Theory (1/4)","uri":"/2023/02/25/irt1/"},{"content":"I have heard of the use of GNU Make for enhancing reproducibility for some time. I did not incorporate Make into my work, however, since a simple build script written in Bash was sufficient. Everything was well in control, and I could structure the workflow to my will.\nIt was not until I started working in a company setting that I found most things out of my control. Decades of conventions have been accumulating and passing on, and personal workflows have to fit into existing ones. In order to fit into my company’s conventions of data analysis (which pretty much just ignore analysis reproducibility), the number of scripts grew exponentially and quickly fell out of my control (see figure below, which is automatically generated by Snakemake). I needed a way to document and track my workflow in a consistent and scalable manner. This was when I picked up Prof. Broman’s great introductory post on GNU Make again. Everything seemed hopeful, but I was soon defeated by the omnipresent Windows. Since it is required to work on Windows machines in my company, and since Make for Windows has difficulties dealing with Chinese file paths, I had to give up on Make. Snakemake came as my savior.\nData analysis workflow graph generated by Snakemake Meeting Snakemake Snakemake was inspired by, but way more complicated than, GNU Make. Since it is backed by Python, cross-platform issues such as character encodings are automatically resolved. Snakemake is a thoughtful project that was originally developed to facilitate computational research and reproducibility. Thus, it may take some time to get started since there are many concepts to pick up. It’s totally worth it, however. Dealing with complex tasks requires a complicated framework. Often, these complications make sense (and are appreciated) only after we face real-world complexities. Going through Snakemake’s tutorial and experimenting with it on the computer would be sufficient to get an average user started. It is not as complicated as it seems at first glance.\nSnakemake Recommended Workflow A great thing about Snakemake is that it is opinionated. This means that certain conventions1 are proposed, and most users would benefit from them since they spare the burden of planning and creating workflow structures.\nFor instance, Snakemake recommends the directory structure listed below for every Snakemake workflow. This structure is so simple that its genius might not be obvious at first glance. There are four directories in the project root—workflow/, config/, results/, and resources/. workflow/ holds the coding stuff. Code for data analysis, computation, and reproducibility are all found in this directory. config/ is for optional configuration and I would skip it here (in my own project, I did not use config files since the Snakefile is sufficient for my purposes). results/ and resources/ are what (I think) make this structure fantastic. resources/ holds all raw data, i.e., data that are not reproducible on your computer (e.g., manually annotated data). All data resulting from the computation in the current project are located in results/. So ideally, you could delete results/ at any time without worry. A single command snakmake -c should generate all the results from resources/. The genius of this structure is that it eliminates the need to worry about where to place newly arrived data, as commonly encountered in real-world situations (e.g., an analysis might require data that you did not foresee).\n1├── .gitignore 2├── README.md 3├── workflow 4│ ├── rules 5| │ ├── module1.smk 6| │ └── module2.smk 7│ ├── scripts 8| │ ├── script1.py 9| │ └── script2.R 10| └── Snakefile 11├── config 12│ └── config.yaml 13├── results 14└── resources An Enhanced Snakemake Workflow I adopted the workflow above in my work. It was great, but I still found two annoying drawbacks.\nLong directory names\nSince in a Snakefile, file paths of inputs and outputs are always repeated, it soon becomes annoying to type in paths starting with resources/... and results/.... In addition, “resources” and “results” have a common prefix, which often confuses me. It would be better if the two terms were more readily distinguished visually.\nConfusing relative paths\nAccording to the documentation, relative paths in different directives are interpreted differently. To be short, relative paths in input:, output:, and shell: are interpreted relative to the working directory (i.e., where you invoke the command snakemake -c), whereas in directives such as script:, they are interpreted as relative to the Snakefile. So it would be cognitively demanding to switch back and forth between the reference points of relative paths while writing the Snakefile. Why not have all paths relative to the project root?\nTo deal with the aforementioned problems, I modified the recommended directory structure and arrived at the structure below:\n1├── README.md 2├── Snakefile 3├── made 4├── raw 5└── src Simplified directory names\nresources/ is renamed as raw/, and results/ is renamed as made/. The workflow/ directory is broken down into src/ (holding scripts) and the Snakefile.\nConsistent relative paths\nSince Snakefile is now placed in the project root, the problem of different relative paths for different directives is resolved, as long as the user always invokes the command snakemake -c in the project root.\nThe source code of this Snakemake workflow can be found here on GitHub.\nSome Notes for Using Git-Bash as Shell The experience of using Snakemake on Windows is great overall. I ran into a few problems, but the problems were usually solvable. There is one particular problem that took me a while to solve. On Windows, the default shell executable used in Snakemake (and Python) is Cmd (or maybe Powershell). However, since I am more familiar with Bash and Unix tools, it is a real inconvenience. I had set up Git-Bash on the company’s Windows machine but then spent a long time figuring out how to set Git-Bash as the default shell in Snakemake. The information for Snakemake users on Windows is scarce. I guess Snakemake is just unpopular among Windows users. After reading the source code, the solution turned out to be quite simple. Just put the code below at the top of the Snakefile and place the path to Git-Bash executable in shell.executable(). This will allow the identical Snakefile to be used on both Windows and Unix-like computers without additional configurations.\n1# Additional setup for running with git-bash on Windows 2if os.name == 'nt': 3 from snakemake.shell import shell 4 shell.executable(r'C:\\Users\\rd\\AppData\\Local\\Programs\\Git\\bin\\bash.exe') “Good” conventions here, as opposed to naturally resulting conventions without the consideration of reproducibility. ↩︎\n","subtitle":"Also for Unix users forced to work on Windows","title":"A Minimalist Structure for Snakemake","uri":"/2023/02/15/snakemake/"},{"content":"下文分享的是兩個非常私密、強烈且痛苦的經驗。但同時，它們也是我最珍視、最感激的兩個經驗。因為這些經驗，我才有機會真正去學習同理。\n情緒失控的爭執 或許是在大四的某個夜晚，記憶中我剛回到家，從公寓的電梯走出來時便聽到母親不耐的說話聲，似乎又在抱怨沒人下樓幫她搬採買的雜物。 同樣習以為常的是，父親用厭煩並帶著略為憤怒的語氣回應著母親。與平常不同的是，今天的母親不甘默默吞下父親的言語攻擊，而選擇反擊父親。\n「神經病」，母親以微弱但又正好能讓父親聽到的音量碎唸著。\n「妳說什麼？」，父親大聲喝斥，今日他也比平常來得更暴躁。\n母親不甘示弱地提高了聲音：「我說你神經病」。\n父親原本站在他房間的門口，聽到這句話時便走向母親，停在客廳放有玻璃水瓶的桌子前。此時我已進屋，看著站在前方的母親以及右前的父親怒目對視。\n「妳再說一次看看！」父親憤怒地吼著，並握起桌上的玻璃瓶。我的胸口此時宛如壓著一塊石頭，吸氣吐氣變得異常困難。\n母親冷冷地複述了那三個字。\n「閉嘴！」，父親怒不可遏。\n「神經病」，母親依然不讓步地複述。\n他們每往來一回，我胸口上的石頭就彷彿多了一個，根本無法呼吸。我似乎感覺到極度可怕的事情即將發生，卻毫無能力阻止它發生。\n接下來，我看著父親失控地衝向母親和我而來。唯一慶幸的是，父親似乎沒有完全失控，還記得將手上的玻璃瓶換成桌旁的畚斗。\n下一秒，我跨出一步站到母親前方，希望擋住即將砸來的畚斗。\n畚斗在我腳邊砸下，碎了滿地。父親轉身快步走回房間甩上房門。母親隨後也走出家門。\n這一切發生的太快，我清掃了地板的碎片，仍遲遲無法平靜下來。半小時後，姊姊回到家，我向她述說剛才發生過的事情。我以為我能順暢地描述整個過程，然而在描述時，我卻哭了出來。我努力想要控制淚水 (平常很容易做到)，卻發現眼淚只是更猛烈地湧出，哭到連嘴巴都合不起來。後來我才意識到，這是恐懼。這是面對暴力卻無能為力的感受、面對熟悉環境卻發現其實毫無掌握能力的絕望。我想，這件事唯一讓我感到幸運的是，它發生在我已經長到夠大、有足夠成熟的心智在事後好好梳理這個經驗之時。\n學校泳池的攻擊事件 一年多前，我的一位朋友 (之後代稱為 A) 經歷了一場駭人的攻擊事件。那是一個大太陽的日子，A 在台大的戶外泳池游泳。游完上岸後，A 走進泳池旁的廁所/更衣區準備沖洗。這天來游泳的人很少，偌大的更衣區沒有其他人，除了 A 以及另一個尾隨進來的陌生男子。或許是因為有許多煩惱的事情縈繞心頭，A 一開始並未注意到有人尾隨著他。下一秒，站在淋浴間的 A 意識到門被一名男子拉開，男子接著將臉貼上 A 的胸部然後開始瘋狂吸吮。A 用力將男子推開，並試圖離開淋浴間，但男子又將他拉了回來，如此反覆了數次。終於，A 掙脫了男子跑回泳池旁。隨後，那名男子若無其事地回到泳池下水繼續游泳。A 感受到一陣劇烈的噁心，回到淋浴間開始沖洗身體，猛力沖洗剛剛被變態吸吮的部位。這時，他才發現到自己的胸部多了一道咬痕…\n我是在咖啡廳聽 A 描述著他的經歷。聆聽的當下，周圍環境宛如是靜止的，A 的一字一句即時地化為眼前的畫面與耳中的聲響，我彷彿站在 A 的軀殼裡經歷著這一切。當那名男子用力吸吮著胸口、在嘗試掙脫未果又被拉了回來之時，我感受到的是時間變得無限緩慢、被一股巨大無比的力量壓制著、對身體及周遭毫無掌控的不知所措。緊接而來的就是強烈的恐懼以及無力感。這種似曾相似的感覺，上次出現在我爸失控砸碎畚斗之時。唯一不同的是，我確切地知道 A 實際上所承受的，要比我強上好幾倍，我只能夠感受到他所經歷到的一小部份而已。\n同理的組成 在經歷過這兩個事件之後，我才親身體驗到原來同理竟是如此困難。若非這些經歷，至今我大概仍無法同理他人。同理很難，因為它涉及兩個重要的元素。第一個元素，我稱之為「消極同理」，是指因具備類似的經驗而能感同身受的理解對方，因為自己曾經走過一次；第二個元素，我稱之為「積極同理」，是指自願且刻意地以對方的視角去經驗對方所經驗到的，換言之，即是去想像與模擬對方的主觀視角，嘗試在自己腦海中走過一次。而同理最困難的，就是這第二個部份，因為除了要投注大量精神去面對陌生的經驗，還要先放下自己所有的預設、評價與標籤。有了這層的認識之後，我才發現我們平常所講的同理，常是指涉「消極同理」。我們常假定，只有在經歷過與對方相同或是非常類似的經驗時，同理才有可能。然而，即使從未有過類似的經歷，「積極同理」也足以讓我們去感同身受。實際去「積極同理」時，大概會發現自己在腦中模擬對方經驗時，依然會感覺到許多熟悉的情緒與知覺。例如，雖然我從未遭受過性暴力攻擊，但透過「積極同理」，我依然能夠經驗到面臨暴力時的恐懼與無力，因為我曾在其它地方經驗過表面看似差異極大，但實則有不少相似之處的事件。同理是發生在感受的層次，而不是客觀世界的相似性，正是因為如此，縱使每個人的境遇如此不同，我們依然有潛力去理解各式各樣的人。\n「積極同理」還有一個困難的地方在於雙方的信任。由於同理時，需要盡可能在腦海中如實重建對方的經驗，過程中需要知道許多細節。對方或因為不信任我或因為不相信我在意，可能會略去許多重點不提；另一方面，當意識到資訊不足時，我們是否能懷著不怕對方拒絕的勇氣，去探尋對方刻意掩蓋的內容1，且同時又能讓對方相信自己不是為了評判、指教或八卦而是因為真的在意？簡言之，同理需要有能力創造出一個安全的空間，讓對方能夠安心說出所有必要的細節，如此，接下來的感同身受才有可能。\n同理雖然困難，但卻是生而為人最可貴的一項能力。人類所面臨的各種悲慘境況 (例如, 戰爭、暴力、歧視與偏見)，幾乎都源自於某些人缺乏或拒絕同理。而一個人最需要的，我也覺得是 (被) 同理，因為同理的本質即是去感受對方所感受到的，而只有在感受的層次上，人才有可能真正溝通、理解、建立連結與善待對方。然而人類歷史的發展，卻將我們往同理的反向推動。如今，我們的社會文化鼓勵著人們追求各種「可見的」事物。從實體的金錢財產到抽象的快樂、名聲、地位與人際關係，一切都被量化與具象化，彷彿完整收集到這些「物品」，就能滿足個人的所有需求 (例如，我們竟然用「經營」的概念來理解人際關係的建立與維持)。追求這些事物的結果是引來了無止盡的競爭 (公司獎金給了同事就給不了我、同學拿了第一名其他人就只能陪襯)，同時也在每個人心中深深埋下「零和賽局」的概念 (你得即我失，你失即我得)。於是，同理變得越來越稀有，有些人未曾嘗試過同理，有些人則選擇關掉同理，因為同理「不合理」——「你心情不好是你的問題，為何要犧牲我的快樂來陪你難過？」。同理最神奇的地方，就是它不適用上述的邏輯，而且只有實際自己體驗之後才能真正理解——同理他人時，的確需要承受不小的苦痛，因為你正感受著對方的痛苦，你正與對方合而為一；然而矛盾的是，正是因為這種合而為一，你同時也能感覺到自己「變大」了，變得更寬心、更有力量、更有空間能容納各種悲歡離合；在這個空間之下，好的、壞的、愉快的、悲傷的、痛苦的，全都是珍貴而富有意義的，我納入對方整合成一個更大的我，一同接納與感受生命帶來的種種。同理與被同理的一方，都會過得更好。\n結語 今年一月時寫了兩篇簡短的文章，藉由電影《命運規劃局》與《異星入境》分別去談我所理解的「愛」與「接納」。如今加上這篇的內容 (拖了三個月XD)，終於湊齊了當初說好的三部曲——「愛」、「接納」與「同理」。三部曲，因為這三件事情彼此之間環環相扣。首先，接納是同理的前提，若無法不帶評斷且如實地接納現實所帶來的一切，那我們將無法與對方站在一起，同理也就不可能發生。而愛若不是建立在同理之上，就會淪為一段只能同甘卻無法共苦——或是，以一方之苦換取另一方之甘——的關係，雙方被囚禁於一個狹小的空間，不斷以愛之名去取悅自己或是取悅對方。「愛」、「接納」與「同理」是我這幾年來學到最重要的事情 (之後也仍需繼續邊實踐邊學習)，它們無法透過書本或課堂習得，而必須透過不斷的實踐，去反覆體會與感受。\n「積極同理」的這種常常需要「一直往深處鑽」的特性，凸顯了一項重要的事實——客觀上，世界上的每個人都不同，然而在感受的層次上，所有人都是類似的。這正好可以用來對比某個荒謬的信念——伴侶/朋友要找跟自己相似的，最好有相同的興趣、個性、價值觀……。我覺得這信念荒謬的地方在於，它背後隱含著每個人都是某種商品，有著各式的功能與特性，而只要在市場上找到匹配的商品，就能夠解決所有人際交流的問題。然而事實是，縱使兩人有眾多相似之處，讓他們在相處上十分輕鬆愉快，只要他們活在現實世界中，必然會遇到有所衝突的地方。只有同理才有辦法帶著雙方跨越衝突的鴻溝，因為同理讓人們得以往深處鑽，而只有在內心深處的這個空間中，我們才能發現彼此是相同的、是一體的。當我們進入到同理的空間時，衝突自然而然就會消弭無蹤。 ↩︎\n","subtitle":"對於同理的一些理解與體悟","title":"最珍視的經驗","uri":"/2022/04/12/empathy/"},{"content":"這邊要來推薦《豬殺令》，大概是這兩三年來我最愛的一部電影。這是一部平靜下來去感受，會內心暖呼呼的電影——不是催淚彈，也沒有大起大落的強烈情緒，但就是樸實真摯的情感緩慢而扎實地在心中逐步擴張，一部觸動靈魂又洗滌心靈的電影。\n下方是我看完電影後的理解與詮釋，內容基本上已經含蓋了重要的劇情。這部電影其實不怕劇情雷，但很怕詮釋雷，所以強烈建議在未看過電影前不要閱讀下方內容。\n前一陣子看了兩次《豬殺令》，在腦袋中又反芻了無數次，真的超愛這部電影！仔細思索故事內容時，發覺這部電影跟《高年級實習生》有些類似，編導似乎都想將心目中的「理想人格」，呈現在片中的主角——我覺得《高年級實習生》的 Ben 以及《豬殺令》的 Robin 分別就是兩部電影中的理想人格。\n在《豬殺令》中，Robin 看似是一個與社會脫節的「怪人」。他隱居在山中的簡陋小屋、不修邊幅、靠著採集松露為生。採到的松露不是拿去賣錢，而是透過以物易物的方式跟中盤商 Amir 交換生活必需品 (多數為食物)。Robin 下山去尋找他被綁架的松露豬時的各種場面，看起來就像是個不可理喻的瘋子，世間的「道理」(例如，松露豬不見再買一隻就好)、城鎮內部運作的「潛規則」(例如，某人權勢很大你惹不起) 都無法阻攔 Robin 尋找他的豬。原因很簡單，因為 Robin 不是為了採集松露才需要豬，他是因為愛那隻豬才需要她。\nRobin 這角色所體現的是那些對自己非常誠實、堅毅地相信自己的價值並且依循這些價值而行動的人，所以他看起來會與這個社會格格不入——不是因為 Robin 很「怪」，而是因為他太真實而這個社會太虛偽了；他知道社會上有太多所謂的「道理」與「規則」並非源自人們最真摯的自己，有太多時候，人們為了依循社會認為的「合理」而對自己內心深處最真實的感受視而不見。所以，Robin 才會對以前被他開除的廚師 Derek 說出這段醍醐灌頂的話 (Derek 年輕時想開酒吧，但最後服膺於社會價值，封閉了這理想而選擇開了能賺錢與累積名聲的高級餐廳)：\nThey’re not real. You get that, right?\nNone of it is real.\nThe critics aren’t real…\nthe customers aren’t real…\nbecause this isn’t real.\nYou aren’t real. Derek, why do you care about these people?\nThey don’t care about you, none of them.\nThey don’t even know you because you haven’t shown them.\nEvery day, you wake up and there’ll be less of you.\nYou live your life for them, and they don’t even see you.\nYou don’t even see yourself.\nWe don’t get a lot of things to really care about.1\n在故事後半段時，Robin 說服 Darius (綁架豬的主使者，也是 Amir 的父親) 說出豬的下落時，也完全依循著自己所擁抱的價值。Robin 第一次去找 Darius 理論時，Darius 只想用錢打發他，完全不告訴他豬的下落、不願意敞開心胸與他對話。多年前 Darius 與妻子曾吃過 Robin 煮的一頓晚餐，那晚他和妻子非常開心，菜餚的美味讓他們即使多年後依舊時常提到這頓晚餐。後來，Darius 的妻子因自殺未遂成為植物人長年臥床，Darius 或因妻子或因事業，也變得冷淡、強悍、變成情感的絕緣體。Robin 透過 Amir 知道了這些事，也清楚地記得自己當年為他們做過菜 (這件事反映 Robin 是多麼熱愛自己廚師的工作——他記得自己煮過的每一頓飯以及服務過的每一位客人)。於是，Robin 決定為 Darius 做他和妻子當年吃過的那頓飯——Darius 嚐了主菜，喝了紅酒……這些熟悉的味道喚起了當年的記憶，跟隨記憶而來的是汩汩湧出的強烈情感。在感受到壓抑多年後湧出的情感，Darius 與 Robin 終於能站在同一個平台上對話。此刻的 Darius 才真正感受到 Robin 真誠而簡單的動機——對豬的愛，就如同自己對於妻子的愛一樣。在這些誠懇真實的情感之下，世俗的規則、利益、權勢、面子、似是而非的理由……都顯得微不足道，唯一重要的是以真相回應 Robin 的真誠。然而不幸的是，真相是 Robin 的豬在被綁架的過程中受傷身亡了。\nRobin 得知豬死掉之後的最後這段劇情又將故事的層次推高了一層。整個故事中，可以看到 Robin 是一個對自己非常誠實的人，但即便誠實如 Robin，仍有不願直視、不願面對的真相。Robin 這十五年來會躲進山中隱居，就是因為妻子的逝去。他離開與妻子共同生活的城鎮，好遠離各種觸景傷情。對於妻子的情感，則轉而寄託於他的松露豬上。為了尋找被綁架的豬，Robin 被迫回到原本生活的城鎮，在這個過程中，他其實也正慢慢地、下意識地開始面對失去妻子的傷痛 (例如，發覺故居院子中的柿子樹已然不在，一旁的孩子問他說樹是死掉了嗎)。在故事接近尾聲時，Amir 與 Robin 坐在小餐館裡，Robin 仍對於豬的死去無法釋懷，碎念著：“If I never came looking for her, in my head, she’d still be alive.”，而 Amir 的一句話點醒了 Robin： “But she wouldn’t be.”。正是這句話讓 Robin 意識到豬的死去已是事實，一項他無論如何幻想也無法改變的真相。他必須懷著這份傷痛，才能真實地活在這個世上。在意識到並接受自己永遠失去豬的同時，Robin 也終於接受了自己永遠失去了妻子的事實。\n這段 Derek 的表情從笑臉迎人、困惑、面有難色、強顏歡笑，到最後快哭出來，真的太精彩了… ↩︎\n","subtitle":"電影《豬殺令》的解析與詮釋","title":"用最純粹的真實打動世界","uri":"/2022/03/11/pig/"},{"content":"以前在能量小姐 (低 GI 廚房) 買餐盒時，總是想嚐嚐它的剝皮辣椒雞湯，但一直覺得頗貴，所以最後總是沒買。今天中午在喝自己煮的剝皮辣椒雞湯時，突然覺得下次買的下去了。過去一直買不下去，背後依循的邏輯是「傷荷包的痛苦大於預期雞湯所能帶來的滿足」，簡單來說，當時的剝皮辣椒雞湯僅僅被化約成價格與享樂而已。如今，在自己動手煮過剝皮辣椒雞湯之後，開始與雞湯有其它的連結了。\n由於湯是自己煮出來的，在品嚐時，便較能將湯中 (原本無法察覺) 的各種味道對應至原材或是烹飪步驟——「香菇似乎放太多了，蓋去辣椒以及雞肉的味道」、「味道有些失和，似乎酸味多了點，或許剝皮辣椒的醃汁能少加一點」、「整體香氣不足，或許雞腿得再煎焦一點、香菇得再煸久一點」。 這些經驗與連結也能帶到其他人煮的剝皮辣椒雞湯上。因此，品嚐剝皮辣椒雞湯不再只是欲望的滿足與消退，現在多了更持久、更有意義的東西——首先，我可能意識到這碗雞湯超好喝，根據湯裡的料或味道我或許可以推測出它好喝的原因、它比自己做的「多」出了什麼、它如何完美調和各種食材的味道；也或許，憑我自己無法得出湯好喝的秘訣。這時，我可能需要向做出這碗湯的人討教才得以解惑。在此，我開始對背後產出這碗湯的一切感到興趣了，雞湯不再只是喝的好爽而已，裡頭更濃縮著活生生的人所投注的精神以及一代代傳承下來的知識經驗。湯 (以及任何非生產線做出來的東西) 是人的延伸。\n後來才驚覺，這不就是異化的概念嗎XD 透過做飯真的學到好多東西啊😂\n","subtitle":"","title":"剝皮辣椒雞湯的啟示","uri":"/2022/03/08/chicken-soup/"},{"content":" Despite knowing the journey and where it leads, I embrace it. And I welcome every moment of it. — Arrival (2016) 電影《異星入境》中，主角 Louise 即使能預見未來女兒會因罕見疾病而早逝，也知道丈夫會因此而離開，她仍然選擇走上這條路，不去更改她早已預知的未來。為什麼？ 我起初認為是因為與女兒相處的經驗太過珍貴，讓 Louise 寧願面對失去的疼痛也不願犧牲掉這些更美好的經驗。後來發現，這解釋背後其實有個非常大的假設—正向經驗比起負向經驗來得「重要」，理性的人們應該會追求「正向 - 負向 \u003e 0」的人生。後來有些長進之後，才發現這個假設應該恰恰與 Louise 的信念完全相反。我想 Louise 相信的是，各種經驗都同等珍貴，不論是歡喜、悲痛、迎接新生命還是面對生命的消亡。即使是心碎之下，還是有感激的時刻。而她是在看遍自己的一生之後，才得出這個結論的。她選擇去用心擁抱生命賜予她的每一個時刻。 或許最難以理解的是，擁抱負面經驗的意義何在？一個合理的解釋是，「辛苦過後的果實最甜美」，光明需要有黑暗的對比才顯得珍貴，所以負面經驗是為了襯托正面經驗才有意義。 但我覺得不僅是如此。 擁抱負面經驗本身就是有意義的。透過這類經驗，我們才能體會到自己的渺小、才能夠謙遜；透過細心品嚐自己的痛苦，我們才能學會擁抱自己，也才能真正理解別人的苦痛、才有辦法同理他人。這些經驗本身雖然不愉快，但正是因為這些經驗，生命才能有溫度，我們才能感受到活著的感覺。 電影 然而，社會與家庭告訴我們的，往往是相反的。我們被鼓勵去擁抱「好」的經驗，去避開「不好」的狀態。在我們能真正平等去感受各種經驗之前，早已根深柢固地認為只有正向經驗才可能有意義，負向經驗只會帶來傷害，一遇到就要趕緊逃開、脫離這種狀態。有時候甚至認為自己處在負面狀態時是不道德的，因為這會帶給親友困擾。然而實際的情況是，越是迴避負面經驗，我們越可能因此受害，而這正是經驗性迴避 (Experiential Avoidance) 告訴我們的事情。這概念是指人們企圖去避免感受特定經驗的傾向，例如，面對負面的想法、記憶、情緒以及身體感受時，人們常會去壓抑它們以避免經驗到這些感受。但這麼做卻會帶來更大的傷害。例如，因為失去孩子太過痛苦而透過酒精與藥物麻痺自己的感受；因為認知到自殺念頭的可怕而拼命壓抑它，反而更容易擔心自殺 (白熊效應) (Hayes et al., 1996)。 與經驗性迴避相反的概念是接納 (Acceptance)。舉例來說，在面對負向情緒時，接納是去不帶評價地接受自己正處在情緒下，並仔細感受情緒，擁抱當下的感覺，而不去嘗試擺脫它。如果感到悲傷，就好好感受悲傷；如果感到憤怒，就好好感受憤怒 1。能做到接納，才有辦法平等地面對與擁抱生命裡的各種經驗。而要做到接納，前提是要能盡量不帶評價地面對眼前的事實。所以，在接納的概念裡，本就沒有好與壞，接納就是用人事物原本的樣貌去理解人事物。也因此，接納本身就蘊含著平等的概念，我們無法只接納「好」的而不去接納「不好」的，因為在分辨好壞的同時，我們就已經下了評價，我們就在透過濾鏡看著扭曲的世界，我們就無法如實地體驗活著的感覺。 參考資料 Hayes, S. C., Wilson, K. G., Gifford, E. V., Follette, V. M., \u0026 Strosahl, K. (1996). Experiential avoidance and behavioral disorders: A functional dimensional approach to diagnosis and treatment. Journal of Consulting and Clinical Psychology, 64(6), 1152–1168. 重要的是去「感受」而不是被「自動化的思考」拉著走，因為這些自動化的思考時常帶著強烈的評價，例如，「你不應該感到生氣」、「哭出來好羞恥」、「悲傷不能解決問題」。 ↩︎ ","subtitle":"從電影《異星入境》看接納與經驗性迴避","title":"擁抱生命賜予的每一刻","uri":"/2022/01/15/arrival/"},{"content":"最近愛上了重看舊片。或許是因為經驗的增長，現在比較能用「感受」而非「思考」的方式去看電影。發現這麼看電影比較能與自身經驗產生連結，也比較容易在看完電影之後帶走一些東西。\n今天來分享一部電影《命運規劃局》。以前看的時候，說不上為什麼但就覺得很好看 (或許是因為 Elise 的性感英國腔？)，但實在也找不出什麼深刻的東西。直到前陣子重看，才發覺「哇！用這種方式去解讀電影，電影變得比太魯閣峽谷還深刻」。(大雷警告，下段劇情簡述)\nDavid 十歲時就失去了母親與哥哥，不久之後也失去了父親。受到父親的影響 (偶像是甘迺迪) ，David 從小便有了從政的夢想。從政之後，他也發現自己熱愛這個行業，他享受站在人群面前，這似乎可以讓他忘卻十歲以來就伴隨著自己的空虛感與寂寞。David 透過群眾對他的喜愛來填補自己所失去的家人的愛。直到他遇到了 Elise，他發現只要跟她在一起，他就滿足了，不再需要透過眾人的喜愛來填補內心的空虛。命運規劃局 (其實就是上帝 + 天使組成的集團) 認為如果 David 與 Elise 在一起，就會改變規劃局要 David 未來成為總統的計畫，因而百般阻撓兩人交往。\n電影 這部電影用了普通人最容易理解的方式來呈現 David 心中的孤獨，就是透過愛情初期的那種非常美好的感受來進行對比。不過我覺得比較有趣的是以更廣博的「愛」去理解電影。如此一來，這裡指涉的愛，便包含了 David 成長過程中未能得到的家人的愛。而 David 與命運規劃局的拉扯，其實就呈現了 (當代) 人們認為最重要的兩項東西「愛」與「成就」之間的拉扯——今天你有了足夠的「愛」，你就不需要「成就」來填補心中的空虛；反過來說，今天你想要有「成就」，你就應該對「愛」有所節制，因為它會阻撓你專注在事業上。這某種程度上可以說是規劃局理解世界的邏輯，然後很驚人地，主流社會似乎也以同樣的邏輯在理解這件事。譬如說，父母可能會不敢表露出對小孩的愛 (還是只有亞洲父母？？？)，因為擔心這樣會「寵壞」小孩，會讓小孩變得無法獨立自主1。有些人則擔心表露出太多愛，會讓對方耽溺在愛的美好之中，而不願脫離舒適圈去面對外部世界的挑戰。簡言之，這個邏輯認為愛使人軟弱。這種感覺也是我在第一次聽到無條件正向關懷 (unconditional positive regard)2 的概念時有過的疑惑。不過直到終於體會到健康的愛所能帶來的不可思議的安定感，以及社會上的許多人未曾經驗這種感受之後，我才終於能跳出這邏輯，重新看待這件事。\n一個我覺得比較好的方法，是以馬斯洛的需求金字塔去理解這件事。如果我們有足夠的愛 (這邊的愛是指無論我做錯了什麼，或是我被老師上司同事同學甚至路人甲討厭，我都知道有那麼幾個對我真正重要的人，他們不會因此就不愛我了)，我們就能有非常穩健的根基去面對世界帶來的艱困挑戰，因為我們內心深處知道，我可以放膽去做而不用害怕失敗。因為即使失敗了，我背後還是有愛我的人能夠接住我，願意陪伴我一同面對這艱困的世界。因為我已經擁有世界上最珍貴的東西了，失敗所帶來的打擊，相比之下顯得微不足道。在這個概念之下的「愛」，是一股使人茁壯的安穩力量，是一個人在面對任何艱困挑戰的先決條件。有了這強固的根基，才能夠在需要時，心無旁鶩地面對挑戰。沒有了愛，我們無法知道在失敗之後，有無能力繼續面對這個世界。\n愛使人軟弱的概念，想起來真的蠻可怕的。有時候，我們並不是沒有人愛，而是因為我們對於愛 (使人軟弱) 的認知，導致沒有人願意大方承認，進而造成我們無法感受到自己被愛著。即使客觀上真的沒有人愛我了 (例如，我成為了世界上最老的老頭，但因太過孤僻沒有持續交朋友，導致現在沒有任何朋友)，我同樣能以愛別人的方式來愛自己，愛自己的所見所聞、愛自己的感受、愛自己即使經歷那麼多狗屁倒灶的事卻依然撐了下去、愛自己生而為人，生而為一個生命的本來面目。「愛使人軟弱」的概念，則可能會摧毀我們自愛的能力，因為我們相信自愛只會讓自己耽溺在舒適圈中，讓自己越陷越深。一旦失去自愛的能力，就無法對自己寬容，無法對自己寬容，任何的挫折與失敗，即使再小，都能將自己擊潰。\n我覺得這對也不對。不過造成小孩能否獨立自主關鍵應該在於「如何」愛小孩，而不是「愛」本身的結果。告訴小孩我愛你不等同於我會替你承擔所有事。你還是有你該負的責任，我愛你是我願意陪著你一起面對你該負的責任，但不是讓你可以免去這些責任。 ↩︎\nhttps://en.wikipedia.org/wiki/Unconditional_positive_regard ↩︎\n","subtitle":"電影《命運規劃局》的反思隨筆","title":"空虛、孤獨與安定的力量","uri":"/2022/01/05/adjustment-bureau/"},{"content":"我最常看的 YouTube 頻道之一是超粒方，也很喜歡裡面的影評。 不過這裡要來推薦的不是超粒方，而是一個 Podcast 頻道。這個頻道對於《遊牧人生》這部電影的一些體悟與反思，對我很有啟發而且也非常令人感動。\n當初在看《遊牧人生》時，看到快睡著，但聽完這 podcast (的 pre-release 版) 時相當感動。Podcast 接近尾聲時，觸碰到了作為一個人的意義 (或者說，一個人死掉之後，還能有意義嗎) 的問題。後來仔細一想，發現這裡的想法跟《捍衛任務》裡 John Wick 強烈的求生意志以及《可可夜總會》都能有所連結。\n這邊最令我感動的地方是1意識到一個人即便已經死亡消逝，他仍能存活於在世之人的腦袋中。這種存活，不光只是被「記著」而已。亡者在世時與人相處的所言所行、人格個性、興趣想法精神…這些都會慢慢滲入並整合成為對方的一部分。透過這種方式，即使一個重要的人已經逝去，他仍然作為我們的一部分，持續地影響我們、幫助我們、與我們一同面對生活的種種。他的身體已然消失，但他最重要的部份——心靈，依然透過我們而持續存活於這個世界。\n下文內容未收錄於 Podcast 中，但為作者想法，如果我沒理解錯的話。 ↩︎\n","subtitle":"","title":"Podcast 隨筆與《遊牧人生》","uri":"/2021/08/31/ivopsybar/"},{"content":"分享一下前陣子看過的文章：〈核心價值 (value) ：迷失方向時的指南針〉。這篇真的是一篇好文章，因為看完之後，它沒有直接給出答案，它依然讓我感到困惑。我覺得會需要讀者一直反覆思考、體驗、感受才能有所收穫的就是一篇好文章。\n內容懶人包 (原文寫得好多了) 面對這個困難的世界，我們常常用各項「目標 (goals)」引導自己前進—考進理想學校、找到好工作 (我的志願是成為OO)、於學習或工作上取得成就、交往結婚、養育子女、存錢買房、去十個國家旅遊……有目標固然很好，但如果僅依靠目標來引導生活卻未深思目標背後的意義 (為何我會想達成這個目標？為何這個目標對我來說很重要？)，那總有一天，當目標都一一完成了，或是更不幸的，當目標太難以致無法達成時，此時我們要如何維持動力？如何不迷失人生方向？\n「核心價值 (values)」是比目標更深層的東西，是指對自己 (而非他人或社會) 重要並且能賦予目標及生活意義的東西。它就像是個羅盤，能夠指出我們前進的方向；但同時，它不是目標，不會有被完成被劃掉的一天，也不會有達不達得到的問題，因而能一直為我們指出前進的方向。根據核心價值行動，縱使無法達成某個特定的目標，我們也不會太過氣餒，因為條條大路通羅馬，永遠有其它符合核心價值的目標可以嘗試，也因為知道自己一直在往理想的方向前進，所以不會為無法達成一個目標而覺得空忙一場。\n自我探索的困境 探索核心價值並不是件容易的事情。個人的核心價值之所以難以釐清，一方面或許可以歸因於我們太過於目標導向的思維習慣；另一方面，要能區分社會期待以及核心價值的差別，也需要足夠的經驗與時間探索。對於如何從茫茫的各種可能中釐清自己的核心價值，一個可能的切入點或許是興趣。這邊的興趣是指需要精神投注且在進行的當下很滿足愉快的事情。我個人的經驗是，去思考自己各種興趣發展出來的原因，可以幫助自己看到它們形成的共同因素。這感覺有點像是測驗學者去對一份測驗 (e.g., 語言能力測驗) 做因素分析，嘗試從這些實際可觀查到的測驗題目中，去歸納出在這些題目之下是在衡量哪些能力 (e.g., 詞彙理解、發音、文法)。我們的各種興趣就如同這些考題，它們之所以形成是因為背後的一些因素 (興趣：核心價值/考題：語言能力的各面向)，核心價值不容易直接被觀察到，但透過可見的興趣，我們可以去推敲、整理、歸納，嘗試找出背後孕育出這些興趣的核心價值。\n不過興趣這個切入點對某些人來說可能幫助不大，因為要知道自己的興趣為何其實並不容易。我想，興趣就像是任何有重量、有價值的事物一樣，並不是與生俱有的，而是需要投注心力與時間培養而來的。而恰巧在我們所生活的世界中，我們的心力與時間都相當有限，因為我們多數的時間精力都用在完成他人要求或是期待我們完成的事情上，卻沒有留下足夠的心力與時間投注在自己身上。我們連放鬆的時間都不夠，更別說要將額外的力氣用在自我探索上。然後這就開啟了惡性循環：越是不經思索地將精力用於滿足他人的需求與期待上，就越不會知道什麼對自己是真正重要的，越不知道什麼對自己是重要的，就會花越多精力去滿足他人的需求與期待。\n當然並不是說不可能從滿足他人需求的過程中發掘興趣，但一廂情願地期待能從被指派的工作中發現自己所好其實相當被動，因為這無異於在碰運氣 (這麼好康？別人要我做的事碰巧跟我尚未發現的興趣有關)。除此之外，我們所隸屬的團體都有自己的目的以及脈絡，團體要成員做的事情因而常是出於自己的目的而非為了成員的個人成長。公司要我們做事是為了賺錢，社團年年籌辦活動是為了維持運作與維繫傳統，甚至就連我們為家庭的付出很常都是在滿足家庭價值與傳統觀念。如果我們將所有的心力都花費在這上面，那就等同於將探索自我的範圍壓縮在這些非常狹隘的領域之中。相較之下，給自己更多的時間與空間，讓自己能跟著思考或感覺自主決定探索的方向，似乎比較有可能發掘珍貴的東西。\n或許學習同等地尊重他人與自己會是很重要的一步。我們尊重他人，因此在自己的能力及責任範圍內，我們盡力去完成該做的以及被交辦的事；但與此同時，我們對自己也應有同等的尊重，尊重自己也應得同等品質的時間與精力，讓我們有辦法投注足夠的心力在想做的事上。\n","subtitle":"〈核心價值 (value) ：迷失方向時的指南針〉","title":"文章分享","uri":"/2021/07/18/act-values/"},{"content":"Last year I have tried home workouts since the gym was shutdown for about four months. During that time, I realized that it is harder to maintain the challenge and effectiveness of the workout since few training equipments are available at home. Later I found out that time is a crucial factor to increase the challenge and effectiveness of the training. By incorporating interval training to my workout schedule, I started to see the gains! During this period, I used intervaltimer.com to create my own interval training timer. This website (mobile app also available) allows users to build and save their own training timer. It is great, but I’m a bit upset with its user interface since it imposes many limitations on how a user can customize the intervals of the timer. In addition, some of the features on the website is locked in the associated mobile app. This has led me to the idea of creating an open source alternative to intervaltimer.com.\nRecently, the outbreak of COVID-19 in Taiwan spare me quite a lot of time, and I have spent some of the time picking up JavaScript. Last Friday (Jun 4), I was randomly browsing the HTML Drag and Drop API. This reminded me of the interval training timer. After watching a video about building sortable drag and drop elements with vanilla JavaScript, I decided that it is time to carry out the project of building an interval timer.\nIt has been almost two years since I wrote an app in vanilla JavaScript1. These days, I have been creating apps with JavaScript frameworks like Vue. It is great, but working with vanilla JavaScript is also pleasuring. Switching back and forth between two different ways of creating web apps is joyful, as it gives you the opportunity to think about programming in two quite distinct ways!\nDraggable Interval Timer To keep the timer minimalist2, I wrote it in vanilla JavaScript, HTML \u0026 CSS (no server required of course). The most important feature of this timer is its dragging functionality, which gives users the flexibility of sorting the exercise blocks at will. Another useful feature is that custom timers created by the user are sharable through URLs. This makes up for the unavailability of the drag and drop API on touch screen devices (e.g. mobile phones) since a user can just create a timer on the PC/laptop and access the timer with a link on her mobile phone. Here is an example!\nDraggable Interval Timer The timer is available at timer.yongfu.name and the source code is on GitHub. I’m looking forward to seeing your custom interval timers!\nThis is when I started to learn JavaScript seriously, thanks to CS50’s Web Programming with Python and JavaScript. ↩︎\nWith more functionalities comes more limitations, this is what has happened with intervaltimer.com, I think. ↩︎\n","subtitle":"","title":"A Timer for Interval Training","uri":"/2021/06/06/interval-timer/"},{"content":"這學期 (109-2) 第二次擔任課程助教，給大學部的同學們上 R 語言。第二次教學，在熱情上減了一半，在教材難度上增加了一半。大概是因為不喜歡重複做一樣的事，這次課程刻意補了上一次 (108-1) 懶得教1、沒時間教2、沒自信可以教3以及沒有能力教4的內容。有了這些新的內容，課程準備起來就比較提得起勁，畢竟知道在去年已經教過的內容上，現在的我實在很難超越當時的自己5。不過，在準備教材的同時，也可以看到自己這一陣子的轉變：對某些概念的認識更加地完整、思緒變得更複雜迅速有條理。能發現這些真的蠻開心的，至少腦袋還是有長一些。擴增教材的另一個目的，是為了補齊上一次未能 (學會然後) 教的缺憾，大概是一種想把圓畫完整的感覺，當然圓不可能畫得完美6，不過整體來說，完整的感覺還是大過缺憾許多，也蠻值得開心的。\n這次的課程全部採取事前錄製影片的方式授課，表面上的目的是擔心遠距教學的情形再度出現 (結果真的出現了…)，實際上的目的則是跟剛剛一樣：我不想要做同樣的事情 (實體授課)，而且這次實體授課大概也不會講得比上次好，所以不如換一種方式授課7。結果無心插柳，最後幾堂課因為突來的疫情改成遠距教學，對我反倒沒造成什麼影響，又是件值得開心的事情。\n在經歷 12 個頗為漫長的週末，終於完成了這個算是完整的 R 語言課程。在這 12 堂課中，或許值得慶幸的一件事情是課程內容主題都不是 state-of-the-art。在能夠選擇時，選擇了去講比較穩定不變的東西、比較不會經過兩三年後就變成歷史名詞的技術或概念。這麼做或許可以讓課程的保鮮期變得比較長，也或許可以讓更多人從這個課程中受惠。這 12 堂課程的完整內容 (影片/簡報/講義/作業/程式碼) 可以在這個頁面取得，歡迎讀者自行使用、分享或是修改並應用於教學之中。\n2021 課程影片 整個學期下來，雖然過得平淡、沒有第一次授課時那種短時間內大量成長的感覺，不過反倒是有種比較完整地完成了一件事情的感覺。這種感覺不會讓人大喜大悲、異常緊張或是過度興奮，但會讓心裡變得厚實舒坦，讓內心安穩一些，然後微微的喜悅就會從心底緩慢而持續地湧出。\n最後，還要特別感謝 Andrea、Yulin、Mao-Chang 以及 Amber 在各個方面的協助，這門課的學生能有你們的照顧真的很幸運。\nLab01: 路徑、終端機、R101 (絕對與相對路徑 \u0026 Terminal) ↩︎\nLab12: 專案成果展示 (Shiny) ↩︎\nLab09: 文本與詞彙的向量表徵 (document-term matrix \u0026 latent semantic analysis) ↩︎\nLab06: Simulating Data with R (Causal inference 101) ↩︎\n那時多有熱情和自信啊！反觀現在真的很難被激勵，不過往好處想或許這代表掌握情緒的能力有所提昇？ ↩︎\n像是這次在教 Web API 時拿來示範用的 Public API 在我上傳教學影片之後就關閉服務了… ↩︎\n這樣就不用和過去的自己硬碰硬比較，擔心自己退步了 (反正我有錄影片，觸及的人就是比較廣比較潮啦！108-1 的你輸了啦哈哈哈哈哈)。比較不見得會進步，但一定會帶來傷害，所以換個方式讓自己沒辦法去比較或許也不錯。 ↩︎\n","subtitle":"","title":"我的 R 開放課程","uri":"/2021/06/03/my-r-course/"},{"content":"Recently, I have learned more about JavaScript and created a few JS web apps. This gave me the idea that we can separate the content and the data in an HTML document to make it more dynamic—the content stays static while the data could be updated independently without rewriting or recompiling the HTML document. This could be done by utilizing JavaScript’s ability to asynchronously fetch data from the web and generate DOM elements based on these data. I implemented this idea in my new R package getable. Basically, getable lets the user insert dynamic HTML tables in R Markdown (HTML output only) by providing the URLs to the tables’ data. Every time when the compiled HTML document is opened, the data are fetched from the web and used to generate the HTML tables. This means that the user can update the data (e.g., hosted in a public GitHub repo) without recompiling the HTML from R Markdown. In addition to hosting data in GitHub repos or on static sites, the user could use Google Spreadsheets as the data store, as shown in the GIF below. Installation getable is now on CRAN, which can be installed with: 1install.packages(\"getable\") or, install the latest version from GitHub: 1remotes::install_github(\"liao961120/getable\") Usage getable comes with a template that you can import in RStudio by selecting: File \u003e New File \u003e R Markdown \u003e From Template \u003e HTML Tables with Dynamic Data {GETable}. Or, you can simply run the command below in the R console: 1rmarkdown::draft(\"name_your_file.Rmd\", template = \"tablefromweb\", package = \"getable\") The template contains several files, of which dfFromWeb.html, dfFromWeb.js, and dfFromWeb.css are required for the compiled HTML to work properly (DO NOT change the RELATIVE PATHs between these files and the source Rmd). Note that you can style the appearance of the HTML tables with CSS in dfFromWeb.css, and if you know a lot about JS, you can even modify the code in dfFromWeb.js to use other JS libraries to generate the HTML tables. You can see a working example here. Inserting Tables Simply use the function renderTable(\"\") in a code chunk to insert a dynamic HTML table. Remember to set the chunk option results='asis': ```{r results='asis'} getable::renderTable(\"https://yongfu.name/getable/demo/data/df.csv\") ``` ","subtitle":"","title":"Getting Tabular Data Through JavaScript in Compiled R Markdown Documents","uri":"/2020/09/09/getable/"},{"content":"這是一篇關於心流 (Flow: The Psychology of Optimal Experience) 的讀書心得。寫這篇文章的目的有兩個，一是記錄與分享書中自己特別有感觸的地方；另一個則是希望能將此書推薦給讀者。當然，跟書的作者比起來，我的文筆遠遠不如，所以如果你已決定要看這本書，就趕快開始看吧，不要浪費時間看這篇推薦文。但若你正猶豫要不要看這本書，或許可以利用這篇文章增強你的閱讀動機。我在看這本書的時候一直有「為什麼我不早點看這本書」的感受。身為一位主修心理學的學生，我一直知道心流理論的存在，也知道這本書，但就是一直沒有翻開這本書來看（這世界有太多書可以選擇了）。在看完這本書後，我只感到在心理系受的心理學訓練所帶給我人生上的啟發與幫助，遠遠不及這本書。\n「心流」是什麼？ 這應該是書中最理論的地方，但要理解「心流」這個概念，並不需要什麼心理學的專業訓練。正如作者所說，這本書是寫給大眾而非文謅謅的學術作品。只要細心體驗過生活，應該都能了解自己曾有過的那種「心流」經驗。\n我們體驗過的正向經驗大致可分成兩種：享樂與樂趣。享樂是指生理需求或社會制約的期待獲得滿足的狀態，例如睡覺、放鬆、進食、性愛、飲酒、吸毒、娛樂等。透過享樂，我們可以暫時讓心神恢復平靜——經由進食，我們可以平復飢餓的不適；工作完回家後，我們常常追劇、看 YouTube 放鬆心情；極端一點的，則是使用酒精或藥物讓自己短暫脫離這紛擾的世界。另一種正向經驗，與享樂接近但本質不同的是樂趣。樂趣會帶給人意想不到的正向感受，並時常伴隨著成就感。與實力相當的人打球、下棋；讀一本書而獲得啟發、與作者產生共鳴；在書桌前努力解出一道困難的題目，或是思索難解的問題；甚至是烹飪、烤餅乾、種菜、插花、寫字 (書法)、作畫。這些都是能帶來樂趣的活動。而享樂與樂趣最大的不同是，前者是在被動滿足需求或欲望，後者則需要具備足夠技能，並經由主動、全心的投入才能獲得，因為只有全神貫注地投入，一個人才具備能面對當前挑戰的能力。這種透過努力而克服挑戰的過程，會讓人經驗到不可思議的正向感受，個體也會在這個過程中成長。\n這種全心全意地投注注意力在一件事情上的狀態即是心流。在這個過程中，因為注意力全部被用在面對當前的挑戰，當事人無暇去注意其他紛雜的事物、甚至會忘卻自我。此時的內心狀態是非常平靜的，任何焦慮、無聊的感受都會被排除在意識之外，腦中的事物只剩當下所面臨的挑戰。心流狀態會在當事者所具備之技能與所面臨之挑戰難度吻合時出現。如果不吻合，則會出現其他狀態——當挑戰難度大於所具備之技能，會令人感到焦慮 (anxiety)；反之，若挑戰太簡單，則會感到無聊 (bordom)。\n以上的內容大致就是心流的理論內容。但心流並不僅是一種心理狀態，它也是一種生活的態度，更是幸福感的泉源。下方是幾個我在讀完書後，特別令我感觸深刻的主題。\n活在當下 成長過程中，我們一直有一個信念——未來是人生最重要的一部份。父母教導孩子，如果現在就養成良好的習慣，對他們的將來會有幫助。老師們也跟孩子保證，現在上的這些課程雖然無趣，但是對將來是有益的，可以幫助他們找到好工作。公司老闆也告訴資淺的員工要有耐心、要努力工作，將來就能晉升，在辛苦漫長的努力後，緊接而來的就是黃金燦爛的退休歲月了。就如美國哲學家愛默生說的，「我們一直想著未來要過什麼樣的生活，但到頭來卻像沒有真正活過一樣」。\n(p. 39)\n容易引發心流體驗的活動都是自發性參與的，不是為了外來的目標與動機。換言之，這些活動的目的即是活動本身 (autotelic)，而從事這些活動所帶來的籌賞，則來自參與者自身努力後所產生的心流體驗。我們不需要仰賴社會上的價值來說服自己和別人，這些能引發樂趣的活動是有意義的。然而事實是，我們時時刻刻都在尋找社會所認同的價值來「證實」自己的行為是有意義的。因此，我們做的每一件事情都必須「合理」、「有目的」，必須讓自己的「未來」更好。我們不斷追尋著未來，卻忽略了人生最重要的，不是客觀的外在環境、不是我們達成了多少目標，而是我們主觀的經驗。我們主觀所經驗到的事物才是真的，它才是我們人生的一切，畢竟我們所意識到的一切皆是我們主觀詮釋的結果。\n活到現在這個歲數，我們耗費了多少精力在對未來感到焦慮？因為這種焦慮，我們又耗掉了多少娛樂時間在撫平這種焦慮？然後又因為千篇一律的娛樂感受到多少次的無聊？但除了未來的焦慮以及生活的無聊之外，我們卻鮮少靜下心來專注在一件事上，隔絕外來的所有刺激，進入那個只屬於自己的世界——沒有未來沒有過去，沒有無聊沒有焦慮，只有眼前那個需要全神貫注面對的挑戰。\n當現有經驗就可以帶來無窮的回報，我們也就不需要把希望都放在不可預知的未來了。\n(p. 112)\n原來我一直很功利 徹底社會化的人只懂得追求周遭的人也認同的獎賞，而這些獎賞往往與天性的欲望不謀而合。他可能會遇見許多可以為人生帶來真正滿足的經歷，但都忽略了，因為這些與他渴望的東西不一樣。他在意的不是現在擁有的東西，而是照著別人的期待走可能得到的收穫。受制於社會制約下的他，只能不斷追求一到手就化為烏有的獎賞1。\n(p. 42)\n我們生活在一個講求功利 (學經歷、權力、地位、財富、名聲) 的社會，任何事都需要符合功利的思維邏輯才顯得合理。即使在以高等教育與研究為目標的大學亦是如此。曾經我自以為是個不受功利思想拘束的人，所以大膽地依照自己的興趣選了個就業光景不如何好的心理學系。然而實際上的情況是，我不停地依循著內化的社會價值，以功利的邏輯說服自己這個選擇以及未來前進之方向的合理性。一開始，我說服自己說，「心理學可以幫助人」，所以或許未來可以讀臨床心理學，成為臨床心理師 (這邊的功利是「名聲」，社會對於懸壺濟世有著非常高的評價，我也覺得這很好，但這並非「由內而外」的動機，是無法長久支持一個人的)。等到我發現自己對於臨床這個領域實在不怎麼感興趣，我開始有意無意地尋找從事研究工作的邏輯。我想，我可以加入一個實驗室，慢慢磨慢慢磨，最後成為一位獨立的研究者，從事自己「喜歡」的研究 (又是一個「未來」會更好的概念)。然而當時我有個深沉的疑惑——如果我真的選擇了一個實驗室熬下去，那我到底是真的對「研究」有興趣還是對研究人員所擁有的「名聲」與「地位」有所憧憬。這個疑惑在我大學快畢業時解開了第一層，在我讀完心流後解開了第二層——求學與研究的每個當下本身就該是籌賞與動力來源，如果無法在這個過程中時時體驗到「解開謎團的樂趣」，那研究之路就不比其它職業來得不「功利」：依然是為了更好的「未來」、為了做出更「重大」的發現、為了得到更高的學術「聲望」。\n長期生長在這個講求功利的社會，讓我們將這些社會價值內化，以致於功利的邏輯滲入我們一切的思考。我們近乎迷信地相信追求這些高社會評價的事物會為自己帶來幸福，卻忽略了真正的幸福不假外求。真正的幸福源自於全神貫注在自己深感興趣的挑戰上。幸福感是由內而外產生的，是來自於努力過後的心靈，而非外來的獎賞 (這充其量只能滿足「享樂」的需求而已)。幸福感是來自於努力的過程，而不是長久努力後所帶來的美好結果與未來 (這是欺騙勞工好用的標語)。如果我們只為了追求「目標」的達成，卻從未仔細體驗「過程」本身，那我們注定要錯過幸福。\n一個人如果過於自滿，就會覺得把精神能量花在新事物是浪費，除非這麼做可以得到外來的獎賞。就這樣，生活不再有樂趣，享樂成為正向經驗的唯一來源。\n(p. 82)\n小結：拿回主導權 現代人愈來愈容易將自己內心的感受解讀成自然的呼喚。直覺成了很多人唯一相信的權威。如果某見識給你的感覺是好的，那就一定是對的。當我們不假思索的順從基因與社會指示時，就等同放棄了對意識的控制，成了無可救藥、缺乏人性的玩物。\n(p. 41)\n回顧截至目前的人生，有太多時候我都在用膚淺的外在價值傷害真實的內在動機，這幾乎已經變成一種反射動作。最常見的景象就是「為什麼要學英文/拉丁文/數學/程式…」。大概除了英文2之外，每一個我都有個符合功利邏輯的理由——拉丁文是因為有些英文詞彙源於拉丁文，所以學習拉丁文有助於英文學習 (高中)；學習數學是因為統計學、生態演化模型都會用到數學，所以要打好數學基礎 (大一至大三)；學習寫程式 (R 語言) 則是因為曾經在處理統計資料時吃過苦頭。但是到了現在，我保有並持續學習的技能卻只剩寫程式而已。原因並不是因為我常常要跑統計或是程式能力對就業很重要3，而是因為在學習寫程式的過程中，我發現這件事本身就能帶給我無止盡的樂趣。其它為了追求社會制約的獎賞而學習的技能，多半淪為三分鐘熱度，沒有在我身上留下任何痕跡，卻佔據了我大半的青春。\n我想，或許就是這種為了服膺於社會功利邏輯的壓力，讓許多人無暇習得足以自娛的技能，因為所有的精力都耗在將自己塑造成符合社會期待的樣子，深信著這是對自己未來最好的道路。但我們鮮少在真正體驗到平靜、幸福的狀態後，仔細感受它的真實性以及之於自己的意義。\n這篇文章並非在責怪我們社會的功利，這種功利是當前社會文化發展下的必然結果，厭惡式的批評不會造成任何改變。但可以改變的是我們對自己以及對世界的認識。我們可以選擇不要以社會制約的價值來看待一切事物。我們也可以選擇不要任由容易被利用4的生理需求擺佈。然這一切需始於對自我意識的掌握，而掌握自我意識，則需始於心流的體驗。\n因為那是社會制約的獎賞，是社會讓你「以為」是獎賞的獎賞，但卻不是真正幸福的來源。人們往往只有在得到這些社會制約的獎賞後，才發現它不如原本所想像的美好。 ↩︎\n因為我在懂事之前 (意即在將社會價值完全內化之前) 就學英文了，所以不曾找過理由。魯凱語則是另一個不為任何功利目的而學習的語言。這學期在語言田野調查的課程中，一邊轉寫魯凱語一邊學習。轉寫族語老師說出來的語言真的是一個很棒的心流體驗。在面對一個完全陌生的語言時，需全神貫注才能抓到老師吐出的每個音，還必須時時確認老師講得族語是否真的是我們所要的內容。過程中，更常常因為發現魯凱語表達某些概念的特殊方式而感到驚喜不已。 ↩︎\n我的程式能力相當雜亂，十分不符就業市場。一開始學 R，接著接觸 Python；學 Python 時又變心開始碰 JavaScript；現在則寫了一些 Vue 的 App。引導我依這個方向前進的，不是潮流或就業能力需求，而是我當下的能力足夠學會什麼，以及這些程式語言足以做出什麼有趣的東西。 ↩︎\n例如，利用大眾對於人際關係、娛樂、美食與性的欲望而賺大錢的公司。 ↩︎\n","subtitle":"心理學經典《心流》閱讀心得","title":"功利社會下的自我反思","uri":"/2020/06/27/flow/"},{"content":"In my previous post, I describe how I created a web app that can search and locate patterns in interlinear glosses written in Word documents (docx) to facilitate the workflow of documenting language. I tried to make the (backend) app extremely easy to install, but still, most of my classmates didn’t even try to install it. It is just to frightening for people without any programming experience to install python on their own. Hence, I decided to make the app even more user-friendly. Now, ANYBODY can use the app if she has access to the Internet and a web browser. Just visit https://glosss.yongfu.name and play with the app to see what happens. In this post, I describe how I made the app completely web-based (without setting up a server). The App’s structure In relation to the title of this post, the app is not just web-based, it is also SERVERLESS—in the sense that there is no backend server listening to the queries entered by the user1. All searching is done locally in the browser. To setup the app, I only need a simple server that hosts static files (for the app written in HTML/CSS/JS and the data in JSON format), and GitHub Pages is all that is needed. Specifically, I utilized three (free) services in order to allow the users to upload and search the glosses contained in their Word documents: GitHub Pages GitHub Pages is used to host my app (liao961120/gloss-search-frontend) and the processed JSON data that contains interlinear glosses (originally written in Word documents) Google Drive Users are provided a GD folder to upload their Word documents Travis CI After uploading their Word documents to GD, users can trigger a Travis CI build by clicking on the bottom-left corner of the app (password needed). Under the hood, I encrypted my Travis CI’s API token so that when the user enters the correct password, a POST request (containing the decrypted token) is sent to Travis CI to trigger a build. During the build, the Word documents saved in Google Drive are download and processed into JSON format and then pushed back to a GitHub repo that hosts the data. The figure below summarizes the flow of the app’s data: Figure 1: The numbers annotated to the arrows indicate the order of the execution. Processes with annotated text appended with indicate that they are triggered by the user. Other processes proceed automatically. Configuration of the Travis build Below is the partial configuration2 of the Travis build that: download the Word documents from Google Drive process the Word documents into a JSON file (and a log file) push the resulting files to gloss-search:gh-pages (set up in the deploy: section) (1) and (2) is done in the script GlossProcessor.py. In the script, I call GitHub30/gdrive.sh to download files from Google Drive (this really saved me a large amount of time). Then I use the module python-docx (described in my previous post) to process the Word documents into a structured JSON file. 1language: python 2python: 3 - \"3.7\" 4 5install: 6 - pip install python-docx 7 8# command to run tests 9script: 10 - python3 GlossProcessor.py https://drive.google.com/drive/folders/${BUDAI_RUKAI} 11 12deploy: 13 provider: pages 14 skip-cleanup: true 15 github-token: $GH_TOKEN # Set in travis-ci.org dashboard, marked secure 16 keep-history: true 17 on: 18 branch: master Encrypt/Decrypt API token in JavaScript Travis CI is normally used as a testing service, but since code is tested by scripts, one can actually write scripts that do things other than testing. Since my discovery of using Travis CI to render R Markdown in the R community, a lot of creative ideas came to me (e.g., using Travis Cron Jobs to regularly update plots depending on the data from a survey). Usually, a Travis build is triggered by a push to the GiHub repo, a pull request, or a preset cron job. A more advanced way to trigger builds is to utilize Travis CI’s API. As described in the API document, to trigger a build on a repo (e.g., liao961120/gloss-search), one need to send a POST request to the endpoint https://api.travis-ci.org/repo/liao961120%2Fgloss-search/requests3 along with the API token provided by Travis CI. The JS (Vue) code below is used for sending the POST request to Travis CI: 1triggerBuild: function() { 2 const url = 3 \"https://api.travis-ci.org/repo/liao961120%2Fgloss-search/requests\"; 4 const body = { 5 request: { 6 branch: \"master\", 7 message: \"Trigger build from glosss.yongfu.name\" 8 } 9 }; 10 const decryptedText = this.CryptoJS.AES.decrypt( 11 this.build_token, 12 this.build_psswd 13 ).toString(this.CryptoJS.enc.Utf8); 14 const header = { 15 \"Content-Type\": \"application/json\", 16 Accept: \"application/json\", 17 \"Travis-API-Version\": \"3\", 18 Authorization: `token ${decryptedText}` 19 }; 20 this.$http.post(`${url}`, body, { headers: header }).then( 21 response =\u003e { 22 this.response = response; 23 }, 24 response =\u003e { 25 this.response = response; 26 } 27 ); 28 } Notice the variable decryptedText in the code above. To prevent my API token from being exposed to the public, I have to encrypt my API token. The encrypted token is saved in the variable build_token, and the password to decrypt the encrypted token is given to the users privately. To trigger a build with the app, the user needs to enter the correct password (saved to the variable build_psswd). After the user enters the password, the app can then decrypt the encrypted token to the original API token. The API token is then placed in the POST request header (Authorization: `token ${decryptedText}`) and send to Travis CI. The encryption and decryption of the API token is done with a Vue wrapper of the JS library crypto-js. A rough way to think of this new app (as compared to the previous one) is that the functionality of the backend part of the app (i.e., data preprocessing in Python) is replaceced by Travis CI. The old app listens on local file changes in the Word docuemnts, but Travis CI can never provide this functionality. However, the benefit of ease of use (no need to startup a server) is huge as compared to real-time data update provided by a backend server. ↩︎ For the full configuration, see https://github.com/liao961120/gloss-search/blob/master/.travis.yml. ↩︎ Note that the slash separating liao961120 and gloss-search needs to be converted to %2F to prevent interpreting liao961120/gloss-search as different URL segments. ↩︎ ","subtitle":"Utilizing Travis-CI for Data Update in a Web App","title":"What if I Have No Server?","uri":"/2020/05/26/what-if-i-have-no-server/"},{"content":"I am taking the course Linguistic Fieldwork this semester. Each week, we record and transcribe Budai Rukai, an Austronesian language spoken by Rukai people (魯凱族). The resulting data (interlinear glosses) are written in a Word document (.docx) as required by the course instructor. Things get worse as the number of documents accumulates each week, since it becomes harder to locate specific linguistic patterns in the corpus of texts, as they are spread across multiple documents. Inspired by the work of my labmate, Don, I created a web app for searching interlinear glosses, which can search for and locate specific patterns in a collection of Word documents. This post describe the web app’s basic design and improvements to puerdon/corpus_processor, the project this web app got inspiration and succeeded from. For instructions about using the app, visit liao961120/gloss-search (English) for more details. Basic Design The app’s frontend was built with Vue.js, and the backend was written in Python 3. The frontend was designed to ease the search and locating of certain patterns in the Word documents and is the only interface the users need to interact with. The backend is used to (1) parse the glosses written in Word documents into Python objects (dictionaries) and (2) perform the search on these Python objects based on the requests sent from the frontend. Backend The most challenging part of the backend program is that of (1) since Word is a WYSIWYG editing software, which means that two Word documents could have the exact same appearance while differ in their underlying structures. In other words, without considering the fact that different users differ in the way they use Word, the naive code used to parse Word documents are doomed to fail, even though the users SEEM to stick to a particular format. In order to deal with this problem, some kind of normalization needs to be done to all the Word documents such that documents that look the same are INDEED the same. The format of the Word document provided by our course instructor is as below: [Number]. [Original language (optional)] [EMPTY LINE (optional)] [Gloss line 1 (original language)] [Gloss line 2 (English)] [Gloss line 3 (Mandarin)] [EMPTY LINE] #e [English translation] #c [Chinese translation] #n [Notes] [EMPTY LINE] Below is an illustration of what it ’looks’ like in a Word document of interlinear glosses: When you press Enter on the keyboard while editing a Word document, you SEEM to be creating a line break, but, in fact, you are creating a new paragraph. To create a line break without breaking the current paragraph, Shift + Enter instead of Enter should be pressed. This is an example of the user behaviors I needed to normalize before I can sucessfully parse the Word documents. To do this in Python, I extract all the paragraphs in a Word document using the module python-docx1 and concatenate them with newline characters (\\n) into a large string. Then, I split the string by \\n into a list of lines, from which the starting and ending positions (the positions where [gloss num]. is found in the list) of each elicitation could be located by simple pattern matching: 1from docx import Document 2 3 4def process_doc(fp=\"corp/20200325.docx\"): 5 6 # Normalize document into a list of lines 7 d = Document(fp) 8 a_doc = '\\n'.join(p.text for p in d.paragraphs) 9 a_doc = a_doc.split('\\n') 10 11 # Find the positions of each elicitation 12 pat_start = re.compile(\"^(\\d{1,2})\\.\\s*$\") 13 glosses_on = [] 14 gloss_num_old = None 15 for i, line in enumerate(a_doc): 16 if pat_start.match(line): 17 gloss_num_new = i 18 19 # Save each elicitation range 20 if gloss_num_old is not None: 21 glosses_on.append( (gloss_num_old, gloss_num_new - 1) ) 22 gloss_num_old = gloss_num_new 23 24 # Save last gloss 25 i = gloss_num_old 26 while True: 27 i += 1 28 if a_doc[i].strip().startswith('#'): 29 if len(a_doc) == i + 1 or (not a_doc[i + 1].strip().startswith('#')): 30 end_idx = i + 1 31 break 32 glosses_on.append( (gloss_num_old, i) ) 33 34 # Get all elicitations in the document 35 glosses = [] 36 for start, end in glosses_on: 37 gloss_num = int(re.match(\"(\\d+)\\.\", a_doc[start])[1]) 38 gloss_lines = [ l.strip() for l in a_doc[(start + 1):end] ] 39 glosses.append( (gloss_num, gloss_lines) ) 40 41 return glosses One major improvement to puerdon/corpus_processor is dealing with gloss lines that span multiple lines, as exemplified in the code chunk below. This problem can be solved by removing all empty lines between the gloss lines and free lines (those starting with #e, #c, and #n) such that, when the free lines are excluded, the number of gloss lines must be multiples of three (the example below has 6 gloss lines in total). Normalizing each elicitation to this format allows me to concatenate multiple gloss lines into three for all elicitaion examples. The code that deal with this can be found in the function assign_gloss_free_lines() in GlossProcessor.py. 14. kay Elrenge watsili kay malri ki lalake ki talialalay kay Elrenge w-a-tsili kay malri this Elrenge AF-RLS-throw this ball 這 Elrenge 主焦-實現-丟 這 球 ki lalake ki talialalay OBL kid _ noble 斜格 小孩 _ 貴族 #e Elrenge throw a ball to the noble’s kid. #c 這 Elrenge 丟一顆球給貴族的小孩 #n After normalizing the Word documents, the documents are parsed and convert into the Python dictionary as shown below: 1{ 2 '20200325.docx': [ 3 (1, { 4 'ori': ['yakay', 'ku', 'tatulru', 'ku', 'ababay', 'ku', 'agili'], 5 'gloss': [ 6 ('yakay', 'have', '有'), 7 ('ku', 'three', '3'), 8 ('tatulru', 'female/male', '女性/男性'), 9 ('(ku', 'yonger_brother/sister-1SG.POSS', '弟妹-我的.第一人稱單數.所有格'), 10 ('ababay/sauvalay)', '_', '_'), 11 ('ku', '_', '_'), 12 ('agi-li', '_', '_') 13 ], 14 'free': [ 15 '#e I have 3 younger brother/sister', 16 '#c 我有 3 個弟弟/妹妹', 17 '#n yakay ku 可省略' 18 ] 19 } 20 ), 21 (2, ...), 22 23 ... 24 '20200408.docx': [...], 25} Frontend The construction of the frontend was relatively easy, given that the most challenging part was already done in my (dead) project on building a web app for interlinear glossing. In brief, the component Leipzig.vue is used to construct the aligned glosses (see figure below) from the data sent from backend (i.e., the python dictionary in the previous section, converted to JSON format). Another relatively challenging part (to me) is implementing the highlighting function, which highlights the matching patterns in the glosses (as in the words with yellow background in the figure above). It was implemented by creating a computed property that wraps the parts of the data matching the search pattern into HTML tags. I can then use CSS to decorate these tags. The remaining parts of the fontend are about communicating with the server, which are relatively easy to set up. I actually just copy-and-pasted the code from my previous KWIC concordancer project. I didn’t spent much time exploring the API of python-docx thanks to Don’s PIONEERING (at least in our university, or even Taiwan, I believe) project puerdon/corpus_processor. ↩︎ ","subtitle":"","title":"Searching Interlinear Glosses Written in Word Documents","uri":"/2020/04/23/gloss-search/"},{"content":"每次接近學期末的時候，寫程式癮就會開始發作 (可能是不想面對無趣的期末報告)，這時候腦袋會蹦出許多很有趣的想法，然後就會迫不及待地想將這些想法實作出來。這次(2019 年末) 的程式癮刺激來源是實驗室的雲端硬碟裡的某個 (版權封閉) 中文語料庫，雖然該語料庫已有很好的搜尋界面，但我就是想 reinvent the wheel，自己手刻出一個 concordancer。不為了什麼，就只是因為這件事本身就很有樂趣。\n初步嘗試：for loop… forever 我本來並沒有太大的雄心壯志，就只想快速弄出個程式界面方便我查找 concordance，想說使用 NLTK concordance 應該很快就可以弄出我想的東西。但 NLTK concordance 只能使用 word form (或 pattern) 去搜尋 concordance，我的需求卻是要能使用 word form 或 PoS tag 搜尋語料庫 (類似 Corpus Query Langauge1，但不用這麼複雜)。但要自己用 Python 實作這個功能也頗簡單，於是我就自己手刻了這個功能。然而事實證明我太過天真了。語料庫的大小約 1000 萬個 token，而每次搜尋時，我的程式使用 for 迴圈跑過整個語料，因此要花非常非常非常久的時間才能完成搜尋。對於非資訊背景出生的我，第一次體驗 $O(n)$ 是件不可忽視的問題以及 Database 存在的必要性。\n重新規劃： Database + Python + Vue 為了解決上述問題我暫時擱置了這個專案 (寒假開始到春節期間) 去學習必備的一些知識2，最後比較有系統地重新規劃了這個 concordancer 的架構：\n這個新的架構分成前3、後端，前端不是本文的重點 (原始碼在此)，就不細談。這邊直接舉一個實例說明這個 concordancer 如何運作：\n首先，使用者在前端輸入一個搜尋的字串 (keyword)，這個字串需符特定的格式：[token 1][token 2][token 3]。每對中括號代表一個 token，中括號內則是描述此 token 的特徵，如 word form 與 PoS tag，例如 [word=\"打\" pos=\"V.*\"] 即是要搜尋 word form 為 打 且詞類為動詞4 的 token。這裡的例子使用 [word.regex=\"^[他她]$\"][word=\"打\" pos=\"V.*\"]，下方的幾個例子都是符合這個搜尋的 2-gram:\n他/Nh 打/VC 她/Nh 打/VC [word.regex=\"^[他她]$\"][word=\"打\" pos=\"V.*\"] 在傳給後端後，會先經過一個 parser 處理，讓後端可以將這個 query 轉換成 SQL 去搜尋 database。在搜尋時，這邊僅會在 DB 中以其中一個 token 的資訊進行搜尋，並回傳所有符合的 token 於語料庫中的位置 (所在文件之 id、第幾個句子、token 於句子中的次序)。這些 token 是可能符合 keyword pattern 的「候選者」 ，讓接下來的 n-gram 比對可以更快速 (search space 從整個語料庫減少到只剩這些「候選者」所組成的 n-gram)。\n透過這些 token 的位置資訊，可以找出含有該 token 的 n-gram。例如，假設這裡使用 [word=\"打\" pos=\"V.*\"] 在 DB 當中搜尋，取得結果後，可以再比對此 token 左邊的 token 是否符合 [word.regex=\"^[他她]$\"]。若符合，則保留此 2-gram，並取得該 2-gram 左右的 context，作為未來要回傳給使用者的 KWIC concordance。\n跑完所有的「候選者」token，即可取得整個語料庫內，符合 keyword pattern 的 concordance。接下來僅需將資料轉換成 JSON 格式再傳到前端即可。\nDatabase 設計 下圖是 Database5 的 table 設計，共有 3 個 table:\nToken: 將語料庫中的每種 token (即 type) 對應至 id。如此搜尋單一 token 的 word form 時，即可搜尋此較小的 table (列數等於語料庫中 type 的數量)，而不用跑過整個語料庫。 Pos: 將語料庫中的每種 PoS tag 對應至 id。同上，可以快速找出符合的 token。 Corpus: 保留語料庫 token 位置資訊的 table。搜尋完 Token 以及 Pos 兩 table 之後，即可透過 token 與 pos id 在 Corpus 裡找到符合的列 (e.g., tk_id == 3 (我) 且 pos_id == 1 (Nh)。這些列裡面含有這個 token 於語料庫中的位置 (text_id, sent_idx, tk_idx)。 原始碼 / 使用語料庫 這個專案一開始是使用版權封閉的語料庫製作，因此語料庫的資料並未放在 GitHub，但後端的原始碼仍放在 liao961120/kwic-backend。\n為了讓這個專案至少能被使用，我另外爬了 Dcard 作為語料 (500 多萬詞，大小約平衡語料庫的一半)，並包成 docker image，方便有興趣的人使用。要搜尋 Dcard 語料庫僅需依照下方的步驟：\n取得 docker image (僅第一次需執行)\n1docker pull liao961120/dcard 執行後端 (執行後，請等待 cmd 出現 Corpus Loaded 的字串)\n1docker run -it -p 127.0.0.1:1420:80 liao961120/dcard 前往 https://kwic.yongfu.name 使用前端界面\n一開始曾想過直接使用現成的 corpus framework，例如 CWB, BlackLab 等。但一方面研究這些 framework 要花許多精力，且因為研究的都是別人做好的 API，不容易學到比較低階、處理語料的問題。 ↩︎\n快速掃過 CS50 的前 5 堂課 (我還是不會 C/C++)、複習之前不怎麼認真看待的 SQL Database以及閱讀 SQLite 關於 indexing 的說明文件 (這最重要)。 ↩︎\n雖然本來不打算做前端，但由於花了大量時間學習 Database 的概念，多花個幾小時刻個前端相比之下簡單許多 (這邊前端的功能不多)。 ↩︎\n這裡的語料是經中研院 ckiptagger 斷詞，可於此檢視其詞類標記集。 ↩︎\n建立資料庫以及索引的原始碼位於 liao961120/dcard-corpus/indexCorp.py。 ↩︎\n","subtitle":"","title":"以 Python 實作 Concordancer","uri":"/2020/03/20/building-concordancer/"},{"content":"I noticed Leipzig.js from George Moroz’s GitHub activity (he starred bdchauvette/leipzig.js a few months ago). This JS library is fantastic, and at the moment I saw it, I came up with an idea of building a web app facilitating interlinear glossing. During Chinese New Year, I finally started on the project. I thought it would be easy since I had some experience with Vue.js before1, but it turned out that leipzig.js wasn’t designed to work with Vue.\nDynamic Input Interface What I had in mind was a web app that, while the user is typing, the rendered glosses get showed synchronously in another panel. Vue’s conventional way of doing this is by creating a two-way data binding (v-model) to capture the user’s input and dynamically render the HTML content based on the inputted data. This conflicts with leipzig.js since it only provides a high level function (Leipzig()) to modify existing DOM elements to construct the glosses. Calling Leipzig() multiple times (without erasing the already rendered HTML) would break the DOM elements, which makes the function hard to work together with Vue2.\nAfter multiple failures of making Leipzig() to work with Vue, I decided to abandoned leipzig.js and recreate its functionality with Vue. This wasn’t as terrifying as it may seem, since I can use leipzig.js’s CSS rules directly to help me align the rendered DOM elements by Vue. What I had to do was making sure that Vue generates the exact same HTML structure as leipzig.js’s rendered glosses. The resulting input interface is shown in the GIF below.\nLeipzig.js rebuilt with Vue The source code of this vue component can be found in Leipzig.vue.\nOther Parts of the App There are several advantages of adopting Vue instead of using traditional JS approaches to build the app. Since what I wanted to build was an app that can also store, manage, and export glosses for the user, I needed a framework to help me manage this complexity (e.g., Vuex), and Vue provides a good and manageable way to build a complex web app. After learning Vuex and experiencing some failures in my previous Vue project (due to increasing complexity as the app grows larger), I’m pretty sure that I could build a better app with less complexity this time. But just when I was moving forward to other parts of the app, I was stuck by other work to do, so I’m currently not developing this app. Currently, the only usable part of this app is its dynamic input interface for previewing glosses.\nIn 2019 summer, I learned Vue.js on The Net Ninja’s YouTube channel and built a markdown editor for fun. ↩︎\nThough it is possible to create a dynamic input experience with leipizig.js with a more traditional JS approach. You can read the source code of this Live demo of Leipzig.js to find out how it works! ↩︎\n","subtitle":"","title":"Recreating Leipizig.js with Vue for Interlinear Glossing","uri":"/2020/02/22/leipzigvue/"},{"content":"本來想在大四畢業的時候，寫個大學生涯回顧或是畢業感想之類的東西 (幫助自己回味)，但那時覺得有太多東西可以寫—換句話說，就是沒東西可以寫，因為沒有什麼特別重要、令人印象深刻的事情。接著，大學四年就這麼悄悄地過完，似乎沒留下多少痕跡，馬上就進入 (遠超意料之外) 繁忙的碩士班生活。有了碩班的對比，大學生活期間獲得的真正重要的東西才逐漸浮現出來—不是學會什麼厲害的技術而具備謀得穩定工作的能力，而是獲得了一些人格特質、信念以及興趣，讓我在面對各種挫折與挑戰時，能有一股力量支持我前進。\n回顧大學生活，會發現這真的是人生重要的轉折時期。這段時間是人生最自由 (也是第一次嚐到自由) 的時期，能像塊海綿一樣，沒有顧忌地到處吸收新知識與想法，無時無刻地成長。也因為成長過程中，各種因素都在彼此影響 (興趣影響到接觸的東西，進而影響到人格特質與信念，而這些又反過來影響興趣，循環不休)，我發現很難記錄下大學時期所獲得的這些內在的東西，所以這邊將這些本是一塊的東西硬是切分成三個概念記錄下來。\nGrowth Mindset 我爸是個很聰明、腦筋動很快的人。像許多人一樣，他覺得智力與聰明才智是天生註定無法改變的，且對一個人日後的成就有非常重要的影響。很不幸地，我並沒有遺傳到他的聰明才智，但卻潛移默化地繼承了他的這種想法。我不確定這種想法與我小時候缺乏自信有無關聯，但我知道在我大學花了三四年與這想法奮鬥並逐漸擺脫它的過程中，我看見自己釋放出前所未見的潛力，並在各個方面持續進步。這些進步全都是後天習得的，與「聰明才智」沒什麼關聯。\n雖然自己是讀心理系的，也在大二就聽過 Fixed vs. Growth Mindset 的概念，但直到自己親身經歷從「認為一個人的能力是天生固定的」到「相信後天努力可以讓一個人變聰明」的過程，才真正體會到這個信念對一個人的發展影響有多麼深遠。而自己之所以能走過這段路，我想很大部份要歸功於運氣吧。\n研究興趣 高中決定申請心理系的時候，其實並不太確定自己對「心理學」的哪個部份感到興趣，只是覺得對人類心智的某個東西有興趣，卻說不上那是什麼。我以為透過一門門系上開設的課程，終能「發現」自己喜歡的是「哪一個」領域。但每個人的成長發展過程都不一樣，怎麼可能運氣這麼好，心理學裡面剛好有這麼一個「領域」跟自己的興趣吻合 (而且還要系上有老師關心這領域)。我猜許多人在這種情況下，都嘗試過「修正」自己的興趣，讓自己的研究題目與老師的專業越接近越好1。但在還沒徹底弄清楚自己真正的興趣是什麼之前，因為各種因素 (升學、大專生計畫等) 而有意無意地說服自己這個題目就是自己「感興趣」的，很可能會讓自己在做這個題目時很痛苦。只有在認識到自己真正感興趣的主題，並且認知到這個主題與目前研究題目存在差異，對於這個主題的興趣才能成為繼續做研究的動力，因為這時才能嘗試去找出興趣與研究題目之間的關聯，好了解做當前題目的意義。\n我很慶幸在隱約感覺到自己的興趣難以被囊括在心理學之下時，嘗試去接觸其它領域 (生物學、人類學、語言學)，也很慶幸自己在其它領域 (生物學) 碰壁時，沒有嘗試欺騙自己對這領域很感興趣，而是選擇離開與接受失敗2。在語言所待了一個學期，我很確定當初的選擇是正確的。語法課所介紹的許多功能與認知語言學的想法，確實在許多地方與我最感興趣的主題有所關聯。我想這就是我讀著枯燥的語言學文章時的動力來源。\nDiversity 「多元」這個概念本身就很多元，可以指涉生活中的太多層面了。上文提到的 fixed mindset 某種程度上可算是「多元」的相反，因為 fixed mindset 相信人生只有一種可能 (至少在能力上)。相信人生不只一種可能是我很重要的動力來源。我的研究興趣非常的特殊3，而我目前讀的語言所是與這個興趣最接近的研究所 (據我所知)。如果我未來要讀博士班，一定是與這個興趣有關 (無關的話，我大概讀不下去)。換言之，我是在充分接受自己未來很可能無法繼續追求研究夢想的現實下，才心無旁鶩地完成碩士班的申請。\n因為已經做好失敗的準備，所以可以專注在知識的追求4；因為相信人生不只一種可能，所以可以接受失敗。畢竟興趣這東西本來就不是注定的5，而是在成長過程中逐漸建構出來的，所以縱使我對這個研究興趣非常地癡迷，但若因種種因素無法繼續追求這個夢想，我也能接受進入職場工作。人生能做自己最喜歡的事固然最好，但這不代表「最喜歡的事」靜止在那邊等著被追求。「最喜歡的事」應該是會動的，並在我們努力體驗人生的道路上與我們相遇。\n另一種「多元」的概念，是意識到並尊重人會因為經歷相異而有所不同。高中以前，我生活在一個傳統保守的環境。上了高中後，接受了一些新想法，讓我當時在價值觀上有些拉扯。大學將近畢業時，終於能比較冷靜地理解與體會各種衝突的價值觀。因為經歷過這種價值觀反轉 (又轉回去，最後轉到中間) 的過程，我能充分體驗到人的想法真的會因成長背景而有頗大的差異。沒有那個最「正確」或最「高尚」的觀念，每種觀念都有其價值與重量，因為它們背後都代表著一個人生活以及成長的經驗。\n小結 回顧大學生活，會發現自己何其幸運。首先，若不是父母給予我經濟支持以及學習的自由，我不可能無憂無慮、隨心所欲地學習周遭的事物。此外，大學時期沒有最好的朋友的陪伴，我也不可能認真體驗生活、反省與思索自己的所作所為。上文中的許多內在特質，都是在與朋友相處交流中，逐漸建構出來的。我想，這就是我大學最幸運的事了！\n我這邊不是在批評學生想盡量讓研究題目符合老師的期待這件事。相反的，我覺得這是必須的。我這邊只是要強調學生不該在未經慎重考慮下，直接嘗試 fit in 老師的期待。 ↩︎\n我是在意識到生物學與我感興趣的主題有很大的關聯，但不是唯一重要的關聯才選擇離開的。在這段期間得到的想法已讓我看到足夠的東西，但更深入的鑽研這個領域，只會讓我逐漸遠離自己的興趣，所以我才選擇離開。雖然這段期間的學習相當地痛苦，但 (事後看來) 卻帶給我許多的收穫。 ↩︎\n事實上我還未在台灣看過與我研究興趣類似的人。 ↩︎\n學習與追求知識本身，就是個非常享受的過程，所以就算未來無法就讀相關領域，光是學習本身就足夠成為讀完碩班的理由。 ↩︎\n我不相信人「生來」就注定該做某事。這又是個 diversity 的概念。 ↩︎\n","subtitle":"","title":"大學獲得的重要東西","uri":"/2020/01/30/undergrad/"},{"content":"這本書關注的主題，一言以蔽之，就是人類演化。這個主題讓許多社會科學家相當感興趣，但又對它保持著一定的距離，因為演化這議題似乎有些敏感 — 碰到演化必須講到基因；講到基因似乎就離不開「生物性」、「天性」；扯到「基因」、「生物」、「天性」，就會覺得要以人類演化來解釋人類行為顯然不足，因為基因是如此地「固定」，但人類行為卻有著巨大的變異。這與以社會文化角度切入了解人類的科學家相當格格不入，因為就他們所觀察到的現象，對人類行為影響最大的應該是社會文化的因素。於是在研究人類的學術領域中，關注社會文化的不會去談生物或演化、關注遺傳生理的不會去談文化與社會。\nThe Secret of Our Success 想強調的是，要了解人類演化必須得跨越這條「基因 vs. 文化」的鴻溝 (在心理學裡也有類似的「先天 vs. 後天」)，因為這兩個東西在人類演化過程中是交織在一起的 — 人類所生存的社會文化環境長期影響了人類演化 (相對於其它生物演化受自然環境影響，人類演化受社會文化環境的影響可能更大)，而透過演化形塑出的人類心智也會反過來影響人類社會文化的樣態。這個想法很有挑戰性 (但已有段時間，見 Not By Genes Alone)，短時間內難以讓人信服，這裡挑選幾個書中的例子將這本書的內容整理下來。\n人類有何特別？ 假設有個外星人受到任務指派到地球觀察這裡的生物，他對「人類」這種生物會寫下什麼報告？人類作為地球生物的一員，最明顯的特徵是他們是棲息地最廣的單一物種1，而且人類在 4-6 萬年前就已踏上澳洲大陸、在 1 萬多年前踏上美洲，使五大洲上皆有了人類的蹤跡。適應新的自然環境對生物來說是很大的挑戰，生物必須演化出各種特徵才能適應環境。但人類這種生物，縱使有著這麼廣大的棲息環境 (從北極圈、喜馬拉雅山、撒哈拉沙漠到亞馬遜雨林)，卻有著極小的遺傳變異。換言之，人類適應新環境的方式與地球上其它的生物都不同 (以基因演化為主)，我們快速 (幾百至幾千年) 適應新環境的方式是透過文化演化，不斷修改累積上一代傳承下來的文化與技術，讓我們獲得更能適應當地的文化知識與技術。\n糙皮病 (Pellagra) 是一種缺乏維生素 B3 所引發的疾病，好發於以玉米為主食的貧窮族群上。知曉糙皮病歷史2 的人，或許會聽過 Joseph Goldberger 這位偉大的醫生為了證實糙皮病並非細菌感染而是營養不良所致所作做的控制實驗。但多數人不知道的是，透過適當的處理，玉米能夠釋放出足夠的維生素 B3，從而預防糙皮病的出現。這種適當的處理方法是傳統上以玉米為主食的的民族 (居住在玉米被馴化的地區) 所發展出來的，但這種食物處理方法並未隨著玉米一起被輸入其它地區，因而在世界各地造成糙皮病的大量出現。\n糙皮病是一個關於人類透過文化適應環境的好例子。生存在野生玉米的棲息地，人類開始馴化玉米作為固定農作物，同時為了適應玉米營養不易吸收的問題，發展出了特殊的食物處理方法。這裡非常有趣的是，雖然這些民族會使用特殊方法處理食傳統食物，但他們通常不知道為什麼 — 會以鹼水煮玉米的傳統民族大概不知道這樣處理可以預防糙皮病。既然如此，這些具有功能性、能幫助我們適應環境的社會慣例 (從食物處理、社會禁忌、甚至到占卜行為) 最初是如何產生、又如何在社群當中散布開來的？\n文化演化 人類社會中，常常有一些奇怪的慣例看起來意義不明，但社會成員通常會依循著這些慣例行事。在很多時候，這些意義不明的慣例具有意料之外的功能，有利於社群成員生存於當地環境。例如，前述玉米食用前的處理意義就相當不明 – 縱使不經過特殊處理，食用玉米短期內也不會有任何病痛症狀。這個食物處理的例子或許還不足以突顯具有功能性的慣例常存於社會中。為了說明人類常常透過遵守「功能未知」的慣例而獲得生存利益的情況確實存在，本書作者舉出許多實例，下面是另一個我很喜歡的例子。\n占卜決定打獵方向 在許多以狩獵為生的民族，打獵前的占卜是相當重要的活動，例如，有些民族打獵前會透過占卜決定要去哪裡打獵。所以打獵前的占卜有任何的功能性嗎？\n心理學與行為經濟學的研究發現人類心智上有一些「弱點」讓其在某些決策上無法達到絕對的理性，例如，人類非常不善於產出一組隨機的亂數。能將行為隨機化有時候是最好的策略，舉例來說，想像 A 和 B 要玩一個小遊戲，他們在遊戲的每個回合都要舉起其中一隻手：\n如果 A 和 B 舉起相同的手 (同為左手或同為右手)，A 就會獲利、B 就會損失 如果 A 和 B 舉起不同的手 (其中一方舉起左手、另一方舉起右手)，A 就會損失、B 就會獲利 在進行好幾回合後，獲利多者獲勝。這個遊戲叫做 Matching Pennies (下圖)，而玩這個遊戲時的最佳 (理性) 策略，無論你是 A 或是 B，就是隨機地選擇一隻手舉起。因為只要有一方的行為不是隨機的，對方就可以利用這個模式讓自己更容易 (或更不易) 與對方舉起同一隻手。人類在玩 Matching Pennies 時，通常不會 (或無法) 使用這種隨機策略，但其它動物如黑猩猩很快就會發現隨機是最佳策略並據此行動。\nMatching Pennies Game 獵人打獵時，其實就是在和獵物玩 Matching Pennies：獵人扮演的角色是 A (希望遇到獵物)，而獵物扮演的角色是 B (希望避開獵人)。因為人類心智上「無法將決策隨機化」的這個弱點，如果每次打獵都是由獵人決定打獵地區，長期下來獵物就能避開獵人。占卜在此能作為一種亂數產生機制 (例如，燒龜甲看其裂痕的形狀以決定打獵的地區)，幫助獵人隨機化決策，消弭獵人行為的可預測性。\n向成功者或多數人學習 玉米食用前的處理或是以占卜決定打獵方向的例子說明了人類社會中存在許多表面意義不明，但實際上卻具有功能性的慣例。但這些慣例何以會神奇地具備這些有利的功能？此外，人類社會難道不會出現劣質、弱化成員生存優勢的慣例嗎？\n這裡就進入了文化演化的核心理論。人類出生之後就生存在一個具有龐大文化知識的環境當中，如何挑選「好」而忽略「壞」的內容去學習，對個體之後的成功和生存有很大的影響。而要自行辨別出「好」或「壞」需要許多成本 (亦即，自行嘗試學習這些內容的後果)，因此在這個情況下最好的方法是依據一些捷徑 (heuristics) 策略，選擇學習與模仿的目標。例如，可以直接模仿或學習社群中較為年長或有成就的人，因為我們可以期待這些長者與成功者的行為或所擁有的文化知識 (或多或少地) 造就了他們的成功。「向多數人學習」則是另一條捷徑3 ，在與「向成功者學習」的搭配下，這兩條策略讓人類的社會能產出許多具有功能性的文化知識。因此，人類心智似乎天生就具備這些學習的傾向，讓我們更易在這種文化環境下生存 — 我們會有意識或無意識地模仿他人 (特別是精英、偶像或是優秀的同儕)、也常有從眾行為。\n舉例來說，假設部落原本普遍沒有打獵前的占卜行為，只有一個獵人會在打獵前占卜。一段時間後，部落成員會發現，這個獵人比較成功，但他們不知道為什麼 (是因為他的服飾、髮型、飲食習慣、還是打獵前的占卜行為？)，於是最好的方式就是模仿他的所有行為。透過模仿這位「成功獵人」，現在部落內有許多獵人在打獵前也會進行占卜，因此其他的新手獵人現在只要模仿「多數獵人」的行為，就可以直接獲取這個占卜行為的好處。\n這裡有個問題需要進一步說明，如果大家都模仿成功獵人的所有行為，那勢必會模仿到許多多餘的行為，進而引申出兩個問題：\n短期內，其他獵人是否會模仿到許多無用、甚至有害的行為？\n確實如此，這個現象放在當代世界或許更為明顯。因為人類具有模仿社會精英的傾向，我們有時會模仿到非常糟糕的行為，最極端的例子是名人在自殺之後，常會引起社會上的模仿性自殺。這也是文化演化的一體兩面：因為人類天生的這些學習傾向，造成社會精英或多數人的行為不論好壞皆有可能大量散布到社會上。那既然如此，為何長期下來，社會上仍會保留下有用的文化知識？ 長期下來，為何占卜行為會被保存下來，而不是「成功獵人」的其它行為？\n在眾多的模仿者當中，不見得每位模仿者皆會模仿到「成功獵人」的「占卜」行為 (有些可能只模仿到髮型或飲食習慣)，因此只有模仿到「占卜」行為的模仿者才會成功。這些成功的模仿者會進一步成為其它新獵人的榜樣，因而會將「占卜」行為傳承下去；但未模仿到「占卜」行為的獵人則不會成功，因而不會成為新獵人的模仿對象，因此其它與打獵成功性無關的行為 (髮型、飲食習慣等)，長期下來會逐漸被篩選淘汰。久而久之，社會上就會保存打獵前的占卜行為。「精英模仿」因此實際上是一套文化演化上的選擇機制，讓文化演化能產出對社群生存有利的文化工具。 我們沒有想像中的聰明 關於人類如何適應地球的廣大區域，許多人的直覺可能是 — 人類很聰明，在遇到困難時能透過智力「想出」解決之道。然而仔細思索前面提到的內容，會發現那些幫助我們解決困難的文化知識不是任何一個人 (像個發明家似地) 獨自想出來的，而是透過漫長的文化演化機制逐步篩選、修改與形塑出來的。舉個天馬行空的例子，如果愛因斯坦被丟到北極圈，他大概無法依靠他的智力想出如何在當地生存 (如何獵捕海豹、製作雪橇、蓋雪屋？)，但住在當地的愛斯基摩人，不需要愛因斯坦的智力，仍能持續存活於北極的惡劣環境。\n事實上，人類其實一點也不聰明，至少就「工作記憶」的層面上 — 人類大腦一次能處理的資訊相當有限，在工作記憶容量上與黑猩猩相差無幾。人類「看起來」很聰明是因為我們擁有許多透過文化傳承下來的「認知工具」，幫助我們更有效率地使用我們效能有限的腦袋。算盤與珠心算就是一種「認知工具」。珠心算高手透過「心像」操作算盤，每步計算過程對應到心像中珠子位置的改變。這種計算機制相較於直接依據數字進行計算，能省下許多認知資源，因為珠算過程中可以完全忽略數字的「數量」意義，僅有在計算完成後，才需將珠子的位置轉換回數字。而珠算高手能如此順暢地「操作」算盤，是因為算盤的結構經過長期演化，逐漸優化使其更易於心像操作 — 算盤的珠子數量正好與人類工作記憶的容量限制相當吻合，每排通常只有 4-5 顆珠子。\n類似珠算的這種「認知工具」在人類社會中處處可見，「數字系統」也是這種例子 — 在沒有「數字系統」的幫助下，人類無法處理大於工作記憶容量限制的數量，因此許多沒有「數字系統」的民族，他們對於數量只有「一」、「二」和「許多」的區別。另一個比較當代的例子是程式設計。程式碼長度的增長很容易就超出人類認知可處理的範圍，因此人類發展出各種程式設計法，幫助我們以更高層次、更簡約的方法去思考程式的設計與邏輯，例如物件導向程式設計就是借助人類時常與物件互動的習慣，減輕程式設計的認知負擔。\n回到愛因斯坦這位天才，如果在他之前沒有牛頓、馬克士威與黎曼，在這些科學家之前沒有微積分、矩陣與代數，而在這些數學概念之前又沒有「數字系統」，愛因斯坦能夠成為愛因斯坦嗎？人類確實比其它動物聰明 (如果將聰明定義為解決新問題的能力)，但不是因為我們有特別高等的智力，而是因為我們生存於一個充滿文化知識的環境。我們具有學習這些文化內容的動機與傾向，因而能不斷使用、改良我們繼承下來的文化。沒有文化，人類一點也不聰明！\n文化驅動基因演化 既然人類的生存如此仰賴文化知識、工具與技術，那文化究竟在人類演化的歷史裡佔有多重要的地位？換言之，人類的體質特徵有多少是為了適應文化環境，而非自然環境所產生？這個「文化能夠驅動基因演化」的想法著實駭人，因為我們總 (錯誤地) 認為人類文化與文明是近幾千或萬年才出現的東西，如此短暫的時間無法對演化速度緩慢的基因造成太大的影響。然而事實是文化出現在人類這物種的時間已相當久遠，甚至遠久於我們這物種，智人 (Homo sapiens) — 人類早在 200 萬年 (甚至更久) 以前就開始製造石器。這些石器並非如表面看起來「簡陋」，事實上製作石器比想像中的還困難，最起碼得要知道如何挑選適合的石頭 (並非所有石頭都可拿來製作石器)。此外打製石器需要特定的技巧，才能逐步將石器形塑成目標樣式。當代的考古學家甚至需花上好一段時間才能「學會」如何重製這些百萬年前的石器。\n手、工具、語言 人類使用石器 (或任何工具) 的久遠歷史突顯出這物種的一個特色 — 擁有極為靈巧的雙手。形塑出我們靈巧雙手的演化動力，大概來自工具的使用與製作。在 400 萬年前或更早，人類開始移居地面並以雙足行走，因而空出了雙手，讓這物種有更多的機會去使用工具。當然，單純地使用工具無法解釋人類何以有如此靈巧的手，因為黑猩猩也會使用工具。極為靈巧的手只有在製做精細複雜的工具才顯得有意義，而一個功能精緻複雜的工具又需經過代代的文化傳承與累積才能形塑出來 (文化演化)。換言之，在一個沒有文化累積的社群裡，並不會出現足夠的天擇壓力 (來自使用與製作精細工具的需求) 去形塑出靈巧的雙手。\n人類身體上靈巧性可與雙手匹敵的另一個部位就是「發音器官」(嘴唇、舌頭、聲帶)。大腦中控制發音器官、語言與雙手等的區域有相當的重疊，這與一些關於語言演化的理論相當吻合：\n人類的口語 (spoken language) 是在手語 (sign language) 之後才出現的。語言演化的軌跡可能大致是這樣的：在與工具共演化 (工具的文化演化與手靈巧性的基因演化) 的過程中，人類製作工具的能力 (包含手的靈巧性以及工具製作時需仰賴的認知能力，如「規劃」、「階層結構的處理4」) 逐漸提昇，因此這些演化出來的能力被用在其它領域上。語言可能就是如此出現的 — 靈巧的雙手能產出各式符號性的組合，雙手 (加上表情與其它肢體動作) 很適合作為編碼、傳遞語言的工具。而隨著工具製作一同演化的認知能力，如「階層結構的處理」也可能在語言中被用於處理語言的階層結構。\n很會流汗的長跑健將 工具製作與語言演化的例子或許顯得有些迂迴。人類身上其實有更多直觀的特徵，顯示文化環境曾在 (且正在) 人體留下痕跡。其中一個最明顯的例子是人類的消化系統：我們的消化系統，相較其它動物，非常弱。我們的牙齒和咬合力非常差，所以無法直接食用堅韌的食物；我們的消化道無法直接消化生肉，解毒食物的能力更弱，所以常有人們因為食物不新鮮或誤食野生動植物而中毒的例子。生存於惡劣自然環境的生物理應要有強悍的消化系統才能生存，但何以人類的消化系統如此脆弱？答案是我們的食物處理與烹飪技術所造成的 — 人類透過這些文化傳承下來的食物處理技術，在食用食物之前就開始消化食物了。這些食物處理技術讓食物更容易消化 (有時甚至為食物解毒與增加食物營養)，減輕消化系統的負擔。長期下來，我們因而不再需要如此強悍的消化系統 (弱化的消化系統能增加攝食的 cp 值，因為身體免除了維持強悍消化系統所需耗費的額外能量)。\n我個人最喜歡的一個例子則是人體的排汗系統。人類作為一種體質相對脆弱的生物 (相比其它動物，運動能力不佳且非常容易受傷)，有一種能力卻遠勝其它動物 — 人類非常善於長跑。長跑對動物最大的挑戰在於體溫的維持：隨著奔跑時間越長，體溫會開始上升，直到動物無法承受為止。例如，狗奔跑沒多久之後就必須停下來，大口喘氣散熱以降低體溫。人類卻可以長時間奔跑而不需停下來散熱，因為發達的汗腺讓我們可以在奔跑的過程中持續透過風吹過皮膚上的汗水進行散熱。但這邊有個弔詭的問題：雖然人類生理上具備高效率的散熱系統，但其生理上卻缺乏相應的儲水系統以應付排汗時的大量水分流失。天擇怎麼可能會塑造出這個殘缺的散熱機制？ 的確，在「自然環境」下，這種殘缺 (只會排汗，無法儲水) 的機制不可能會演化出來。一切只有在意識到人類是以「人造物」而非自己的身體去儲水，這個只知排汗不知儲水的散熱機制才顯得合理。換言之，沒有文化學習，人類就不會知道如何儲水與尋找水源，也就不可能演化出這個奇怪的散熱機制。接下來的問題就是，人類為何「需要」演化出這個散熱機制，它對人類的生存有何利益 (畢竟其他動物沒有這系統也活得好好的)？\n透過觀察生存於非洲 (人類的起源地) 傳統部落民族的狩獵方式，一些人類學家對於人類為何演化出這散熱機制提供一個可能的解釋。在炎熱的非洲大陸上，多數的掠食者如獅子和獵豹是在清晨或傍晚等較涼爽的時間進行狩獵，原因如同上述：炎熱的氣候會使這些掠食者的體溫上升而無法進行狩獵。但人類因為擁有高效率的散熱機制，反而可以利用動物的這項弱點，在天氣最炎熱的中午進行狩獵 — 雖然短時間內人類的速度跟不上獵物，但獵人可以緩慢且持續地追蹤獵物，獵物因而需被迫持續移動，直至無法承受飆升的體溫中暑而成為獵人的囊中物。因為這個原因，爆發力不如獵物的人類仍能透過持久戰成功捕獲獵物。這套散熱系統成為人類狩獵的優勢 (基因上的)，但這套系統要能演化出來的前提是人類已經具備某種儲水的能力 (文化上的)。\n一個新的物種 不斷累積與傳承文化的特性在人類這物種上已出現許久，長久下來逐漸形塑出特殊的生理特徵以應付生存於文化環境下的需求。上文的例子著重在文化所驅動的外在生理特徵 (動作控制、消化道、流汗等) 的演化，但忽略了一個非常有趣的議題 — 文化在人類大腦上留下過什麼痕跡？\n從外表的生理結構來看，人類的頭腦真的很「大」。這種「大」不單純只是腦化指數的數值大而已，甚至會造成女性難產，人類因此得縮短懷孕期，讓嬰兒提早出生以免頭大到生不出來。這現象說明了有一股很強的天擇壓力想將人類的腦袋變大 (不惜提升母親和嬰兒在生產時的死亡率)。那大腦袋的用途究竟為何？\n面對這個問題，「智力」這答案總是立刻蹦出，因為智力能幫助人類解決困難的問題 (雖然智力包含了許多子成份，如「推理」、「抽象思維」、「語言」與「學習」等，但許多人過於 (或只) 強調智力的「推理」與「抽象思維」成份)。但就如前面舉出的假想例子，頭腦既大又聰明的愛因斯坦仍無法透過推理能力想出如何獵捕海豹和建造雪屋。如果暫時跳開大腦袋與智商之間的關聯，而將焦點放在人類的發展過程，可以發現人類的腦袋除了「大」以外還具備以下特徵：\n頭腦的成長期非常長，直到二十幾歲人腦仍持續在成長。 人類出生時，心智雖然不是一塊白板 (blank slate)5，但非常接近白板：人類所擁有的能力或知識幾乎都是後天習得，剛出生的嬰兒幾乎什麼都不會。以學術詞彙來說，人腦的可塑性 (plasticity) 非常高，後天經驗能夠大幅形塑人類的腦袋。 這些特徵顯示人腦是一個為了應付長期學習的器官，而且對人類生存至關重要 — 天擇不惜增加嬰兒與幼童的死亡率，就是要讓人腦在出生後持續地邊成長邊學習 (這就是為何各個民族文化的差異如此巨大：不同的文化經驗塑造出不同的腦袋)。然而為何我們的成長過程需要如此長久？難道我們無法在短期 (5、6 年？) 內就學會我們文化的知識嗎？答案很明顯，曾在學習過程中遇過困難的人更能體會：人類透過文化所傳承下來的知識太過龐大，不要說短期，甚至一個人終其一生也無法掌握這些文化內容的一小部份。\n透過文化演化的機制，對於人類生存有利的文化知識與技術不斷被創造、改良並傳承下來，個體因而能透過後天習得這些知識獲得生存益處。隨著文化內容累積的越來越多，個體能獲得更多的生存利益，前提是他必須習得這些文化內容，因此一股新的天擇壓力誕生了 — 學習能力更好的腦袋會是更佳的生存利器。面對這個情況，天擇因此大幅增加了人腦的體積與可塑性、延長了嬰兒與兒童期、甚至製造出了青春期 (其它哺乳動物兩三年即成熟，沒有青春期)，讓人類獲得更長久的時間學習周遭的文化內容。\n繼承龐大文化內容的後果 這個「人腦為了學習文化內容而演化」是一個非常驚人的例子，它就像是一個能夠進行自我催化的化學反應：為了學習文化內容，天擇增強了人類的學習能力，這進一步造成更多文化內容的累積 (因為更好的學習能力能保存下更多的文化內容)，因此又反過來使增強學習能力的天擇壓力更加沉重。長期下來，人類在這兩套系統的協力演化之下，逐漸演化出令許多生物學家困惑的生理構造與能力。\n當然，人類學習能力的增長仍有上限，因為這個能力的增強伴隨著其它成本，例如，頭腦是個非常耗能的器官，需要足夠的營養和熱量；腦容量也受限於母親的產道大小 (人類為了能直立行走，骨盆腔較為窄小，進而限制了產道的大小)。因此，人類的學習能力會停滯在某個臨界點，但奇怪的是人類的文化似乎沒有受到這個限制 — 人類的腦容量 30 多萬年來幾乎沒有什麼改變，但我們的文化技術與知識卻在近代如指數函數般地快速成長累積6。我們是如何做到的？\n如前文所述，任何一個人終其一生都無法完全掌握人類的所有文化內容，但人類仍透過兩個 (在文字出現後，變成三個) 機制將龐大的文化內容保存下來：\n專業分工\n因為沒有人能單獨掌握所有的文化內容，讓每一個人都掌握一小部份的文化內容便是一個最好的方式。透過專業分工，每種「專家」能更深入地學習特定領域的文化內容，除了讓這份文化內容能更精確地傳承給下一代，也增加了改良這文化內容的機率。\n擴大社群規模\n專業分工讓人類能夠傳承更龐大的文化內容，因為每個人擁有的重疊的文化內容減少了。在專業分工的配合下，只要更多人的加入，人類就能將更大量的文化內容傳承下去 (更多腦袋，更多硬碟)。因此，人口數較大的社群，通常會擁有更龐大的文化知識與技術。\n文字系統\n文字系統出現並普及之後，幾乎將人類所能傳承下來的知識量的限制解除了 — 人類所能傳承下來的知識不再受限於整個社群的記憶限制 (更別說近代電腦及網路的出現)。\n將龐大文化內容傳承下去的機制對人類社會造成了極為深遠的影響，例如社會階級的形成、國家的誕生或甚至一神信仰的出現都可能與此有密切的關聯。\n小結 在此必須打住了，否則文章的長度就會變得與書本身一樣長了。The Secret of Our Success 提出了很多具有挑戰性的想法，更為這些想法大膽地提出一些解釋。透過這篇文章，我挑選書中的一些例子 (可能或多或少地加入一些我個人 (有誤) 的理解與詮釋)，將書的主幹整理了出來 (當然還有很多精彩的東西因為篇幅限制無法寫到)。這本書的內容，可說是我大學二年級以來一直在找尋的東西7，很高興在大學畢業前能看到這本書。\n蜜蜂的棲息地也很廣，但棲息在世界不同地區的蜜蜂是不同物種。 ↩︎\n我是在這才是心理學書中看到這例子的。 ↩︎\n至於原因並不直觀，需要比較複雜的演化模型去說明。結論就是，在一個團體裡面，只要有少數人的行為是有利生存的，「向多數人學習」這個策略對個體生存是有利的。詳細原因可見 Not By Genes Alone 或 Culture and the Evolutionary Process。 ↩︎\n人類社會中的許多工具結構其實非常複雜。例如，看似結構非常簡單的矛仍能分成幾個子構造，而每個子構造又各自有自己的結構與製作方式。在製作工具時具備一些處理「階層結構」(或「遞迴」與「組合」) 的認知能力，對於工具製作有應能提供不少幫助。 ↩︎\n亦即，心智具備某些傾向讓人類比較容易學會某些事情，如學會「怕蛇」。 ↩︎\n智人在非洲出現了約 20 萬年後才開始遷出非洲；再過了 8 萬年，世界五大洲皆有人類的蹤跡；最近的 2 萬年，我們發展出的知識與技術讓我們能造訪月球。 ↩︎\n我一直知道我對心理學與人類演化有很強烈的興趣。大一大二時，透過 Steven Pinker 的幾本書接觸到演化心理學，內容非常有趣但我一直不太確定少了些什麼。現在想想，原因或許是演化心理學的想法 (太過關注自然環境而忽略了文化環境) 和演化生物學太過接近 ，但人類與其它哺乳動物的差異卻如此巨大，演化心理學因此不易解釋人類何以如此。大三時，看了 Not By Genes Alone (作者之一為 The Secret of Our Success 作者，Joseph Henrich，的老師)，讓我從椅子上跳了起來：人類學家能看到的東西真的和其它人很不一樣，再加上從演化生物學與族群遺傳學借來的數學工具，這本書闡述了一套相當有挑戰性卻又嚴謹的關於人類演化的理論。The Secret of Our Success 可視為是 Not By Genes Alone 的續集，但內容偏重在實徵證據而非人類演化的理論 (理論基礎已由 Not By Genes Alone 和更早的 Culture and the Evolutionary Process 奠定)。 ↩︎\n","subtitle":"","title":"閱讀筆記：The Secret of Our Success","uri":"/2019/08/15/secretofoursuccess/"},{"content":"如果要從大學四年所看過的書選擇一本最推薦的，《好人總是自以為是》 (The Rightious Mind, Jonathan Haidt) 會是我推薦的那本。就書的背後理論思維而言，其實《好人總是自以為是》與另一本非常有名的書《快思慢想》(Thinking, Fast and Slow, Daniel Kahneman) 相當類似，闡述的皆是心理學中影響廣泛的 dual process theory—亦即人類認知上在處理資訊時，有兩種非常不一樣的方式：直覺式、自動化的 vs. 控制的、費力思考的。我認為《好人總是自以為是》比《快思慢想》有趣的是，它的主題與我們的生活習習相關—為何人們在道德、政治及宗教立場上，分化與對立的現象如此嚴重？\n近幾年不論是新聞內容或是周遭朋友同學轉發的內容，都讓我越感擔心。對於同一件事情，我常常看到完全相反的詮釋，更令我感到不適的是，只要知道發文的機構或人是誰，就能猜對內文的立場。我們或許永遠無法確定事情背後的真相，但有一件事是確定的：沒有人永遠是正確的。從這個原則就可以判定，驅動這些兩極化論述的，是人自動化、直覺式、對於特定事物既有的信念，論述的內容不論多有邏輯或沒有邏輯，都是在這些直覺信念之後產生、用來說服他人和自己，自己所抱持的信念才是正確的。這點尤其值得我們這些接受當今高等教育的學生的警惕。現今的高等教育瀰漫的是一種崇尚自由、平等、關懷弱勢、社會正義的價值。與此同時，接受過高等教育的人通常在論述能力上相對優越，也因此許多支持這些自由與正義價值的論述在品質與邏輯嚴密性上都非常完善，而反方的論述常常在我們這些「知識份子」眼中顯得相當「無腦」。這裡正是「知識份子的傲慢」開始滲入的地方，我們常常仰賴我們優秀的論述能力，堵住與我們意見相左之人的嘴，一方面期待對方能在我們縝密的論述下被感召，另一方面也更加鞏固自己所抱持的特定信念。但我們通常沒意識到的是，剝開這些冠冕堂皇的論述之後，所剩下最核心的信念與價值觀念，其實並沒有優劣對錯之分，它反映的不過是每個人成長過程中逐步形塑出的人格、價值與道德觀。\n成長的過程中，時常聽到教育者或長輩諸如「邏輯思考很重要」、「辨認他人論述中的謬誤很重要」的論述，讀了心理系更讓我增添了為他人所犯的論述謬誤命名的能力。但我覺得我們的教育裡缺乏了一項更重要的東西—同理與自己不一樣的人，尤其是與自己立場天差地別的人。大學最後兩年的體驗，讓我發現這件事非常困難，因為它需要的不只是邏輯思考，更需要能以「牴觸個人核心價值」的觀點進行思考。我認為婚姻平權議題 (對我們這一代) 很適合當作一個開始，練習同理與自己立場相反的人，因為我們對這議題正反兩方背後的價值信念本來就有一些了解 (只是不見得慎重面對與思考過)。\n婚姻、家、愛情 在婚姻平權的議題上，我與周遭大多數的朋友同學一樣，立場是支持婚姻關係不應僅限於男女之間。婚姻平權的概念在我們這代中，似乎直覺到我們無法理解任何合理的反對理由。但究竟為什麼我們的許多長輩，包含我自己的父母親，反對婚姻平權的立場如此強烈？我一直不能 (或是拒絕？) 理解他們的想法，直到看過了《好人總是自以為是》、接受了一學期難以參透的文化人類學的薰陶之後，才開始稍稍理解為何婚姻平權的概念對許多長輩帶來如此強烈的衝擊。原因是這個概念強烈地攻擊了華人傳統文化下的家庭及婚姻的觀念。如果我們翻開民法親屬編 (越早期的越明顯)，會發現裡面不時透露出這種傳統的家庭概念；從儒家經典中也可體會到這種家庭的觀念：例如「仁者，人也，親親為大」、「孝弟也者，其為仁之本與」、「不孝有三無後為大」、「君子之道，造端乎夫婦；及其至也，察乎天地」，從這些名言中可以發現儒家的核心觀念「仁」與「孝」乃至「君子」都密切地與儒家文化下的「家」的概念緊密結合在一起，而「家」的組成，在儒家與華人文化下 (還有很多其它文化) 是以「夫婦」為基礎。這套龐大的價值觀念是許多人待人處事的依據與道德準則，修改民法 (某種程度上是許多人心中道德價值的體現) 的婚姻規定就如同在攻擊這套核心的道德價值。這對許多人來說是無法接受的，他們的感受或許類似郭靖聽到小龍女和楊過既為師徒又是伴侶時，所出現驚恐與憤怒。\n婚姻平權的論戰，底層所反映的其實是關於婚姻核心價值觀念的差異。我們這一代受到西方觀念的影響較大，「平等、正義」是我們這輩的許多人最核心的道德價值。但對於我們的長輩，「家庭」則非常核心—父慈子孝、相夫教子、修身齊家治國平天下 …… ，理想的「家」有個既定的模樣，是很多人非常重視甚至追求的重要目標。對於這兩種價值觀念，我認為兩者的本質皆相當的良善與感人，更別說有誰對誰錯、誰優誰劣的問題。事實上，在有了這層的思考之後，我開始認為在當今的臺灣社會選擇修改民法婚姻章確有不合理之處。民法親屬編是目前多數人道德價值的體現，修改民法親屬編就如同否決了多數人的道德價值，強迫人們「改良」他們的道德觀念，朝西歐與北美的主流思維邁進。\n下方以一個更極端的例子來說明修改婚姻以及家庭的定義可能對一個人的價值觀所產生的衝擊。\n愛情作為家庭的基礎 不論是在對同性婚姻接受度較高的歐美或是深受儒家思想薰陶的華人文化中，組成家庭的前提皆是婚姻，而婚姻的前提是愛情。換言之，「兩人間的情愛關係」與「家」的概念彼此是緊密結合的。這種對「家」的概念，排除了社會上特定的人組織家庭的可能，例如，有些人無法對其他人產生愛情的感受、有些人就是運氣不好找不到對的人、有些人 (因為太過特殊) 找不到願意以婚姻關係共同生活的人。\n由於我們 (無論支持或反對婚姻平權) 對於家庭的想像是建立在愛情的基礎上，關於多元成家 (家庭的組成不僅限於婚姻與血緣關係) 的討論引起的關注就相當稀少。我們這一輩的許多人對於婚姻平權的想像，背後的核心價值除了「平等」之外，也隱含著家庭是以「愛情關係」為基礎。就這點上，無論同性婚姻或是僅限一男一女的婚姻，其對於家庭的價值是相當類似的。因此，如果我們認為只有自己抱持的那種價值才是正確的，「不認同家庭應只能以愛情為基礎」 的人，也能進一步對於民法親屬篇婚姻章以及司法院釋字第七四八號解釋施行法提出批評，因為這些法律皆將家庭的組成限縮在愛情關係之下。\n同理自己討厭的人 這邊我想強調的是，不論是華人傳統家庭價值、婚姻平權的概念抑或多元成家的想法，它們背後各自的核心道德價值，並無好壞優劣。但同時，因為這些價值與個人的道德判斷有非常強列的連結，我們太容易因為這些信仰就對意見相左的人產生厭惡，拒絕去了解對方，甚至開始憎恨對方。能意識到許多爭執源自這些核心價值上的差異，並能接受自己不一定正確而對方也不一定錯誤，甚至能以對方的觀點看待事情並認同對方的想法—這是我對一個成熟、有智慧之人的理想想像。有智慧的人不一定能邏輯縝密地滔滔雄辯，但卻具有足夠的認知複雜度，能理解、接受與認同多種價值同時並存的可能。\n這是一件超級困難的事，因為要抑制道德直覺所產生的反感並同時以他人的觀點思考，需要消耗非常多的認知資源。大家寧可泡在同溫層裡互相取暖，既輕鬆又可反覆確認自己的正確性。生存在一個充滿正反對立的世界，這或許是我們最該培養的能力—在口嘴爆發前，我們付出多少努力同理自己討厭的人？\n","subtitle":"","title":"同理自己討厭的人","uri":"/2019/07/16/struggle-to-understand-people-u-hate/"},{"content":"I recently found a great course about web programming on edX and learned a lot from it (I knew little about back-end web development and were unfamiliar with JavaScript). When I learned that it is possible to draw an SVG with the mouse in the browser, some interesting stuff came up to me — can I download the SVG after I finished drawing? I found some solutions on the web pretty easily. Meanwhile, another interesting idea came upon — can I use the browser to convert local SVG images to PNG images? It turns out that this is completely possible with pure JavaScript. I started to realize how mighty web browsers are. As an exercise to familiarize myself with JavaScript, I implemented this idea in a simple static web page, svg2png, which can convert multiple SVG images to PNG images (with options to set the resolution of the PNGs). No Server, Zero Dependencies One good thing about svg2png is that the computations are all done in the browser. So compared to other online services (e.g. this), there are no file upload limits. Also, svg2png lives on the web, so there is no need to download any thing (instead of a modern browser) before using it. How It Works? I used HTML5’s to let users upload their SVGs. After that, the files are converted into object URLs. This allows me to load the SVG images (invisibly) with , so I can get the information of these images (height and width). The second step is to render the SVG on Canvas with the canvg library in order to convert it to PNG later (using canvas.toBlob()). The trick to adjust the DPI of the output PNG is by scaling the height and width of the canvas, which is handily provided by the scaleWidth and scaleHeight options in canvg(). But since I have no way to get the size information of the original image directly from the File object retrieved from , I have to load the SVG images with the tags first. The source code of this simple project can be found here on GitHub. ","subtitle":"","title":"Convert SVGs to PNGs with Your Web Browser","uri":"/2019/07/06/svg2png/"},{"content":"從學期初修 Python1 就一直在吃老本，直到最近學到物件導向2才開始認真思索寫筆記這件事。就寫筆記而言，我最熟悉的工具3當然是 R Markdown，而 RStudio 目前對 Python 也有不錯的支援。但我平常把 JupyterLab 當成 Python 的 IDE 在使用，自然比較習慣這個環境，因此就決定使用 Jupyter Notebook 來寫筆記。我第一個想到的問題就是：R Markdown 的世界裡有 bookdown 將多個 .Rmd 變成一本書 (網頁)，但 Jupyter Notebook 似乎沒有這麼方便的工具4。我錯了。好概念傳播的很快，受到 bookdown 的啟發，Jupyter 的世界裡也出現了一套類似的工具 — Jupyter Book。 What is Jupyter Book? 其實 Jupyter Book 的概念非常簡單： 將 Jupyter Notebook (.ipynb) 轉換成 markdown (.md) 使用 Jekyll (靜態網頁產生器) 生成網頁 但它真正厲害的地方在於它的靜態網頁模板 (範例)： 版面風格與 Jupyter Notebook 類似 網頁模板添加許多功能 .ipynb 下載連結 binder 連結 直接執行程式碼 (by Thebe Lab) Jupyter Book 的 Jekyll 模板 Jupyter Book 提供了相當清楚易懂的說明文件。唯一需要注意的是，Jupyter Book 必須使用 Jekyll 才能在個人電腦上預覽網頁，而安裝 Jekyll 是非常麻煩的事 (尤其在 Windows 上)。為解決這麻煩，Jupyter Book 很細心的提供了以 docker 使用 Jekyll 的方式，減少安裝 Jekyll 的麻煩 (但學習使用 docker 又是另一個麻煩事)。如果對於 Jekyll 運作了然於心，其實可以不用在個人電腦上安裝 Jekyll 預覽網頁，直接將生成的檔案丟到 GitHub 上讓 GitHub Pages 生成網頁。 但對多數人最頭痛的應該是 Jekyll 模板的結構，這邊提供快速上手的說明 (換言之，在不懂 Jekyll 下使用 Jupyter Book 的模板)。 使用 Jupter Book 安裝 1pip install jupyter-book 匯入模板 1jupyter-book create mybookname 這個指令會匯入 Jupyter Book 的 Jekyll 模板 (簡化)： ``` mybookname/ ├── _config.yml ├── content │ ├── images │ │ └── logo │ │ └── favicon.ico │ └── notebook.ipynb └── _data └── toc.yml ``` 在這模板裡面， - **`content/`** 存放 Jupyter Notebook 的地方，可依照自己喜好使用任意檔案結構 (e.g. `content/01/` 裡面放多個 notebook，如 `content/01/intro.ipynb`, `content/01/hello-world.ipynb`；或是直接將 notebook 丟在 `content/` 之下)。 - **`_data/toc.yml`** `content/` 中的檔案結構可有很大彈性，因為網頁的目錄 (左欄) 連結是在 `_data/toc.yml` 中**手動設定**的。例如，要將 `content/01/intro.ipynb` 生成之網頁的連結放在目錄上 ，得在 `_data/toc.yml` 設定： ```yaml - title: Introduction url: /01/intro not_numbered: false expand_sections: false ``` - **`_config.yml`** 這裡是設定 Jekyll 網頁的一些資訊，例如網站名稱和作者等。特別需要注意的是 `baseurl` 和 `url` 這兩個項目。如果你的網頁是透過 GitHub Pages 產生的 (假設這份 Jekyll 模板上傳到 GitHub 的 `mybookname` repo)，那 baseurl 就會是 `/mybookname`： 1baseurl: /mybookname 2url: https://.github.io 輸出 (不需 Jekyll)：將 Working directory 設成 mybookname/，執行下方指令 1python scripts/clean.py # 清理之前產生的檔案 2jupyter-book build ./ # 從 `contents/` 產生 Jekyll 能處理的檔案 上傳至 GitHub (記得到 Repo 的 Settings 裡設定，讓 GitHub Pages 使用 master branch 生成網頁) 這樣就大功告成了！ My Python Note liao961120/pynote 是我透過 Jupyter Book 設置的 Python 筆記，與上面介紹不同的是，我使用的是 netlify 而非 GitHub Pages；另外，我也透過 Travis-CI 幫我執行 python scripts/clean.py 與 jupyter-book build ./，所以就不用在每次修改筆記後，還得在電腦上跑這些指令。 本來沒預計要修，沒想到選上了又加上學分不足，想趁這個機會逼自己熟悉一下 Python。 ↩︎ 去年七月的時候，開始栽入 R 套件開發的世界。這讓我比較深入的接觸 R 語言，同時也讓我更深刻體驗到「(一段時間後) 看不懂自己程式碼」的感覺。一方面，我覺得 R 向量式的思維習慣，讓我在學 Python (不使用 numpy, pandas 等套件) 的過程中有點卡；另一方面，我過去一直不太能掌握的物件導向的概念，但最近聽完課後竟然有頓悟的感覺。我猜原因不是因為老師講得特別好，單純是因為自己寫過太多 buggy 的程式，所以現在開始比較能體會 OOP 的思維方式如何減輕程式開發時的認知負擔。以前程式還寫得不夠多的時候，OOP 對我而言只是一堆生澀的術語和概念，反而造成學習上的困難。 ↩︎ 我認為一個好的筆記需要具備這些特徵： 能長存 (i.e. 不會被資源回收掉、不小心殺掉或遺忘在電腦與雲端硬碟的某個資料夾裡) 方便瀏覽，包含能快速找到筆記的位置 (我放在哪裡) 以及特定內容 (OO 概念寫在哪裡？) 換言之，好的筆記意味著好的檔案管理與搜尋方式。在這個時代下，(靜態) 網頁正是管理筆記的好工具：它能長存且方便瀏覽 (i.e. 能輕易的在頁面間切換、在頁面內搜尋文字，找到所需資訊)。 ↩︎ 當下的直覺想法是使用 nbconvert 將 .ipynb 轉換成 .md，把這些 .md 丟到一個網頁模板，再用靜態網頁產生器 (Jekyll 或 Hugo) 生成網頁。於是我開始這樣做，但試了一陣子之後就發現這些網頁模板都太複雜了，光是要研究它們就要花上不少時間。另外，使用這些模板還有一個缺點 — 人們喜歡熟悉的東西，換言之，常用 Jupyter Notebook 的人會習慣它的界面 (白、橘色)，但使用網頁模板會破壞這個習慣，讓使用者產生不適感。 ↩︎ ","subtitle":"Jupyter Book 簡介","title":"將 Jupyter Notebook 整理成一本書","uri":"/2019/05/11/jupyter-book/"},{"content":"Although I’m a strong believer in R Markdown, I’m not so sure about it in the realm of slide making. After using GUI slide making tools such as Powerpoint or Google Slides for years, it is not easy to get used to making slides with markdown, since it requires additional processing in the brain – the conversion between images in the brain and the markup language to create them on the slides. xaringan::inf_mr() greatly reduces this burden on the brain, but one serious drawback still persists. When making slides with Markdown, the source document often gets very long. I frequently found myself lost in the source document (after scrolling up and down), not knowing which part of the slide is it that I’m looking at1. To be fair, there are great things about making slides with markdown, for example, it’s easier to manage and reuse images through URLs, and markdown is convenient for formatting code (automatic syntax highlighting). I have been making a lot of slides with Xaringan lately. After spending hours on Xaringan, I began to have more faith in making slides with markdown because I figured out some tips to reduce the pain caused by markup langauges. The idea is simple – reduce the length and complexity of the source R Markdown document. knitr is my friend here. Use Several Source Documents bookdown uses several Rmd files to generate a book. The same idea can be used in Xaringan, and here are some advantages I can think of: By splitting the source document into several meaningful sub-documents, you can locate particular parts of your slide faster (the so-called “chunking”). Your slides can be easily reused, since you can copy a part of your slides by choosing the relevent Rmd file(s). (Not copy-and-pasting text from the parts you want in one lengthy Rmd file) Child Document To use several source Rmd documents to generate a single Xaringan (or any R Markdown) output, use knitr chunk option child to include other Rmd files in a Rmd document. For example, I would create one index.Rmd and several Rmd files with meaningful names (e.g., opening.Rmd, intro-github.Rmd, contact.Rmd, etc.): 1my-slide/ 2├── index.Rmd 3├── opening.Rmd 4├── intro-github.Rmd 5├── contact.Rmd 6├── html-table.txt 7├── img/ 8└── addons/ 9 ├── custom.css 10 └── macros.js The chunk option child is then used in index.Rmd to include other *.Rmd: 1 2```{r setup, include=FALSE} 3knitr::opts_chunk$set(echo = FALSE) 4options(knitr.duplicate.label = 'allow') 5``` 6 7```{r child='opening.Rmd'} 8``` 9 10```{r child='contact.Rmd'} 11``` Complex Markups in Independent Files Sometimes we may want to put cool things in the slides that can only be created from HTML syntax, for example, the table below (generated from TablesGenerator.com): HeuristicplausibleimplausibleSyntax-drivenplausibleP600 N400✗implausibleP600N400 This table looks simple, but it is generated by a 2728-character-long HTML code, which greatly increases the length and complexity of the source document. To make the source document cleaner, you can put code of these kinds in separate text files and include them into the R Markdown source document by knitr::asis_output(readLines('html-table.txt'))2 and R Markdown inline code r : 1A table generated from 2[TablesGenerator.com](https://www.tablesgenerator.com/html_tables#): 3 4`r knitr::asis_output(readLines('html-table.txt'))` remark.js Built-in Functionalities remark.js actually provides useful functionalities to help reduce the complexity of the slides. READ THE DOCs, and it might save you a great deal of time. Xaringan and remark.js both provide good wiki pages, and I regret I read them too late – after I spent a lot of time copy-and-pasting and made my Rmd source document uglier. Below are some notes extract from Xaringan \u0026 remark.js Wiki pages. They serve as quick references (for myself) and aren’t meant to be detailed. Again, READ THE DOCs! Configuration Xaringan Configuration (remark has a more thorough documentation) is set in YAML frontmatter: 1output: 2 xaringan::moon_reader: 3 nature: 4 ratio: \"16:10\" 5 beforeInit: [\"addons/macros.js\", \"https://platform.twitter.com/widgets.js\"] 6 highlightLines: true 7 highlightSpans: false 8 navigation: 9 scroll: false 10 css: [default, default-fonts, addons/custom.css] 11 yolo: false 12 seal: true remark Special Syntax name: Adding ID to a slide name: about ## About reference with see [About](#about) count count: false This slide will not be counted. template name: template-slide Some content. --- template: template-slide Content appended to template-slide's content. layout Macros Image with Absolute postition: Define macros in addons/macros.js: 1remark.macros['abs'] = function(width=\"30%\", left=\"85%\", top=\"15%\", cl=\"\") { 2var url = this; 3return ''; 4}; Use it in markdown: 1![:abs width, left, top](url) 2 3![:abs 30%, 50%, 0%](url) 4 5 This is the major drawback of markup languages compared to GUI authoring tools. Although markdown itself is designed to deal with this drawback (i.e. the too-complicated HTML syntax), the inherently complex structure of slideshows still complicates the source document of the slides writen in markdown. ↩︎ Note that there is only one line in html-table.txt so readLines() returns a character vector of length 1. If there are multiple lines in the text file, one needs to use paste(readLines('html-table.txt'), collapse = '\\n') to properly print out the text file. ↩︎ ","subtitle":"","title":"Tips to Reduce the Complexity of Slide Making with Xaringan","uri":"/2019/04/29/xaringan_tips/"},{"content":"Those who use knitr::include_graphics() frequently in their R Markdown files may discover some inconsistencies (from the user point of view) if the same Rmd is used for multiple output formats, especially when PDF (LaTeX) is involved. The following code works fine for HTML outputs but fails when the outputs are PDFs: 1knitr::include_graphics('local.gif') 1knitr::include_graphics('https://commonmark.org/images/markdown-mark.png') The first case is obvious since it’s impossible to include a GIF in a PDF document. The second case may cause some users to scratch their heads: “Why can’t I insert a PNG image in the PDF document?”. The answer is that, for PDFs, knitr::include_graphics() only works for local images because knitr::include_graphics won’t download the images for you, whereas for HTMLs, the images don’t have to be downloaded (images can be sourced using HTML img tags). To fix the problem that arise in the first case, one can use images that are compatible for both output formats, such as PNG or JPEG. Another solution is to include different figures for different output formats: 1if (knitr::is_html_output()) { 2 knitr::include_graphics('local.gif') 3} else { 4 knitr::include_graphics('local.png') 5} To fix the problem that arise in the second case, one has to remember not to pass an URL as path to knitr::include_graphics() if the Rmd is to be compiled to PDFs. include_graphics2() I can never be sure when I would want to make my Rmd, originally targeted for HTML outputs, available in PDFs. Also, I don’t want to write if (knitr::is_html_output()) every time I want to include a GIF. It is also a headache to download every figure from the web just for the PDF output. So I decided to write a wrapper of knitr::include_graphics() for myself, dealing with all the problems mentioned above. For including different figures for HTML and PDF outputs, use: 1include_graphics2('local.gif', 'local.png') The first argument local.gif is used when the output format is HTML, and the second, local.png, is used when it’s PDF. include_graphics2() also works fine with URLs. It is totally fine to use URLs instead of local file paths in the example above: 1png_url \u003c- 'https://commonmark.org/images/markdown-mark.png' 2gif_url \u003c- 'https://media.giphy.com/media/k3dcUPvxuNpK/giphy.gif' 3include_graphics2(png_url, gif_url) Source Code I made include_graphics2() available through a package I maintain (lingusiticsdown). You can read the documentation of include_graphics2() and a vignette of its usage on the package web page. For those who want to use include_graphics2() but don’t want to depend on linguisticsdown, feel free to copy the source of include_graphics2() to your personal package. If you don’t have one, you can still use the source script as regular R scripts, but remember to add the following lines to the top of the script: 1library(knitr) 2library(tools) ","subtitle":"","title":"Wrapper of knitr::include_graphics to Handle URLs \u0026 PDF Outputs","uri":"/2019/03/10/include_graphics2/"},{"content":"2023.2.27 更：下文套件已失修，若有需求，請參考 thesis。\n去年十月份的研究所甄試，實在找不到合乎主題的文章報告可以附在審查資料。想想自己可以拿來說嘴的大概只剩 R Markdown，於是寫了一篇 (硬是扯上語言學的) 文章交了上去。為了怕被面試老師問：「你在文章中說的可信嗎？文章中哪裡可看出你的研究能力？」便在文章中引用自己撰寫的套件1。由於還是害怕文章太過空泛，又將文章中的部份想法 – R Markdown 模板，實作出來以備不時之需，ntuthesis 因而誕生了。結果 … 面試時，老師們根本沒有問到有關 R Markdown 的問題。\n緣起 這個用 R Markdown 寫論文的想法，其實在一年多前剛認識 R Markdown 時時就有了。當時還不太確定是否可行，僅覺得如果可行的話一定會很有趣。現在，我百分百確定這是可行的 、九成九確定它能幫你省下論文排版的功夫。\nntuthesis 原本僅是個 bookdown (R Markdown 的一個擴充) 論文模板，但為了讓其便於使用，我將它作成套件。ntuthesis 的目的只有一個 – 讓作者能專注在論文內容的寫作。其它論文寫作的麻煩事：排版、口試委員審定書、目錄、圖目錄、表目錄、文獻引用格式、浮水印，全部都能自動生成。論文模板的概念並不新穎，且現存許多 LaTeX 論文模板。但撰寫 LaTeX 的過程非常辛苦，因為作者必須時時將注意力放在論文排版上。相對的，ntuthesis 讓作者能用 Markdown 撰寫論文，甚至可使用 R 語言直接在論文中動態產生結果 (如統計圖、數值與報表)。\n總是需要跨出第一步 現在的問題是，臺灣還沒有人用過 ntuthesis 撰寫論文，而當「第一位」總是相當可怕的，更何況還要脫離自己習慣的寫作環境 (例如，MS Word)。作為 ntuthesis 的作者，我一定會用 ntuthesis 撰寫論文，但不會是在短期內 (我才剛錄取研究所)。所以這篇文章的目的基本上有兩個：\n推銷 ntuthesis：\n尋找願意使用 ntuthesis 撰寫論文的研究生\n為 ntuthesis 擔保2：\n你專心寫論文，我負責處理論文格式。\n如果使用 ntuthesis 撰寫臺大3論文，你只要擔心論文的內容，任何套件相關的問題 (如排版設定不正確、bug、說明文件不清楚等) 我都會 (盡力) 協助解決 (不擺爛)。\n一點點的使用門檻 天下沒有白吃的午餐，使用新東西固然有點門檻：\n若已在用 R Markdown，那這門檻應該很低\n若已經會用 bookdown，恭喜你沒有門檻問題\n若不懂 R Markdown，甚至完全不懂 Markdown\n請花 3 分鐘閱讀維基百科，花 5 分鐘用用看 Markdown\n花一段悠閒的時光 (我是在臺中至臺北的客運上用平板滑完的)，輕鬆地閱讀僅 137 頁的 bookdown Book (PDF)。請不要很認真的細讀每頁，挑自己需要的看。請不要把它當成是負擔。當成是增廣見聞，欣賞世界上竟然有這種東西。看這本書真的很享受。\n問題 若有任何關於 ntuthesis 的問題，請儘管提出來。我最歡迎在 GitHub 提出 issue，但若沒有 GitHub 帳號，也可以透過 Email 與我聯絡。\nlinguisticsdown 當時僅有 3 個函數，而且是在兩天內寫完然後提交到 CRAN。雖然簡陋，但我覺得這套件是目前我所製造出來的一堆玩具中最有用的。 ↩︎\n由於我未來會使用 ntuthesis 撰寫論文，所以請相信我有動機 (至少自此之後的兩年) 維護此套件。 ↩︎\nntuthesis 其實亦可撰寫他校論文，但目前缺乏他校 (封面) 模板。 ↩︎\n","subtitle":"用 Markdown 撰寫博碩士論文","title":"ntuthesis","uri":"/2019/03/07/ntuthesis/"},{"content":" 語言到底是什麼？ 這個出現在語概課本第一章的問題，通常會說明人類語言和動物的溝通有什麼本質上的不同，並且指出人類語言具有結構、可組合的特性等等。但這些說明只是描述人類語言的特性，但對於語言是什麼，或更精確地說 – 語言如何產生 (包含認知處理上、生理發展上、演化歷史上)，並未提供有洞見的解釋。\n我想認知語言學的主要貢獻在於，其看見語言以外的世界，強調認知系統對於形塑語言的影響。對於語言的分析，因此需跨出「語言」，擴展到人類的認知系統。這真的是充滿雄心壯志的抱負，但我並不覺得這帶來的影響是全然正面的，因為面對人類心智運作的問題時，事情會變得非常非常非常複雜。特別是語言，從人類語言的相關研究可以發現語言真的很「雜亂 (noisy)」 (相較於視覺系統)。這使將戰線拉得如此廣泛的認知語言學可能變得十分混亂，在沒有透徹理解一個現象中比較基礎的因素前，就直接研究更複雜的整體現象1。這也是為何我對於 語言是什麼 這個問題感到困惑。我一直找不到一個清晰、穩固的基礎，作為評斷各種對語言的看法與理論的基準。我們似乎根本不了解語言是什麼，卻常在很抽象的層次上試圖說明語言是什麼。\n於是，我決定先從 Tomasello 的專著下手，因為他的研究興趣是語言 (甚至是人類) 的演化，而就我所學過的東西而言，演化論應是僅次於數學，最可靠的一種想法。\n看見更基礎的元素 看完 Origins of Human Communication (Tomasello, 2008) 後，不得不佩服 Tomasello 這位學者。剛了解他研究對象是嬰兒與靈長類動物、研究主題是心理學和語言學時，不太能理解為什麼，只以為他是興趣廣泛。讀完書後才了解到，這些內容都與 Tomasello 想回答的問題密切相關 – 語言的起源。\nTomasello 厲害的地方在於其能夠跳脫「語言」的框架，嘗試找出語言要能運作所需的更基礎的元素。而要找出這些基礎的元素，需要跳脫過去語言相關研究關注的主體 – 成年人類的語言使用，將目光聚焦在「尚未」發展出語言的族群上 – 黑猩猩與人類嬰兒，藉此找出哪些因素造成成年人類和這兩個族群有如此不同的語言表現。\n例如，「口語」在語言學中可說是最受關注的焦點 (相較於「文字」與「手勢」)，所以許多對於語言的研究與分析都是以口語作為基礎。但 Tomasello 強調了「手勢」的重要性。要了解「語言 (language)」，我們應該先了解「溝通 (communication)」，因為語言只是人類溝通的一種形式，而「指向 (pointing)」與「比手畫腳 (pantomiming)」則是人類溝通的另外兩種重要方式。「手勢」( 指向 與 比手畫腳 ) 的重要性在於，由於其所能攜帶的資訊量遠少於語言，其特別容易突顯人類溝通時，溝通者需具備的社會認知結構。這裡以書中提到的例子說明：\n我和妳正在去圖書館的路上。走到圖書館外面時，我指向停放腳踏車的地方。這時妳應該會感到奇怪，因為妳不知道我要表達什麼。\n但假設如果妳剛和男朋友分手，而且妳和我都知道這件事，而我指向的地方停有妳男友的腳踏車，那這個手勢就和上一種情況有完全不同的意義 (代表：「妳男友可能在圖書館裡，妳確定要進去嗎？」)。\n又或者，假設我指向的地方停著妳之前被偷的腳踏車，那這個手勢又代表著完全不一樣的意義。\n在上述的幾個例子中，同樣的手勢，甚至在同樣的客觀背景 (physical context) 下，表達了相異且十分複雜的意義。這些例子的差異僅來自於我和妳先前的共同經驗。這種現象在人類溝通中 (包含手勢與語言) 隨處可見。因此，這裡需要回答的問題是，什麼樣的認知結構使得人類在溝通時，能夠自然而然地形成上述的詮釋。\n黑猩猩 vs. 人類 關於上述的問題，一個最精簡的答案是溝通者理解彼此是「有意圖 (intentional)」的個體，所以在我指向腳踏車停放處時，妳會進行類似的推論：「他為什麼指向那裡，他想告訴我什麼？」但這只是個過於簡化的答案，因為黑猩猩也能理解溝通的對象是有意圖的，但黑猩猩卻無法發展出人類般複雜的溝通方式。例如，在餵食黑猩猩的情境中，若黑猩猩觀察到實驗者不願意 (unwilling) 餵食時，黑猩猩會表現出失望的行為；但如果實驗者是因為其它原因 (例如，餵食孔太小或是因為其它事情分心) 無法 (unable) 餵食，黑猩猩會持續耐心的等待 (Call, Hare, Carpenter, \u0026 Tomasello, 2004)。\n所以，人類溝通運作的關鍵不僅僅是能夠理解彼此具有意圖，還需要其它關鍵因素。Tomasello 在經過大量對於黑猩猩與人類嬰兒手勢溝通 (gestural communication) 的研究後，指出人類與黑猩猩之間存在一項重要的差異 – 人類的溝通本質上是合作性的 (cooperative)，但黑猩猩的溝通幾乎是自我中心與命令式的 (imperative)。換言之，黑猩猩的溝通意圖僅有要求 (request) 其他黑猩猩或人去做自己要他們達成的事情；人類的溝通意圖則不只侷限於要求，更多情況下是為了提供對方資訊 (inform) (例如前述的圖書館例子：我透過手勢告知妳對妳 (但不是我) 而言有用的資訊)，或是單純為了分享 (sharing) (例如：「今天發生了 … ，心情好糟喔！」)。值得注意的是，即使人類意圖要求別人為自己做事情的時候，仍然是以「合作性」的方式提出要求。例如我們通常使用婉轉的方式提出要求 (例如，使用問句)，在對方完成我們的要求後，表達感謝甚至是必須的。\n溝通的合作性結構 人類溝通中的「合作性」結構與人類的社會認知結構 (social-cognitive infrastructure) 之間關係密不可分。要有合作性的對話產生，溝通者之間必須建立一種抽象、共享的共同經驗 (common ground) – 溝通者彼此都知道，且也都知道「對方知道我知道」的事情。承接上述圖書館的例子：\n妳知道我指向停腳踏車處是為了告訴妳：「妳男友在圖書館裡」。這是因為我知道妳和妳男友分手，而妳也知道我知道這件事。\n值得注意的是這裡包含所謂遞迴 (recursive) 的認知結構：妳必須知道「我知道妳和妳男友分手」這件事，上述溝通的情況才會產生。換言之，如果妳並未告訴過我妳和妳男友分手這件事，但我從其它管道得知了這件事，妳對我指向圖書館外的腳踏車就不會有「他在告訴我，我男友在圖書館內」這個詮釋 (縱使妳看到了妳男友的腳踏車)2。因此，形成共同經驗的前提是，溝通者必須具備一種「遞迴讀心 (recursive mindreading)」的認知能力。\n當合作性的溝通是建立在共同經驗上，溝通者才能從共同經驗中推斷哪些資訊是對方感興趣的。同時，因為預設溝通者的動機是合作的，日常溝通中，我們才會以這種方式詮釋模糊的訊息：\n某個陌生人拍拍妳的肩膀再指向妳右邊臉頰，妳應該會認為他在告訴妳：「妳臉頰上有東西」。\n這個手勢明明有千百種詮釋方式，但因為人類會隱性地預設對方是善意的、提供的是對自己有用的訊息，所以會形成這種詮釋。\n語言的起源 人類的溝通，不論是指向 (pointing)、比手畫腳 (pantomiming) 或口語 (spoken language) 皆是建立在上述合作性的溝通結構3。基本上，「合作」可說是造成今日語言的面貌的最重要的原因之一。\n句法 人類語言中複雜的句法不會無緣無故出現，一定存在某種使用上的功能性壓力 (functional pressure)，才使語言演化出句法。人類溝通中的合作性結構提供了這種壓力。黑猩猩溝通的意圖僅有要求 (requesting) ，而要表達要求僅需非常精簡的 (或甚至不需要) 句法：試想祈使句的文法有多精簡 (例如，「(你) 走開」)。這是因為 要求 的溝通意圖所涉及的情境非常簡單 – 當下 (時間點)、你 (主體)、去做某事 (事件)。但人類的溝通是合作性的，我們溝通的意圖多半是為了提供訊息 (informing) 和分享 (sharing)。\n提供訊息 涉及的情境就比單純要求別人作某事要複雜的多了，因為提供訊息給對方常需涉及不在當下 (時間上或空間上) 的事件 (對方不知道的事情，例如，「我在回來的路上看到眼鏡蛇」，常常不是在溝通當下能看到的事情)。提供訊息 這種溝通上的需求因而就使句法變得必要 – 我們需要標示訊息中不在對方當下注意力可及範圍內的人或事件、需要標示誰 對 誰 做了什麼 事，而句法可以達成這些目的。分享所見所聞又比起提供訊息涉及更為複雜的情境，因為我們需要完整地描述所見所聞或甚至「說故事」。這使得溝通必須仰賴更加複雜句法才能達成目的，所以語言中開始出現更複雜結構，例如，子句結構，以描述許多彼此環環相扣的事件。\n先有手勢，才有口語 語言的相關研究十分強調 口語 的重要性，甚至有關尼安德塔人是否具備語言能力的爭論都被簡化成尼安德塔人會不會說話的爭論4 (Arensburg et al., 1989; D’Anastasio et al., 2013)。口語固然有其重要性，但 Tomasello 認為口語不是語言的起源。口語是在手語之後 (或伴隨手語）出現的，而手語則是由比手畫腳演變而成。主要論點如下：\n黑猩猩，目前在演化樹上與人類最接近的親屬，能透過手勢彈性地傳達多種訊息。但黑猩猩的發聲器官卻相當侷限，其發出的聲音僅能用於固定的情緒表達。因此在演化歷史上，「手勢」會是個比較好的材料，較可能作為形塑語言的基礎。\n口語 (spoken language) 是約定俗成 (conventionalized) 與專斷的 (arbitrary)。換句話說，同樣的物體，例如「月亮」，在不同語言會有完全不同的形式 (發音)。因此，除非經過約定，口語本身並無法「攜帶」意義。相反地，手勢本身就能蘊含意義：\n指向 (pointing) 能透過引導對方的注意力至你我之外的第三個物體，期望對方在看到物體後能做出適當的推論5，解讀我欲傳達的訊息。但「未約定俗成的口語」就只是無意義的聲音而已，而聲音無法像「指向」一般具有彈性操弄對方注意力焦點的功能 (可以指向這裡、那裡、我、你、他)，因為聲音只能將對方注意力吸引到自己身上。\n比手畫腳 (pantomiming) 比起指向 (pointing) 又更能彈性地傳達意義。比手畫腳 是透過手勢去模擬真實世界中發生的事情，因此即使溝通者之間的共同經驗 (common ground) 很少，也能透過這種方式溝通 (試想自己是如何在語言不通的外國生存的)。\n因此，演化歷史上比較可能的情況是先有手勢，而手勢中能傳達較豐富意義之 比手畫腳 逐漸被約定俗成，進而形成了手語6。手勢中的 指向 則保留自今，變成口語的輔助。而口語則是在手語出現之後才演化出來，並取代原本手語的角色。\n在適當的環境之下，人類能自然而然發展出手語。手語在各方面都與我們所熟知的「語言」沒有什麼太大的區別 – 使用手語時，大腦語言區或活化；手語和所有語言一樣富含規則，違反句文法的使用也會讓手語使用者覺得「怪怪的」。事實上，語言 = 口語 的概念似乎太過深植人心，導致許多人在知道「原來使用手語時，大腦語言區會『亮起』」時，感到非常訝異 (包含我)。但如果在演化歷史上，語言本來就是從視覺模態 (visual modality) 轉變成聽覺模態 (auditory modality)，那今日所觀察到的手語現象就有相當自然的解釋。\n小結 Truth is much too complicated to allow anything but approximations.\n— John von Neumann\nTomasello (2008) 提供了一篇龐雜，但相當具有說服力的關於語言演化的故事。然而任何理論都是假的，包含 Tomasello 令人印象深刻的故事，有些理論卻是有用的。在這裡，演化論扮演相當好的角色 – 它幫助我們思考、提出合理的猜測，去解釋人類那些看似彼此沒有任何關係的功能如何互相依存與支持，在演化歷史上孕育出新的功能。縱使這個故事是假的，它提供了許多能被驗證的假設。看著假設提出與驗證的過程，我似乎能在複雜到令人絕望的心智科學中，看見一條能夠前進的道路。\n參考資料 Arensburg, B., Tillier, A. M., Vandermeersch, B., Duday, H., Schepartz, L. A., \u0026 Rak, Y. (1989). A Middle Palaeolithic human hyoid bone. Nature, 338, 758. Journal Article. https://doi.org/10.1038/338758a0\nCall, J., Hare, B., Carpenter, M., \u0026 Tomasello, M. (2004). “Unwilling” versus “unable”: Chimpanzees’ understanding of human intentional action. Developmental Science, 7(4), 488–498. https://doi.org/10.1111/j.1467-7687.2004.00368.x\nD’Anastasio, R., Wroe, S., Tuniz, C., Mancini, L., Cesana, D. T., Dreossi, D., … Capasso, L. (2013). Micro-Biomechanics of the Kebara 2 Hyoid and Its Implications for Speech in Neanderthals. PLOS ONE, 8(12), e82261. Journal Article. https://doi.org/10.1371/journal.pone.0082261\nGeeraerts, D. (Ed.). (2006). Cognitive linguistics: Basic readings. Berlin: Walter de Gruyter.\nTomasello, M. (2008). Origins of human communication. Cambridge, Massachusetts: MIT Press.\n我覺得這是人類心智科學普遍的情況，因為人類大腦非常複雜、難以窺視其運作，不像物理、化學等基礎學科，我們難以知曉心智的哪些功能與現象是基礎的，哪些是複雜的。↩\n或是另一種更複雜的情況：妳並未告訴過我妳和妳男友分手這件事，但我從其它管道得知了這件事，而妳又從某個管道得知「我從其它管道得知妳和妳男友分手」這件事。換言之，我知道妳和妳男友分手，妳也知道我知道這件事 (這種結構還能持續無限遞迴下去)，但因為我們從未將此事納入我們的共同經驗 (common ground) 中，亦即這件事並未在我們過去的互動中「公開」(妳並未告訴我)，這件事就不會造成妳認為我是在告訴妳「妳男友在圖書館」。↩\n值得注意的是，當溝通中的合作性結構消失時，可能代表著非常可怕的事情即將發生。試想自己見過最火爆的爭吵，對話者說話 (「吼」話或許比較貼切) 不是為了提供訊息或分享，而是單純希望壓制對方、讓對方閉嘴。若透過聲音彰顯的怒氣不足以抑制對方，甚至可能使用肢體暴力。↩\n當然，若尼安德塔人確實會說話，那確實 (至少從 Tomasello (2008) 的理論) 能證實其擁有語言能力。但這裡想強調的是，縱使尼安德塔人不會說話，並不代表尼安德塔人不具有語言能力。↩\n指向 (pointing) 所能傳達之訊息的複雜程度，受到溝通者之間的共同經驗 (common ground) 影響。摯友、家人之間有很許多共同經驗，所以一個 指向 就能傳達含有非常複雜意義的訊息。相反的，陌生人，或甚至是來自不同國家的陌生人，彼此的共同經驗就很少，因此無法透過 指向 傳達太過複雜的訊息。↩\n有趣的是，口語 (spoken language) 與 比手畫腳 (pantomiming) 之間的對應關係也可在發展語言中的嬰兒身上見到。嬰兒在 1 歲到 2 歲的過程中，使用 指向 (pointing) 的頻率逐漸攀升，但使用 比手畫腳 (pantomiming) 與固定手勢 (conventional gestures) 的頻率卻持續下降。換句話說，口語 的功能在嬰兒語言發展過程中，逐漸取代了 比手畫腳 與固定手勢 (Tomasello, 2008, p. 145 - 153)。↩\nLast updated: 2019-03-07 ","subtitle":"","title":"閱讀筆記：Origins of Human Communication","uri":"/2019/03/04/originsofhumancommunication/"},{"content":"這學期修課 (地圖與地理資訊系統) 需要使用 ArcGIS。由於 ArcGIS 僅能運行於 Windows 上，加上 我真的受不了 Virtualbox 跑 Windows 的速度 我不想在我的電腦 dual boot Windows 因此我用了 Windows To Go，將 Windows 10 安裝於外接硬碟，讓電腦可以透過外接硬碟直接運行 Windows10。 因為我對電腦硬體沒什麼概念，安裝 Windows To Go 花了我一大堆時間。這個筆記主要參考 Macbook Air 2013 製作 UEFI 引導的 Windows To Go 和 將Windows 10打包至USB裝置帶著跑 這兩篇文章撰寫而成，這裡僅是快速記下我在 Windows 上使用 Diskpart 安裝 Windows To Go 至外接硬碟的過程，若有問題…不要問我。 順便在這裡呼籲老師們，站在教育者的立場，應該少使用閉源、不支援跨平台的軟體。當然，我理解每個領域有自己傳統、預設的系統，但世界正在改變，而且學生的就業也越來越廣泛，這些「傳統」的系統常常價格昂貴，不會是學生未來會使用到的軟體。 事前準備 Windows 10 ISO 安裝檔 (我從學校的授權軟體下載) 一台已安裝 Windows 10 的電腦 一個外接硬碟 先將 ISO 檔複製到 Windows 10 的電腦上，按右鍵掛載。其會以 CD ROM 的形式出現在本機 (e.g. V: 槽) Diskpart 建立磁區 以系統管理員權限執行命令提示字元，依序執行以下指令： diskpart list disk select disk #: # 是數字，為要安裝 Windows To Go 的外接硬碟 clean: 清除硬碟上所有 partition convert GPT: 使用 GPT partition table create partition EFI size=300: 建立 EFI 開機磁區 (300 MB) list partition select partition #: # 是數字，選擇剛建立的 EFI 磁區 format quick fs=fat32: 將 EFI 磁區格式化 assign letter=S: 將 EFI 磁區掛載於 S 槽 exit: 離開 diskpart 使用 GUI 建立磁區 理論上這邊也可以用 Diskpart 建立，但我沒 (興趣) 試過，所以依照我原來看到的教學寫下。 打開 電腦管理 (可用搜尋)，會出現下圖 在左欄點選 磁碟管理，即可在中間下方看到剛建立的 EFI 磁區以及未配置的磁碟空間 右鍵點選未配置空間 \u003e 新增簡單磁區 建立 NTFS 分割 (格式化)，可以自行決定要多大的空間 (我設 150 GB)。 記下此新增的磁區掛在哪個槽 (e.g. E:) 將 ISO 檔安裝到外接硬碟 回到命令提示字元，輸入： dism /apply-image /imagefile:V:\\sources\\install.wim /index:1 /applydir:E:\\ 其中，V 是掛載之 Windows ISO 檔，E 是剛劃分之 150 GB 外接硬碟中的磁區。 在上述指令跑完後，使用 bcdboot E:\\Windows /s S: /f UEFI 將開機檔案從 Windows To Go 的磁區 (E:) 複製到 EFI 磁區 (S:) 中。 在外接硬碟安裝 Windows To Go 接下來，就可以拿著這個外接硬碟去開機了。找一台支援 UEFI 開機的電腦，開機前先將外接硬碟插入，開機時按快捷鍵進入開機選單，選擇以外接硬碟開機。若不想每次開機都要進入開機選單，可修改 Boot option priority (見下圖)，將此外接硬碟設為第一優先，電腦內建作業系統開機設為第二。如此，在每次開機時，若外接硬碟是插著的，那就會以 Windows To Go 開機；若未插入外接硬碟，則會進入電腦原本的作業系統。 第一次開機時，會跑出安裝 Windows 的畫面。安裝好之後，就完成了 Windows To Go 的設定。 參考資料 上文中的兩張圖片分別取自 1. 和 2. Macbook Air 2013 製作 UEFI 引導的 Windows To Go + Time Machine的外接硬碟 將 Windows 10打包至USB裝置帶著跑 diskpart 硬碟分割指令 What Is an EFI File? ","subtitle":"","title":"安裝 UEFI 開機功能之 Windows To Go","uri":"/2019/03/02/wintogo/"},{"content":" Taiwan Language Survey is a small project I worked on during May to June in 2018. The idea was to create a survey that continuously collects data and a web page that visualizes the collected data. The web page is updated weekly using Travis-CI. The main purpose of this survey is to raise public awareness of language loss in Taiwan. Hence, the survey is designed to collect data that can provide valuable information about language loss, for example, some questions were asked to gain insight about the change of linguistic competence acoss generations in a family (i.e. across the subject’s parents, the subject, and the subject’s children). In addition to changes within family, information about the age of the subjects is also colleceted, meaning that we can see how linguistic competence changes among diffrent age groups of subjects in a community, i.e. is a language becoming more dominant or entering the dying process? Visualization is a powerful tool to capture how linguistic competences of different langauges are changing. But creating visualizations necessitates creativity – how can language loss be visualized? Below, I illustrate one of the methods I created for visualizing language loss – a visaulization inspired by the age-sex pyramid. Age-Sex Pyramid of Language The age-sex pyramid is used to visualize the population structure of a community. The vertical axis indicates the age and each horizontal bar represents an age group. The horizontal axis indicates the population size of male or female of a particular age group. The age-sex pyramid is a great tool to visualize the population structure since the ‘shape’ of the pyramid gives readers a lot information. For example, an ‘expansive pyramid’ has longer bars at the bottom of the pyramid, which indicates the population is young and growing. A ‘stationary pyramid’ looks like a rectangular bar, indicating the population sizes of different age groups are about the same. A ‘constructive’ pyramid indicates a shrinking population, which is narrowed at the bottom. Similarly, a modified version of the age-sex pyramid, which I’ll call the ‘age-sex pyramid of language’, can be used to visualize the population structure of a language and predicts the language’s vitality. Instead of visualizing population size, the age-sex pyramid of language visualizes the average fluency of a language on the horizontal axis. Figure 1: An age-sex pyramid of Taiwanese. The red bars on the left indicates females of different age groups and the blue bars on the right indicates males. The average fluency (values on the horizontal axis) is calculated from a six-point scale (0-5) on Taiwanese fluency. As shown in Figure 1, the shape of the age-sex pyramid of Taiwanese in Taiwan1 is an ‘inverted triangle’, which is almost never seen in the conventional population pyramid. However, this inverted triangular shape is expected to appear quite often, since it indicates an ongoing language loss in a community. Vitality of Language Shape of Pyramid Shrinking and Dying Inverted Triangle Growing Triangle Stable Rectangular Bar Drawing Age-Sex Pyramid with ggplot2 As complex as it might seem, an age-sex pyramid created with ggplot2 is actually a (modified) bar chart. I learned this on stackoverflow, and the trick is Use ifelse to flip the value (here, population size) according to the gender of the age group Use geom_bar(stat = \"identity\") to let the heights of the bars represent values in the data frame, i.e. the value given to y Use coord_flip() to make the bars horizontal set.seed(1) df0 \u003c- tibble::tibble( Age = rep(c('10-19', '20-29', '30-39'), 2), Gender = rep(c('Female', 'Male'), each = 3), PopSize = sample(0:100, size = 6, replace = T) ) df0 Age Gender PopSize 10-19 Female 26 20-29 Female 37 30-39 Female 57 10-19 Male 91 20-29 Male 20 30-39 Male 90 library(ggplot2) pl \u003c- ggplot(df0, aes(x = Age, y = ifelse(Gender == 'Male', PopSize, -PopSize), fill = Gender)) + geom_bar(stat = 'identity') + coord_flip() pl To center the plot (i.e. to make the point where population size is zero at the center of the plot), we have to scale the axis of ‘population size’ (y) with scale_y_continuous: pl + scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, 25), labels = abs) + labs(y = 'Population Size') where abs in labels is the function abs(). By default, ggplot2 pass the value given in breaks to the the function specified in labels. Visualizing Language Loss To create an age-sex pyramid of language, the data structure needed is exactly the same as the one above, except the variable, PopSize, is replaced by ‘average fluency’ of a language. But since most people in Taiwan can speak more than one language (e.g. Mandarin-Taiwanese, Mandarin-Taiwanese-Hakka, Mandarin-English, etc.), the real data from the survey is a bit more complex Basically, the data structure needed to draw an age-sex pyramid of language looks like: Gender Ethnicity Age Group Avg. Fluency female Mandarin 20-24 4.53 male Taiwanese 20-24 2.78 … … … … female Hakka 25-29 2.23 male Taiwanese 35-39 3.57 Preparation of Data The raw data of Taiwan Language Survey can be retrieved here. The survey and raw data is in traditional Chinese. I’ll skip the step of cleaning raw data (e.g., turn variable names to English) and used the cleaned data survey.rds instead. temp \u003c- tempfile() download.file('https://raw.githubusercontent.com/twLangSurvey/twLangSurvey.github.io/master/data/survey.rds', destfile = temp) data \u003c- readr::read_rds(temp) head(data, 3) date curr_resid curr_resid_since settle_5yy home_town gender age kid_num edu_level work income work_hr tribe Mand_listen Mand_speak Tw_listen Tw_speak Hak_listen Hak_speak Ind_listen Ind_speak SEA_listen SEA_speak Eng_listen Eng_speak first_lang when_Mand when_Tw when_Hak when_Ind when_SEA when_Eng m_guard_identity f_guard_identity dad_Mand_speak dad_Tw_speak dad_Hak_speak dad_Eng_speak dad_Ind_speak dad_SEA_speak mom_Mand_speak mom_Tw_speak mom_Hak_speak mom_Eng_speak mom_Ind_speak mom_SEA_speak dad_mom_Mand_fq dad_mom_Tw_fq dad_mom_Hak_fq dad_mom_Ind_fq dad_mom_SEA_fq dad_mom_Other_fq me_dad_Mand_fq me_dad_Tw_fq me_dad_Hak_fq me_dad_Ind_fq me_dad_SEA_fq me_dad_Other_fq me_mom_Mand_fq me_mom_Tw_fq me_mom_Hak_fq me_mom_Ind_fq me_mom_SEA_fq me_mom_Other_fq 2018-06-12 106 1996 是 428 女 56 2 大學 金融業 10萬以上 45 - 50 小時 不具原住民身份 5 4 3 2 0 0 0 0 0 0 2 1 華語 7歲前 7歲前 未學會 未學會 未學會 12-15歲 父親 母親 3 0 0 0 0 0 3 4 0 0 0 0 幾乎全用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎全用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 約一半 多數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 2018-06-13 204 2004 是 204 女 57 1 碩士 製造業 10萬以上 50 - 55 小時 不具原住民身份 5 5 5 5 0 0 0 0 0 0 4 4 華語 7歲前 7歲前 未學會 未學會 未學會 12-15歲 父親 母親 5 5 0 1 0 0 1 5 0 0 0 0 幾乎不用 多數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 多數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 多數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 2018-06-13 103 2013 是 100 男 37 2 大學 金融業 55,000 - 60,000 25 - 30 小時 不具原住民身份 5 5 2 1 0 0 0 0 0 0 3 2 華語 7歲前 未學會 未學會 未學會 未學會 7-12歲 父親 母親 5 5 5 1 0 0 5 3 0 4 0 0 幾乎全用 少數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 多數使用 少數使用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎全用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 幾乎不用 The survey data, data, contains 22 variables and 267 observations (subjects). I only need these variables below to create the plot I want: gender: The gender of the subject age: The age of the subject m_guard_identity: Male guardian of the subject, should be ‘father’ in most cases f_guard_identity: Female guardian of the subject, should be ‘mother’ in most cases _speak: The subject’s fluency of a language. is one of ‘Mand’ (Mandarin), ‘Tw’ (Taiwanese), ‘Hak’ (Hakka), ‘Ind’ (languages of the indigenous peoples, aka Formosan languages, belong to Austronesian languages), ‘SEA’ (languages from South Easth Asia), and ‘Eng’ (English) dad__speak: The subject’s male guardian’s fluency of a language. is same as above. mom__speak: The subject’s female guardian’s fluency of a language. is same as above. library(dplyr) lang \u003c- c('Mand', 'Tw', 'Hak', 'Ind', 'SEA', 'Eng') cols \u003c- c('gender', 'age', 'm_guard_identity', 'f_guard_identity', paste0(lang, '_speak'), paste0('dad_', lang, '_speak'), paste0('mom_', lang, '_speak') ) data \u003c- data[, cols] %\u003e% filter(gender == '男' | gender == '女') Since some content of the survey data is in Chinese, the code below is used to translate it to English: ch2eng \u003c- function(x) { if (x == '男') return('Male') if (x == '女') return('Female') if (x == '母親') return('Mother') if (x == '父親') return('Father') if (x == '無') return('None') if (x == '(外)祖父') return('Grandpa') if (x == '(外)祖母') return('Grandma') if (x %in% c('阿姨', '嬸嬸', '舅媽', '姑姑', '伯母')) return('Aunt') if (x %in% c('叔叔', '伯伯', '舅舅', '姑丈', '姨丈')) return('Uncle') message('No translation found for `', x, '`', '\\n') return(x) } ch2eng_vec \u003c- function(vec) { new_vec \u003c- vector(typeof(vec), length(vec)) for (i in seq_along(vec)) { new_vec[[i]] \u003c- ch2eng(vec[[i]]) } return(new_vec) } data \u003c- data %\u003e% mutate(gender = ch2eng_vec(gender), m_guard_identity = ch2eng_vec(m_guard_identity), f_guard_identity = ch2eng_vec(f_guard_identity)) head(data) gender age m_guard_identity f_guard_identity Mand_speak Tw_speak Hak_speak Ind_speak SEA_speak Eng_speak dad_Mand_speak dad_Tw_speak dad_Hak_speak dad_Ind_speak dad_SEA_speak dad_Eng_speak mom_Mand_speak mom_Tw_speak mom_Hak_speak mom_Ind_speak mom_SEA_speak mom_Eng_speak Female 56 Father Mother 4 2 0 0 0 1 3 0 0 0 0 0 3 4 0 0 0 0 Female 57 Father Mother 5 5 0 0 0 4 5 5 0 0 0 1 1 5 0 0 0 0 Male 37 Father Mother 5 1 0 0 0 2 5 5 5 0 0 1 5 3 0 0 0 4 Female 44 Father Mother 5 4 0 0 0 4 5 5 0 0 0 0 5 5 0 0 0 0 Male 62 None None 5 5 0 0 0 1 5 0 4 0 0 0 5 5 0 0 0 0 Female 56 Father Mother 5 5 0 0 0 1 5 5 0 0 0 0 5 5 0 0 0 0 Defining Ethnicity You might notice that there is no variable in the data which explicitly indicates the ethnicity of the subject. To serve the purpose of this survey – visualizing the vitality of languages in a community, ethnicity is defined solely by the linguistics competence of a subject’s guardians. To give a specific example, let subject A has a mother who can speak2 Mandarin and Taiwanese and a father who speaks Mandarin and Hakka, then subject A is categorized as a Mandarin, a Taiwanese, and a Hakka simultaneously. The function filter_ethnic() is used to filter out subjects with specified ‘ethnicity’. This function is useful for drawing age-sex pyramid for each of the six languages. filter_ethnic \u003c- function(df, lang, lev = 3) { sp_lang \u003c- vector(\"character\", 3) sp_lang[1] \u003c- paste0(\"dad_\", lang, \"_speak\") sp_lang[2] \u003c- paste0(\"mom_\", lang, \"_speak\") sp_lang[3] \u003c- paste0(lang, \"_speak\") df2 \u003c- df %\u003e% filter(m_guard_identity != 'None' | f_guard_identity != 'None') %\u003e% filter(.data[[sp_lang[1]]] \u003e= lev | .data[[sp_lang[2]]] \u003e= lev) %\u003e% select(age, gender, sp_lang) return(df2) } Assinging Age Group to Subjects Remember that the data structure needed for plotting age-sex pyramids requires age group to be the basic unit. data now consisits of single subjects, and we need information to group subjects together according to their ages. mutate_age_group() creates a new variable age_group by the subject’s age. mutate_age_group \u003c- function(df, range = 5){ df$age_group \u003c- as.character( cut(df$age, right = F, breaks = seq(10, 95, by = range)) ) return(df) } cut() takes a numeric vector as its first input, and codes the values of the vector into new values according to the interval they fall into. In mutate_age_group(), cut() codes the input vectors according to the intervals specified in the argument breaks. Now we can use these functions to add more information to the data frame. The idea is to first create a separated data frame for each ethnicity3 (by filter_ethnic()), then attact new variables to the data frame that indicate a subject’s ethnicity (ethn_group) and age group (age_group). lang \u003c- c('Mand', 'Tw', 'Hak', 'Ind', 'SEA', 'Eng') lev \u003c- c(3, 3, 3, 3, 3, 0) ethn_list_df \u003c- vector(\"list\", length(lang)) for (i in seq_along(lang)){ ethn_list_df[[i]] \u003c- filter_ethnic(data, lang = lang[i], lev = lev[i]) %\u003e% mutate(ethn_group = lang[i]) %\u003e% mutate_age_group() %\u003e% select(age, gender, age_group, ethn_group, paste0(lang[i], \"_speak\")) %\u003e% rename(lang_fluency = paste0(lang[i], \"_speak\")) } Then we can recombine these data frames back to a single one, and this new data frame now has information about a subject’s ethinicity and age group he/she belongs to. (Note that the new data frame is expended since one subject can have several ethnicity, i.e. one subject can appear in different rows of the data frame with different ethnicity.) bind_rows(ethn_list_df) %\u003e% head() age gender age_group ethn_group lang_fluency 56 Female [55,60) Mand 4 57 Female [55,60) Mand 5 37 Male [35,40) Mand 5 44 Female [40,45) Mand 5 56 Female [55,60) Mand 5 53 Male [50,55) Mand 5 Data for Plotting Finally, we are ready to group the subjects together according to his/her gender, ethnicity, and age_group. After grouping, we can use dplyr::summarise() to calculate each group’s fluency of the language, which will be used as the variable on the horizontal axis of the age-sex pyramid. pl_data \u003c- bind_rows(ethn_list_df) %\u003e% group_by(gender, ethn_group, age_group) %\u003e% summarise(mean(lang_fluency)) %\u003e% rename(avg_fluency = `mean(lang_fluency)`) head(pl_data) gender ethn_group age_group avg_fluency Female Eng [15,20) 1.750 Female Eng [20,25) 2.950 Female Eng [25,30) 2.600 Female Eng [30,35) 3.000 Female Eng [35,40) 1.750 Female Eng [40,45) 2.125 Plotting Function We are going to draw 6 age-sex pyramids, one for each languages. So instead of writing ggplot() six times, I wrote a plotting function: library(ggplot2) pl_pyramid \u003c- function(data, title = NULL) { ggplot(data, aes(x = age_group, y = ifelse(gender == 'Male', avg_fluency, -avg_fluency), fill = gender)) + geom_bar(stat = \"identity\", width = 0.7) + scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 1), labels = abs(seq(-5, 5, 1))) + coord_flip() + scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\"), breaks = c(\"Female\", \"Male\")) + labs(x = \"Age\", y = \"Fluency\", fill = \"\", title = title) } Now we can start plotting. Let’s try Mand (Mandarin) and Hak (Hakka) first. We can use dplyr::filter() to filter out people speaking these languages: tweak \u003c- theme_bw() + theme(axis.text = element_text(size = 15), legend.justification = \"right\", legend.position = \"bottom\", legend.box = \"vertical\") pl_data %\u003e% filter(ethn_group == 'Mand') %\u003e% pl_pyramid(title = 'Mandarin') + tweak pl_data %\u003e% filter(ethn_group == 'Ind') %\u003e% pl_pyramid(title = 'Formosan Languages') + tweak Wait! It seems quite strange. The age-sex pyramid of ‘Formosan Languages’ doesn’t look like a pyramid at all! This is because there were very few subjects defined as ‘indigenous people’ in the survey. To make plots like this (with only one or two age groups) comparable to others (such as that in ‘Mandarin’), we need one more function to insert missing age groups to the data frame so that ggplot can draw empty bars for us. First, we need to find out all age groups in the data: age_groups \u003c- unique(pl_data$age_group) age_groups [1] \"[15,20)\" \"[20,25)\" \"[25,30)\" \"[30,35)\" \"[35,40)\" \"[40,45)\" \"[45,50)\" [8] \"[50,55)\" \"[55,60)\" \"[60,65)\" \"[65,70)\" Then we can write a function fill_empty_age_group(), which takes a data frame as its first argument and checks whether there are age groups missing in the data frame (using its second argument, age_group_all, as comparison). If the age group is missing, fill_empty_age_group() appends a new row, which has age_group set to the missing age group and avg_fluency set to 0 (create an empty bar), to the input data frame. fill_empty_age_group \u003c- function(df, age_group_all) { for (i in seq_along(age_group_all)) { if (!(age_group_all[i] %in% df$age_group)) { df \u003c- rbind(df, list(gender = \"Female\", ethn_group = \"doesnt_matter\", age_group = age_group_all[i], avg_fluency = 0)) } } return(df) } Now we’re ready to explore language loss in Taiwan. multiplot() is used to put multiple plots together. The source code of multiplot() is copied directly from Winston Chang’s Cookbook for R. tweak2 \u003c- theme_bw() + theme(legend.justification = \"right\", legend.position = \"bottom\", legend.box = \"vertical\") py1 \u003c- pl_data %\u003e% filter(ethn_group == 'Mand') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'Mandarin') + tweak2 py2 \u003c- pl_data %\u003e% filter(ethn_group == 'Tw') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'Taiwanese') + tweak2 py3 \u003c- pl_data %\u003e% filter(ethn_group == 'Hak') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'Hakka') + tweak2 py4 \u003c- pl_data %\u003e% filter(ethn_group == 'Ind') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'Formosan Languages') + tweak2 py5 \u003c- pl_data %\u003e% filter(ethn_group == 'SEA') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'Languages of South East Asia') + tweak2 py6 \u003c- pl_data %\u003e% filter(ethn_group == 'Eng') %\u003e% fill_empty_age_group(age_groups) %\u003e% pl_pyramid(title = 'English') + tweak2 multiplot(py1, py2, py3, py4, py5, py6, cols = 2) Language Loss in Taiwan As described in Age-Sex Pyramid of Language above, we can learn about a language’s vitality in Taiwan from the age-sex pyramids drawn above. For Formosan and South East Asian languages, there are too few data, and we can’t learn much about these languages from the plots. For Mandarin, the shape of the pyramid is rectangular, indicating the linguistic competence of Mandarin is stable across people of all ages. This is expected as Mandarin, in Taiwan, is the most prevalent language, and most people use it as the primary language in workplace and home. Pyramids of Taiwanese and Hakka have inverted trianglular shapes, indicating lower linguistic competence among yonger people. Indeed, it’s not uncommmon to see the fathers and mothers talk to their children in Mandarin but talk to their parents in Taiwanese. Taiwanese is the second prevalent language in Taiwan, but it is shrinking, particularly among young people. Hakka ranks third in prevalence. It faces similar situation to Taiwanese but the situation is even worse, since the usage of Hakka is mostly restricted to Hakka people whereas usage of Taiwanese is not restricted to particular groups of people (many indigenous and Hakka peoples speak fluent Taiwanese). English has a special status in Taiwan. It is not a native tongue to people born and raised in Taiwan, but many people know at least a little English. This is because formal education and the (awareness of) globalization lead parents to place importance on English education of their children. This also gives the age-sex pyramid of English its appearance – a triangular shape with wider bottom than top. English is the only language that is growing in Taiwan and, arguably, the language with the strongest vitality. Table 1 summarizes the discussion above. Table 1: Situations of the four most prevalent languages in Taiwan. Vitality of Language Shape of Pyramid Examples (Taiwan) Shrinking and Dying Inverted Triangle Taiwanese, Hakka Growing Triangle English Stable Rectangular Bar Mandarin The samples are not representative though, and it might only reflect the situation in Taipei (see the geographical distribution of the samples below). But I think there are still reasons to believe that other locations in Taiwan have similar phenomena. ↩ Defined by scoring 3 or above in a self-reported 6-point scale measuring the fluency of a language spoken by the subject’s parents.↩ For English, different from all other languages, the level used to determine ethnicity is 0, i.e. there is no filtering occuring, and all subjects are used. This is because, in Taiwan (and many other non-English speaking communities as well), English is not related to ethnicity and is strongly related to formal education and job requirements.↩ Last updated: 2019-04-03 ","subtitle":"Create an “Age-Sex Pyramid of Language” with ggplot2","title":"Visualizing Language Loss in Taiwan","uri":"/2019/02/17/visualize-language-loss/"},{"content":" As the R Markdown ecosystem becomes larger, users now may encounter situations where they have to make decisions on which output format of R Markdown to use. One may found none of the formats suitable – the features essential to the output document one wants may scatter across different output formats of R Markdown. Here is a real example I encountered. I wanted to create a document that: supports bookdown syntax, e.g. text references has an “Edit on GitHub” button for every chapter that links to the edit page of the source .Rmd on GitHub The two features above can be obtained easily with bookdown’s default GitBook output format, but one more feature is essential to the document I want: A document that supports tabbed sections1 So basically, I wanted bookdown::gitbook that supports tabbed sections, shown in Fig. 1. However, it’s not possible. This is a feature unique to rmarkdown::html_document, not bookdown::gitbook. Figure 1: bookdown::gitbook supports tabbed sections? This is just a fake figure. Now I have to find a way, not provided by the default output formats of R Markdown, to create a document with the above three features. My first thought was to find out how to add the tabbed sections feature to bookdown::gitbook via JavaScript, but since I’m not familiar with JS, I gave up JavaScript and decided to use the “native” R Markdown approach. I turned to bookdown::html_document2, which is based on rmarkdown::html_document (supports tabbed sections). The source repo of my document is a bookdown project, which has several .Rmd files. Each .Rmd file starts with a level-one heading and defines a single chapter. Since I wanted tabbed sections from rmarkdown::html_document, I have to use bookdown::html_document2 as the output format, which creates a single HTML output file. Adding to Level-one Headings Setting bookdown::html_document2 as the output format creates a single document output (as opposed to bookdown::gitbook which creates several HTML files by default), so if I want to add an “Edit on GitHub” button for every chapter, I have to track the original .Rmd that generates the particular chapter. “Merge and Knit” vs. “Knit and Merge” There are two rendering approaches in bookdown. The default is Merge and Knit, which combines all source .Rmd files into one single .Rmd file then knits the document. In this case, it would be impossible to track the source .Rmd file for each chapter (unless I create a lookup table manually). I can track the source .Rmd files easily, however, if the document is rendered using the Knit and Merge approach. When using the Knit and Merge approach, the code chunks in the source .Rmd files are run and the results embedded before the documents get combined together in a single output file. This mean that I can retrieve the source .Rmd file name while knitting the file (by knitr::current_input()). This gives me all I need to create the link to the edit page of the .Rmd source file on GitHub. Setting up: _bookdown.yml To switch from the default “Merge and Knit” to “Knit and Merge”, set new_session: yes in _bookdown.yml: new_session: yes before_chapter_script: 'addons/pre_chap.R' Setting up: addons/pre_chap.R To insert the link to the edit page on GitHub, put `r edit_btn` (inline R code) at the end of the h1 heading of each .Rmd file, for example: # Function Factories `r edit_btn` edit_btn is a string variable holding the link2 to GitHub. It is computed in the R script addons/pre_chap.R, which is run every time before knitting a .Rmd file: url \u003c- 'https://github.com/liao961120/parallelCode/edit/master/' gh_edit_path \u003c- paste0(url, knitr::current_input()) edit_btn \u003c- paste0('', '","subtitle":"","title":"Inserting “Edit on GitHub” Buttons in a Single R Markdown Document","uri":"/2019/02/10/rmd_edit_btn/"},{"content":"R-bloggers.com is a great platform for R users, but I sometimes feel awkward to publish posts on R-bloggers when I have things to share that are only relevant to users in Taiwan1. Inspired by R-bloggers, I thought maybe I could use Travis-CI and GitHub to create a blog that automatically updates its posts by retrieving them from submitted RSS feeds, just like R-bloggers.\nR-bloggers for Taiwan The name of the blog I created is “R部落客”, which is R-bloggers literally in traditional Chinese.\nThe blog is served using GitHub Pages and is hosted under the organization, Rbloggers, on GitHub. To make the platform work, there are three repositories created – RSSparser, Rbloggers.github.io, and facebook-publish, all integrated with Travis-CI. I set up a daily cron job to run on RSSparser repo, which generates JSON files to be used for creating new posts. After finishing the build, Travis-CI pushes the JSON files to gh-pages branch of RSSparser and triggers a build (using Travis-CI API) to run on Rbloggers.github.io repo. The build on Rbloggers.github.io then writes new posts by retrieving the JSON files saved on branch gh-pages of RSSparser. Another build is also triggered to run on facebook-publish by the build on Rbloggers.github.io, which shares the new posts created in Rbloggers.github.io on Facebook (using Facebook API).\nI learned about the capabilities of Travis-CI in the bookdown book and the blogdown book. Although not directly related to R, I think most R users will benefit a lot by if they know how to use Travis-CI.\nLooking for R Bloggers \u0026 Users from Taiwan Currently, R部落客 is in its infancy and needs support. We are looking for R users and bloggers who read and write in traditional Chinese.\nIf you are an R user, you can follow R部落客 via Facebook fan page.\nIf you write blog posts about R in traditional Chinese, I believe R部落客 will be a great platform to advertise your work. You can read more about joining R部落客 here.\nSuch as a new R package about 批踢踢, how to painlessly knit Chinese R Markdown documents to PDF, and how to perform Chinese word segmentation in R etc. ↩︎\n","subtitle":"","title":"Using Travis-CI to Create R-bloggers for Taiwan","uri":"/2019/01/30/recreate-rbloggers/"},{"content":" 從第一次接觸 R Markdown 至今經過了一年又三個月。而最近六個多月，我都沒有使用過 Microsoft Word 寫東西，因為 R Markdown 強大的功能使我得以不用 Word 也能生存 – 我用 R Markdown 寫作業、文章、書、論文和部落格 (本文)，製作筆記、投影片、甚至網頁。事實上，這正是 R Markdown 最吸引人的地方：\n用相同的語法撰寫各式類型的文件。\n兩種媒介：紙本 vs. 網頁 寫作的目的有一部分是要給人看，因此排版自然而然就成為作者的工作之一。在文字普及後的絕大多數時間，排版僅需要考慮紙本印刷物。即使十幾年前電腦已十分盛行，排版軟體仍主要在解決紙本印刷物的排版問題。這本身不是個問題，但今日網路已成為主流的資訊傳播管道，網頁因而成為與紙本同等 (甚至更加) 重要的文字承載媒介。\n這對文字工作者2造成了新的負擔，因為兩種不同的文字承載媒介 – 紙本與網頁，在歷史上各自衍生出兩種設計邏輯十分迥異的排版系統， \\(\\TeX\\) 以及 HTML，而這兩種系統彼此是無法 (輕易) 相互轉換的 (例如，嘗試使用 Chrome 將網頁轉存成 PDF 時，你會發現一張圖片常被分割在不同頁面)。因此，今日的文字工作者在寫作前得先回答一個問題：\n我的作品會活在網路上還是要印出來？\n兩種工具：WYSIWYG vs. WYSIWYM 上一段內容或許對許多人來說相當陌生，因為我們都是用 Microsoft Word 長大的，根本不知道什麼是 \\(\\TeX\\)。MS Word 和 \\(\\TeX\\) 一樣，皆是以紙本排版為目的而設計，但 MS Word 不同於 \\(\\TeX\\) 在於其為所見即所得 (What You See Is What You Get, WYSIWYG) 的排版軟體，讓使用者不須接觸到複雜的標記式語言 (Markup Language)3。但簡潔的使用界面需付出代價 – 複雜的檔案格式4，因而較難進行檔案管理5。\nFigure 1: \\(\\TeX\\) 排版系統 相較於所見即所得的排版軟體，所見即所思 (What You See Is What You Mean, WYSIWYM) 的系統如 \\(\\TeX\\) 和 HTML 通常以純文字檔的形式儲存，再以編譯器輸出或以閱讀軟體 (瀏覽器即是 HTML 的閱讀軟體) 讓讀者看到最終的排版結果。這類系統少了 Word 難以管理的缺點，但卻有一個令許多使用者卻步的大缺點 – 太過複雜的標記式語言讓作者難以專注在寫作的內容，必須時時擔心排版問題。\nFigure 2: 所見即所得 (WYSIWYG) vs. 所見即所思 (WYSIWYM) Markdown：改善 WYSIWYM 面對過於複雜的標記式語言，John Gruber 發明了 Markdown 以避開撰寫網頁時過於複雜的 HTML 語法。Markdown 是一種容易學習、極為簡化的標記式語言。事實上，由於原始的 Markdown 語法過於簡單6，造成許多衍生「風格」Markdown 的發明以擴充原本不足的語法。這也間接導致今日 Markdown 並非可通行 (portable) 於所有聲稱支援 Markdown 的平台。\nFigure 3: Markdown 轉換成 HTML Markdown 的出現使得所見即所思的排版方式不再如此地惡名昭彰，因為 Markdown 簡潔的語法使其極易閱讀(比較圖 3 最左與最右)。原本為了處理網頁排版而設計的 Markdown，也將目標擴展到了紙本媒介 (將 Markdown 轉換成 \\(\\TeX\\) 再輸出成 PDF)，這大大增強了使用 Markdown 撰寫文件的動機：同一份文件能選擇輸出成適合「網頁瀏覽」或「紙本列印」的格式。\n總結來說，Markdown 的出現使過去排版系統的問題出現了改善的契機：\n簡化複雜的標記式語言，讓 WYSIWYM 的排版方式變得平易近人 使用相同原始檔 (source file) 輸出適合網頁及印刷的格式 所見即所得 所見即所思 紙本 MS Word \\(\\TeX\\),\nMarkdown (近年目標) 網頁 部落格文章撰寫後台\n(e.g. WordPress, Medium) HTML,\nMarkdown (初創目的) R Markdown：R + Markdown R Markdown，顧名思義就是 R 語言 + Markdown。R 是一個統計計算的程式語言。你可能會好奇結合程式語言和 Markdown 的用途為何？R Markdown 最初發展的目的，有一部份與 R 語言強大的繪圖功能有關：\n透過在 Markdown 文件中穿插「程式語言區塊 (code chunk) 」，可以讓程式語言執行的結果直接顯示於文件中。如此就不須手動將 (統計軟體跑出來的) 圖片插入文件中。\n下方的是個簡單的例子，用 3 行 R 指令畫出著名的鳶尾花數據集：\nlibrary(ggplot2) ggplot(data = iris) + geom_point(aes(x = Sepal.Length, y = Petal.Width, color = Species) ) 這項功能對從事數據分析或科學研究的使用者非常實用，因為它自動化了麻煩的事情。\n但對於非 R 語言使用者，使用 R Markdown 有比較好嗎 (相對 Markdown 而言)？\nR 社群 一個程式語言 (包含套件擴充功能) 的發展與特色，很大部份與使用族群之組成相關。不同於其它多數程式語言，使用 R 語言的人，絕大多數都屬於使用者 (user) 而非開發者 (developer)。換言之，許多 R 使用者並沒有深厚的程式基礎，使用 R 的目的是為了 (快速) 解決當下問題，而非開發工具給其他人使用。\nR 社群因此發展出非常友善的文化，而 R 許多套件的設計也預設使用者是沒有程式經驗的。例如，許多 R Markdown 相關套件 (e.g. shiny, htmlwidgets) 目的在於幫助不熟悉或完全不懂 HTML/CSS/JS 的使用者快速製作 (互動式) 網頁。此外，R 套件常將外部功能整合進 R 的世界，並且提供簡單易讀的說明文件，讓使用者省去自行研究的麻煩 – R Markdown 正是這樣的例子。\nPandoc R Markdown 背後運作的關鍵是 Pandoc，一個用於不同標記式語言間格式轉換的工具。如上文所述，Markdown 原本被用作簡化的標記式語言來撰寫網頁，而 Pandoc 則是用來將 Markdown 轉換成 HTML 的 (其中一種) 工具。然而，Pandoc 不只能將 Markdown 轉換成 HTML。它也能將 Markdown 轉換成其它多種格式，而其中最實用的就是前述以紙本媒介為目標的 \\(\\TeX\\) 及 PDF。\n透過 Pandoc 的加持，R Markdown 能輸出成多種格式。此外，也因為 Pandoc Markdown7 的語法完整，R Markdown 能進行非常精緻的排版 (精緻到能輸出可直接送印出版的書籍)。但 Pandoc 強大的功能卻反而讓它與沒有程式經驗的使用者絕緣，因為這些使用者不可能會去研究、下載、打開 Command Line 執行 Pandoc。R Markdown 將對於新手過於複雜的 Pandoc 隱藏起來，讓使用者透過 GUI 界面就能直覺地使用 Pandoc 的功能。\nFigure 4: R Markdown 運作流程。先由 knitr 將 .Rmd 中的程式碼執行結果插入 .md (文件由 .Rmd 轉換為 .md)，再透過 Pandoc 將 .md 轉換成其它輸出格式。 回到 R Markdown 對於非 R 語言使用者而言，R Markdown 有比 Markdown 好嗎？\n我的答案是肯定的。R 社群的組成使得 R Markdown 致力於讓自己更易於使用。因此，縱使使用目的並非資料分析或科學研究這類「硬」功能，使用者仍能從 R Markdown 獲得許多好處：\n美觀的預設樣式\nMarkdown 轉換成 HTML 後能在網頁上正常顯示，但要漂亮的顯示，需要額外的 CSS 裝飾。這對一般使用者來說是非常困難的 (誰懂 CSS 啊？)。R Markdown 內建許多相當美觀的輸出樣式 (Bootstrap)，所以使用者不須去擔心 CSS 的問題。甚至對於更龜毛的使用者 (對所有內建樣式都不滿)，也可以安裝社群中其他人開發的樣式。例如，你可以在這裡看到本文透過 rmdformats 套件以 Read The Docs 風格輸出的樣式。\n統一的 Markdown 語法\n文章前面提及，原始的 Markdown 因為語法過於簡化，導致各種「風格」 Markdown 語法的出現。例如，GitHub, Stack Overflow, Jekyll (靜態網頁產生器), Hugo (靜態網頁產生器) 等等所支援的 Markdown 在語法上都有些微差異。換句話說，使用者無法使用相同的 Markdown 語法橫行各個平台，需要記得不同平台支援哪些語法8。\n這個問題對於自行架設部落格的使用者可能是個問題，因為當其想換新的網頁模板時，可能使用不同的靜態網頁產生器 (例如，Jekyll → Hugo)，而原本以 Markdown 撰寫的文章在新的靜態網頁產生器可能會語法不相容。R Markdown 在此可以解決這個問題：透過 html_fragment 輸出格式，靜態網頁產生器能透過 R Markdown 輸出的 HTML，而不非語法不一的 Markdown，產生網頁。\n程式語言支持\nR Markdown 的另一個特色是程式語言的整合。即使不會寫程式，背後有程式語言支持的 R Markdown 仍能提供使用者非常方便的功能。R Markdown 支援使用特殊語法在文件內文插入變項，例如，我可以在文內插入 R 「回傳目前時間的函數」，自動顯示文件輸出時的時間：\n上次更新：`r Sys.time()` 會輸出成：上次更新：2021-09-16 15:38:23\n這個功能對於從事數據分析的使用者 (e.g. 寫統計學作業的學生) 非常方便。例如，他可以使用 p-value = `r pval` (pval 儲存先前計算出來的數值) 直接在內文插入 p-value，而不用在每次數據更新時 (例如發現之前資料有誤)，重新手動複製貼上數值。\n強大的靠山\nR 社群今日能如此活躍，有很大一部分是因為 RStudio 這家公司的推行。R 使用者用到的許多套件是由 RStudio 僱用的工程師專職開發的，因此，這些套件會持續的維護、更新、出現更強大的新功能。\nR Markdown 的生態圈最主要的貢獻者 – Yihui Xie 即是 RStudio 的工程師。在這些開發者的努力下，R Markdown 在這幾年功能越來越強大：涵蓋的範圍從簡單的文件、投影片、互動式網頁、學術期刊、書籍甚至到網站與部落格 (Xie, Allaire, and Grolemund 2018)。\nR Markdown 的未來 以紙本印刷為主流的過去在今日留下許多痕跡。學術論文格式和印刷物排版的處理仍以 \\(\\LaTeX\\) 和 \\(\\TeX\\) 的功能最為齊全，但以 \\(\\LaTeX\\) 撰寫而成的文件難以轉換為適合網頁閱讀的格式。至於另一個方向，以 HTML/CSS/JS 為基礎排版的網頁，有機會轉換成排版美觀、適合印刷的 PDF 格式嗎？\n以目前資訊科技的發展來看，後者是比較可能的，畢竟網路科技是程式軟體開發的重要焦點，以網頁為媒介的文件只會越來越多；相對來說，\\(\\LaTeX\\) 可能只剩學術界在使用，學術界本身更是缺乏人力去做這種吃力不討好的工作 (開發程式將 \\(\\LaTeX\\) 轉換成 HTML)。\n事實上，目前已有實際的專案在嘗試這件事。例如，網頁的 CSS 可定義 page media，嘗試控制瀏覽器如何生成 PDF。另一個更有前景的專案是 R Markdown 生態圈裡的新套件 – pagedown。pagedown 旨在直接生成以印刷為目的的 HTML 文件，而略過 R Markdown 透過 Pandoc 及 \\(\\LaTeX\\) 生成 PDF 的歷程9。你可以在這裡看看本文以 pagedown 輸出的樣式。我完全不需更動文章內容，只要使用不同的輸出指令，就能從部落格文章瞬間轉換成漂亮的紙本印刷物 (可用 chrome 列印 → 儲存成 PDF)。\n小結 因為歷史因素，現存兩種文字承載媒介 – 紙本和網頁。對多數人來說，必需在使用紙本或使用網頁發表之間抉擇。\n排版軟體的兩種設計邏輯 – WYSIWYG vs. WYSIWYM，各自存在優缺點。WYSIWYG 使用起來直覺，但檔案管理不便；WYSIWYM 則語法複雜，但能作出十分精美之排版且檔案管理方便。\nMarkdown 結合了過去 WYSIWYG 和 WYSIWYM 的優點，也解決了兩者的缺點。同時，Markdown 也有潛力解決紙本發表與網頁發表之間的矛盾。\nR Markdown 擴充 Markdown 語法使其能撰寫相當複雜的文件 (如期刊文章、書籍、網站等) 並結合程式語言的支持，自動化許多麻煩事，讓使用者能快速上手使用。\n面對紙本 vs. 網頁，未來似乎向著以網頁為文字承載媒介主流的世界前進。而資訊科技的發展，使得紙本印刷之排版困難逐漸能被網頁技術解決。\nI have a dream that one day all students and researchers will forget what “formatting a paper” even means. I have a dream that one day journals and grad schools no longer have style guides. I have a dream that one day no missing $ is inserted, and \\hbox is never overfull.\n— Yihui Xie10\n這段文字是 Yihui 的夢想，我想也是 R Markdown 的終極目標：讓文字工作者能專注在寫作內容，而將格式與排版等麻煩事交給 R Markdown。\n參考資料 這篇文章的想法源自下列資源。\nGinev, Deyan. 2017. “LaTeX Is Dead (Long Live LaTeX) Typesetting in the Digital Age.” Authorea. December 18, 2017. https://www.authorea.com/users/5713/articles/19359-latex-is-dead-long-live-latex-typesetting-in-the-digital-age/_show_article. Peng, Roger. 2018. “Teaching R to New Users - From Tapply to the Tidyverse \\(\\cdot\\) Simply Statistics.” Simply Statistics. July 12, 2018. https://simplystatistics.org/2018/07/12/use-r-keynote-2018/. Xie, Yihui. 2018a. “The User-Developer Spectrum in the R Ecosystem.” Yihui Xie | 谢益辉. July 13, 2018. https://yihui.name/en/2018/07/user-developer/. ———. 2018b. “Write a Book with Bookdown and Publish with Chapman \u0026 Hall.” Yihui Xie | 谢益辉. August 6, 2018. https://yihui.name/en/2018/08/bookdown-crc/. Xie, Yihui, JJ Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. CRC Press. 關於 R Markdown 的介紹有許多網路資源，繁體中文可參考此文 (http://www.learn-r-the-easy-way.tw/chapters/17)，英文資源可先從 bookdown (https://bookdown.org/yihui/bookdown) 下手。↩︎\n這裡對文字工作者的定義很廣，任何常寫文章的人都可算是文字工作者 (例如，學生) 。↩︎\n排版是一件複雜的問題。試想，電腦要如何知道哪些字需要粗體、斜體、空行、縮排等等？標記式語言處理的就是這類問題：透過特殊字元標記出文字中需要特殊格式的地方。↩︎\nWord 檔 (.docx) 不是純文字檔案，只能用 Word 等複雜的軟體才能閱讀編輯，更不能直接於網頁中顯示。↩︎\n舉例來說，假設我的資料夾中有 200 份 Word 檔，我要如何找出其中哪幾份文件有出現特定字元，例如，「王小明」？\n相較之下，如果這 200 份文件是純文字檔，要找出哪幾份文件中有出現「王小明」就非常簡單。在類 unix 的電腦上(如 Mac 或 Linux)，只需要打開 Command Line 輸入 grep -r '王小明' *.Rmd 就可找到這些檔案 (在 Windows 上也行，但指令可能不太一樣)。↩︎\nMarkdown 的語法雖然簡單，但這並不代表其排版能力弱於 HTML，因為 Markdown 語法可與 HTML 混用。↩︎\nPandoc 自行定義的 Markdown 語法，是市面上盛行的其中一種 Markdown「風格」。Pandoc Markdown 的特色在於其支援功能非常完整，例如，它除了大幅擴充原本的 Markdown 語法外，甚至還增加了插入文獻引用的功能 (如同 Endnote，自動處理文獻引用格式)。↩︎\n其中一種解決方法是只用最簡化的 Markdown。如果還需更複雜的排版，就使用 HTML tag。↩︎\n這是一個困難重重的過程，看見 \\(\\LaTeX\\) 噴錯是一件驚心動魄的事。↩︎\nhttps://twitter.com/xieyihui/status/1022873179532996609↩︎\n","subtitle":"Why R Markdown?","title":"網路時代下的寫作","uri":"/2019/01/22/write-in-rmd/"},{"content":"在似乎完成了很多但又似乎沒達成什麼的感覺中，大學生活逐漸步入尾聲。回顧這幾年學到的東西，沒幾個可以拿出來說嘴，更別說是對每個人都有用的能力或技巧。不過有些小技巧似乎過於「微不足道」，反而沒人意識到這些技巧也是花時間摸索出來的。\n沒有指引之下的學習需要經過不少試誤、耗費許多時間精力。這篇文章將記錄這三年多來學會的一些「微不足道」的小技巧。再不寫下來，我或許會把這些事情視為理所當然，忘記這背後是多少經驗的累積。\n影印書籍 大一時就開始去影印店印一些課本1，當時什麼都不知道，只是將 PDF 檔交給影印店。隨著印過越來越多書才逐漸抓住印書時該注意的一些小地方：\n減輕重量：\n書要印成 B5，A4 太大過重 書的頁數不要超過 500 頁，若超過可分成兩個 PDF 檔印 頁數增減：\nPreface, Acknowledgment 可以刪掉，但 References 和 Index 不建議刪掉 如果 References 和 Index 很多頁，可以將兩面合併為一面 如果一本書分成兩部份印，最好兩本都保留 Contents, References 和 Index。亦即，盡量讓每部份的書 self-contained 美觀：\n注意奇偶數頁碼：\n如果你曾看過書的 PDF 檔，或許曾注意到有些頁面會刻意留白。這些空白頁是為了確保奇數頁碼的頁面永遠在(翻開的)書的右邊，偶數頁碼的頁面永遠在書的左邊。此外，新章節(Contents, Chapter x, References 和 Index 等都算章節)通常以奇數頁開頭2。\n若頁碼是在頁面的正中間下方(而非角落)，那通常就沒有上述的特徵。\n在增減書的頁面時(特別是刪除 Preface 和 Acknowledgment 頁面)，有時需在新章節開始前插入或刪除空白頁面，以維持奇數頁碼的頁面永遠在(翻開的)書的右邊，偶數頁碼的頁面永遠在書的左邊。\n裁切 PDF：\nPDF 的頁邊空白過大或有不重要的字串時，可以使用 briss 之類的軟體一次裁切所有的頁面。 如果喜歡，可以自製封面\n請老闆印書背\n如果是台大學生或住公館附近，推薦遠雄影印\n寫卡片 儒家文化下的讀書人要精通琴棋書畫，但我從未看過我們這代精通四藝3的人，我自己更是一項都不行。四藝當中，我認為最重要的是「書」。儘管平時並不如何在意自己字醜，但要寫卡片時(尤其是重要的卡片)，才發現字醜真的很要命。\n對於許多字寫不漂亮的人(至少我是這樣)，很大一部分原因是無法掌握每個字所需佔據的空間。我在寫卡片時，需先在卡片上用鉛筆打草稿，而這也是最頭痛的地方。首先，需小心翼翼的輕描免得留下印痕；寫完之後需將鉛筆筆跡擦掉，因此常連帶將未乾的原子筆墨水拓開而毀掉卡片。為了解決這些問題，我決定不在卡片上打草稿，而是用一張跟卡片一樣大的白紙當草稿寫過一次，然後在卡片上用鉛筆輕點定位出每個字的位置。這真的浪費很多時間，因為\n需要先測量卡片的大小，剪出符合此大小的白紙，並在這張白紙上：\n決定字的大小，然後計算、點出一行中每個字的位置 決定行距，然後標出每行的位置 寫完草稿 在卡片上點出每個字的位置後，再將字寫上去\n當我第一次這樣寫完卡片後，我忽然意識到第 1 點一定有工具能代替手工。於是，我找到了一個完全符合需求的線上方格紙產生器：\n這個方格紙產生器可以自訂字的間距 (Grid Spacing) 以及紙張和頁邊的大小。因此有了這個工具後，就可以在量測完卡片的大小後，輸入數值做出草稿用紙。\n上台報告(投影片) 上台報告對我來說是個很大的挑戰，因為我會緊張到忘記本來要講演的邏輯。克服這挑戰的關鍵當然是練習 – 寫下報告的講稿反覆練習，直到能自然(自動化)地完成報告。但除了練習之外，我過去一直忽略工具的重要性。事實上，有了正確的工具後，根本不需擔心會忘記要講什麼。\n幾乎所有投影片工具 (PowerPoint, Google Slides, reveal.js, Remark.js) 都有簡報者模式 (presenter mode)：\nGoogle 簡報者模式。左邊是簡報者看到的內容，右邊是觀眾看到的同步內容。 Remark.js 簡報者模式。左邊是簡報者看到的內容，右邊是觀眾看到的同步內容。 但我知道有人因為不知道簡報者模式需配合延伸螢幕使用而未曾使用這個功能(對，就是我！)。透過延伸螢幕，可以在自己的電腦與投影幕上同步顯示投影片，只是其中一個是有筆記的簡報者模式，另一個則是給觀眾看的一般模式。\n電子辭典 在電腦上看英文內容遇到不懂的單字時，往往要開啟瀏覽器去查單字。若有很多單字不懂，就要反覆切換視窗畫面。Mac 內建有辭典工具，透過反白文字就可在小視窗中查詢該單字，並可以安裝第三方字典檔擴充其詞彙。\nWindows 和 Linux 使用者沒有這麼方便的內建辭典功能，但透過 GoldenDict 可以達到一樣的效果。GoldenDict 是一個開源的字典軟體，本身不含字典檔，但支援多種字典檔案格式，因此可以安裝多個字典檔擴充其詞彙。\n使用方式：\n將字反白後再按快捷鍵 (可自訂)，就會跳出小視窗顯示該字的查詢結果 GoldenDict，開源電子辭典軟體。支援多種字典檔案格式，但本身不含字典檔，需額外下載。 記錄想法 沒有記憶就彷彿不曾存在 。生活在資訊量爆炸的時代，「記住重要的事情，忽略不重要的刺激」似乎變成一項重要的能力。如果我們不努力記下重要的事情，那這些記憶就會被其它瑣碎的、不重要的刺激4洗掉。\n面對海量的資訊，網路上有許多專案管理軟體可以幫忙記下事情。我自己習慣使用 Trello，因為覺得它使用起來相當順手(筆記可用 Markdown 語法、Board, List, Card 的階層分類方式)。例如，我用一個 Board 記下未來可能會用到的線上資源。除此之外，目前還有其它十幾個 Board，各自管理一個主題。\n用什麼專案管理軟體當然不是重點，最重要的是，在遇見重要想法時能立刻記錄下來。生活在這奇怪的時代 (資訊科技可以幫助人們過得更好，但也可以把人們害得很慘)，學習善用資訊科技的產物變得非常重要。\n對於某些科系，像是資訊相關科系，可能更有機會印書，因為許多書都可取得開放的 PDF 檔。 ↩︎\n這裡是以橫式書寫，例如英文書。LaTeX 可以在 document class 中設定 oneside 或 twoside 來決定是否要有這項特徵。 ↩︎\n現代版的四藝也行。我可以想到的是：吉他、麻將(?)、鋼筆字、向量繪圖。 ↩︎\n通訊軟體訊息、社群網路上自動播放的影片、廢文…族繁不及備載。 ↩︎\n","subtitle":"","title":"大四老屁股的一些小技巧","uri":"/2019/01/04/trivial-skills/"},{"content":"Blogdown makes it easy to create Hugo blogs or personal websites, and it is becoming more and more popular in the R community. Once the blog is created, people might want to submit their blogs’ RSS feeds to R-bloggers. But before that can happen, one must modify the RSS template to meet the requirements of RSS submission. Due to my successful experience in creating a new Jekyll RSS template for my blog, I thought it would be easy to customize the RSS template of Hugo blogs to make it suitable for R-bloggers. I was WRONG. Hugo has stricter rules for modifying RSS templates1, and it took me a while to figure out how to modify the category/tag RSS templates. By default, Hugo generates different RSS feeds for each section and taxonomy2. I will write about how to modify a subset of them to create category- or tag-specific RSS feeds for R-bloggers, i.e, making the field of the RSS feed display full content, rather than an excerpt, of a post. Overwriting the default RSS Template You won’t find any RSS template files shipped with Hugo themes (at least Hugo Acadimic theme doesn’t). RSS files (index.xml) will be generated (in public/) according to the default RSS template. To override the default RSS template without touching the theme template, Copy the default RSS template Change {% raw %}{{ .Summary | html }}{% endraw %} to {% raw %}{{ .Content | html }}{% endraw %} Create subdirectories in layouts/ and save the RSS template files (name them rss.xml) in them (one for each subdirectory). Putting RSS templates in different subdirectories will have effects on different kind of RSS feeds. The file name (e.g. rss.xml) of the template also matters and can’t be arbitrary. Read Hugo’s documentation on Lookup Order for RSS Templates for details3. An Example: Hugo Acadimic Theme I use Hugo Acadimic theme as an example to set two kinds of RSS templates: A template that causes RSS feeds under every category to display full content of the posts Browse RSS feed GitHub source: layouts/categories/rss.xml A template with fields to capture all post tags Browse RSS feed GitHub source: layouts/post/rss.xml Directory Tree Below is the directory structure (simplified) after adding RSS templates to Hugo Acadimic theme. I added and modified rss.xml in /layouts/post/ and /layouts/categories/. Then /docs/post/index.xml and /docs/categories/r/index.xml get generated according to /layouts/post/rss.xml and /layouts/categories/rss.xml, respectively. Note that /docs is /public by default, when publishDir = \"docs\" is not set in /config.toml. Using /docs allows me to publish Hugo blog on GitHub Pages. DO NOT add publishDir = \"docs\" in config.toml if you’re serving the site with netlify. 1/ 2├── index.Rmd 3├── Hugo-RSS.Rproj 4├── config.toml 5├── content/ 6│ ├── home/ 7│ └── post/ 8│ ├── 2015-07-23-r-rmarkdown.html 9│ └── 2015-07-23-r-rmarkdown.Rmd 10│ 11├── layouts/ 12│ ├── post/ 13│ │ └── rss.xml # Add/Modify RSS template here 14│ └── categories/ 15│ └── rss.xml # Add/Modify RSS template here 16│ 17├── docs/ # change 'public' to 'docs' in /config.toml 18│ ├── index.html 19│ ├── index.xml 20│ ├── home/ 21│ ├── post/ 22│ │ ├── index.xml # set in '/layouts/post/rss.xml' 23│ │ ├── index.html 24│ │ ├── 2015-07-23-r-rmarkdown/ 25│ │ └── 2015-07-23-r-rmarkdown_files/ 26│ └── categories/ 27│ └── r/ 28│ ├── index.xml # set in '/layouts/categories/rss.xml' 29│ └── index.html 30└── themes/ 31 └── hugo-academic/ Source Code You can check out the complete directory at GitHub and the web page generated from this repo. For Jekyll blogs, creating a new RSS feed is just like creating a new page in the blog, and one can even use custom file names for RSS template files (such as feed.rbloggers.xml). ↩︎ See https://gohugo.io/templates/rss/ for more information. ↩︎ I have to admit that I don’t completely understand Hugo’s RSS template look up order. I can’t predict the behaviors of Hugo precisely. ↩︎ ","subtitle":"","title":"Customizing Hugo / Blogdown RSS Templates","uri":"/2018/12/13/hugo_rss/"},{"content":" I was thinking about creating a glossary in bookdown and found out that there was already an issue about it. I like Yihui’s recommendation: use Pandoc’s definition lists. This was exactly what I had been doing, but I quickly found out that there was a major drawback – the definition lists won’t order alphabetically unless written in that way. So I wrote an R function to reorder the definition lists written in R Markdown. Note that this functions only works for R Markdown files containing defintion lists exclusively. If the R Markdown files aren’t whole-definition-lists, the function will fail. Usage To order the definition lists alphabetically, simply put the Rmd file path in the function. To have a different output file, provide the output file path as the second argument. sort_def_list(\"glossary.Rmd\") # sort_def_list(\"glossary.Rmd\", \"reordered.Rmd\") The output in PDF looks like this (I used the multicol package)1: Source Code sort_def_list \u003c- function(in_file, out_file = NULL) { library(stringr) library(dplyr) data \u003c- readLines(in_file) # Extract, remove yaml header yaml \u003c- which(data == \"---\") head \u003c- c(data[yaml[1]:yaml[2]], \"\\n\") data \u003c- data[(yaml[2]+1):length(data)] # Indexing lines def_start \u003c- which(stringr::str_detect(data, \"^: \")) - 1 def_end \u003c- c(def_start[2:length(def_start)] - 1, length(data)) def_ranges \u003c- dplyr::data_frame(term = data[def_start], start = def_start, end = def_end) %\u003e% dplyr::arrange(term) %\u003e% dplyr::mutate(new_start = cumsum( c(1, (end-start+1)[-length(term)]) ) ) %\u003e% dplyr::mutate(new_end = new_start + (end-start)) # Create ordered definition list data2 \u003c- rep(NA, length(data)) for (i in seq_along(def_ranges$term)) { start \u003c- def_ranges$start[i] end \u003c- def_ranges$end[i] n_start \u003c- def_ranges$new_start[i] n_end \u003c- def_ranges$new_end[i] data2[n_start:n_end] \u003c- data[start:end] } # Rewrite rmd if (is.null(out_file)) out_file \u003c- in_file data2 \u003c- c(head, data2[!is.na(data2)]) writeLines(paste(data2, collapse = \"\\n\"), out_file) } To see the source R Markdown file, visit glossary.rmd. To see the output PDF, visit glossary.pdf↩︎ Visit R-bloggers Last updated: 2020-02-13 ","subtitle":"","title":"Create a Glossary in R Markdown","uri":"/2018/10/24/glossary-maker/"},{"content":" I’ve written a post about rendering IPA symbols properly regardless of the output format of the R Markdown document. I implemented the ideas into an R package, linguisticsdown. linguisticsdown provides a Shiny interface to facilitate inserting IPA symbols in R Markdown. See a quick demo of the current feature of linguisticsdown in the gif at the end of the post.\nA live demo is hosted on shinyapps.io. For more details, visit linguisticsdown.\nLast updated: 2019-03-07 ","subtitle":"","title":"Easy Linguistics Document Writing with R Markdown","uri":"/2018/09/09/linguistics-down/"},{"content":" I was thinking about promoting reproducible research in Linguistics, or more precisely, how to attract people with no programming skills to have incentives to learn at least a bit programming, so that they have the ability to make their research more reproducible. I arrived at the solution: start by adopting R Markdown to write articles (see the last section for details), but making R Markdown more friendly to novices in a particular field of academia is crucial to enhance their incentives to learn programming. Tasks Specific to Linguistics I came out with some common tasks related to document writing in Linguistics (I will thank you if you tell me other tasks I missed): Typing IPA symbols. Drawing syntax trees. To enhance R Markdown’s ability to do these tasks without compromising one of it’s great feature: render nicely to both HTML and PDF with the same source, one needs to consider the incompatiblity of LaTeX and HTML code. Solving the first problem (IPA symbol) is easy, draing syntax trees is hard and I haven’t have a solution yet1. Typing IPA Symbols There are two problems to be solved in order to facilitate using IPA symbols in R Markdown: Input method Font support (only related to PDF output) The first one is essentially about mapping some combination of keys to unicode strings. This post demenstates how to solve the second, which is more fundamental. After doing a little research, I came out with a quick solution which stems from the combination of IPA Symbols in R, How do I use a particular font for a small section of text in my document?, and Conditional compilation of book chunks to ensure compatibility with both HTML and XeLaTeX. The solution is very simple: define a new font family that supports IPA symbols in LaTeX and use conditional compilation to render the document: when compiled to HTML, use raw unicode string; when compiled to PDF, wrap LaTeX code around IPA unicode strings. To define a new font family for IPA symbols, set header.tex and include it by setting the yaml header of R Markdown document: output: bookdown::pdf_document2: includes: in_header: header.tex Here’s header.tex: % Set font size \\usepackage[fontsize=12pt]{scrextend} % Set font family \\usepackage{xeCJK} \\usepackage{fontspec} \\setmainfont{Calibri} \\setCJKmainfont[ BoldFont={HanWangHeiHeavy} ]{HanWangHeiLight} % IPA font \\newfontfamily\\ipa{Doulos SIL} \\DeclareTextFontCommand{\\ipatext}{\\ipa} The font, Doulos SIL, which supports IPA symbols can be freely dowloaded. The code chunk below is for conditional compilation: ipa \u003c- c('e\\u026A', 'a\\u026A', '\\u0254\\u026A') if (knitr::opts_knit$get('rmarkdown.pandoc.to') == \"latex\") { ipa \u003c- paste0(\"\\\\ipatext{\", ipa, \"}\") } The IPA symbols are set in the variable ipa and can be access inline in R Markdown with, e.g., r ipa or r ipa[3], which renders to eɪ, aɪ, ɔɪ and ɔɪ, respectively. The source of this post is in my GitHub repo. You can reproduce it locally to see the differnce between HTML and PDF output of this post. Obstacles to Adopting a Reproducible Workflow Skip this section if you’re tired of stuff about reproducibility and R Markdown. Reproducible research not only enhance scientific progress but also saves researchers a great deal of time, by automating repetitive and error-prone tasks in research. So if there are good reasons to adopt a reproducible workflow in research, saving time (in the long run) might be a good one. Programming skill is fundamental to automating repetitive tasks, which saves one’s time. However, learning programming to save time makes no sense to many people, since it is terrifying, hard, and time consumming2. So the problem now becomes: How to reinforce the incentive to learn programming? Again, by showing people how to save time, but this time, programming skill is not required. I think R Markdown is a very promising starting point, since writing is necessary for researcheres, and one can use RStudio without any knowledge of R. When becoming familiar with R Markdown, one begins to adopt a reproducible workflow and might notice the capability of R language, hence gaining more incentive to learn R. Many people in academia uses Microsoft Word to write articles and papers. However, R Markdown has several advantages over MS Word: Easy to inserting images and tables in documents. Values of variables (e.g. values in tables or p-values) are automatically updated when raw data changes. Easy citation using citation keys (Zotero + Better BibTeX greatly facilitates this). Mutiple output format, e.g. LaTeX, PDF, Web Page, Book, etc. Template support for Journel articles, such as Elsevier, Sage, Springer, so no formatting is needed. But I think all benefits about R Markdown mentioned above aren’t enough to persuade people into giving up MS Word, since people are conservative in adoping new things. If using R Markdown (or R) has benefits specific to the field related to the researcher, it greatly enhances the chance of adopting R Markdown. Hence, if I want to persuade people to use R Markdown, I can first build R packages that enhances the ability of R Markdown in that field. There are LaTeX packages supporting drawing syntax tree, but LaTeX package is not compatible with HTML output.↩ I actually stared and gave up learning programming languages three times (C++, C, and then Python) before I successfully learned R.↩ Last updated: 2019-03-07 ","subtitle":"","title":"Rendering IPA Symbols in R Markdown","uri":"/2018/09/06/ipa-symbols/"},{"content":" Two reasons make it difficult: The feed you submit should ONLY be about R (e.g: with R code, or directly related to the R world/community). This causes problem because most bloggers write posts spanning several topics, and some blogging platform doesn’t support RSS feed for a particular category of posts. Make sure the HTML of your feed’s content is well formatted – otherwise it would not be able to load on r-bloggers. This includes to NOT copy-pasting from RStudio’s notebook extension – the feed should NOT include “base64” images, make sure your images are saved as png or similar file formats. For posts written with R Markdown and HTML Fragment output format, the embedded images generated from code chunks (such as outputs from plot()) are base64 images. Fixing the First Problem For Jekyll sites using jekyll-feed plugin to generate sitemaps, it is not possible to have RSS feed for a particular tag or category of posts. However there’s a workaround using Jekyll’s Liquid syntax to write an RSS template, as indicated by this post. I modified the template to make it suitable for the criteria set by R-bloggers, you can take a look at the file feed.rbloggers.xml. Fixing the Second Problem To fix the second problem, set self_contained: false in the yaml header of the R Markdown document. With this setting, plots from code chunks are automatically generated in a figure directory, and the .html output uses tags to source the plots in that figure directory. --- output: html_fragment: self_contained: false --- However, this creates a problem since the relative path in the tags probably won’t work in the remote directory that hosts your site. To overcome this problem, you have to change the default figure directory and post-process the output .html file. For example, the source of this post(rblogger-criteria.rmd) is two layers under the root dir of the web site. +---assets/ | +---rblogger-criteria-img/ +---_includes/ +---_plugins/ +---_posts/ +---post_source/ | +---rblogger-criteria/ | | +---rblogger-criteria.rmd | | +---rblogger-criteria.html +---index.html +---_config.yml I set my figure directory to assets/rblogger-criteria-img/: ```{r setup, include=FALSE} knitr::opts_chunk$set( fig.path = \"../..https://img.yongfu.name/assets/rblogger-criteria-img/\" ) ``` so when the site is rendered, the image would be in https://liao961120.github.io/assets/rblogger-criteria-img/ The last thing to do is processing the output rblogger-criteria.html and replacing ","subtitle":"","title":"Making Jekyll Blog Suitable for R-bloggers","uri":"/2018/08/02/rblogger-criteria/"},{"content":"這邊將使用 jiebaR，介紹使用自訂詞庫的斷詞方式，並提供自訂詞庫的製作方式。 示範語料 這裡使用金庸神雕俠侶第三十二回 — 情是何物作為斷詞的文本。武俠小說在此是個很好的例子，因為裡面有許多人物名稱和專有名詞。 因為著作權問題1，語料的原始檔(032.txt)將不會出現在本文的 GitHub repo 中。 製作自訂詞庫 取得小說這類文本的角色名稱與特殊名詞乍看之下可能非常耗工耗時，但有些時候其實相當容易，尤其是著名的小說。這要歸功於維基百科，因為越是著名的小說，其越有可能有詳盡的維基百科頁面，而維基百科對製作詞庫最重要的特色在於其頁面的超連結，因為通常只有專有名詞才會成為一個維基頁面上的超連結。 這邊使用維基百科的神鵰俠侶角色列表作為詞庫的來源。以下使用rvest套件清理此頁面： library(rvest) library(dplyr) library(magrittr) library(knitr) path \u003c- \"神鵰俠侶角色列表.html\" # 這裡已先行下載網頁，若無可直接使用網址 data \u003c- read_html(path) %\u003e% html_nodes(\"ul\") %\u003e% html_nodes(\"li\") %\u003e% html_nodes(\"a\") %\u003e% html_text() 觀察頁面後，可發現多數與小說相關的詞彙都位在 unordered list 下的連結內文( tag)，因此透過 3 個html_nodes()取得連結，並用html_text()擷取連結內文。 接著看看擷取的詞彙，可以發現這些詞彙依照順序大致可區分成三個來源： 自維基頁面的目錄擷取之連結 內文的連結(這是我們要的) 其它連結 對應至頁面最下方，與小說有關但並非小說主要內容的連結，如，「射雕英雄传角色列表」。另外，也包含維基百科頁面的固定連結，如「編輯」、「討論」、「下載為PDF」等。 data \u003c- unique(data) data[1:3] [1] \"1 主角\" \"2 桃花島\" \"2.1 「北丐」門派\" data[21:25] [1] \"楊過\" \"射鵰英雄傳\" \"楊康\" \"穆念慈\" \"全真教\" data[207:211] [1] \"射雕英雄传角色列表\" \"倚天屠龙记角色列表\" \"查\" [4] \"论\" \"编\" 我們要的內容介在data[21](楊過)至data[206](樊一翁)之間。此外，亦可手動加入連結中沒有的詞彙： data \u003c- as_data_frame(data[21:206]) %\u003e% rbind(\"過兒\", \"靖哥哥\") # 手動額外輸入 head(data, 4) %\u003e% kable(\"markdown\", align=\"c\") value 楊過 射鵰英雄傳 楊康 穆念慈 最後，將data存成.csv檔，方便未來使用： readr::write_csv(data, \"sdxl_wordlist.csv\") jiebaR 斷詞 準備好自訂詞庫後，要開始對文本進行斷詞。 jiebaR 斷詞可以選擇外來檔案或將檔案讀入後在進行斷詞，這邊將文本檔案讀入再斷詞： library(stringr) raw_text \u003c- readr::read_file(\"032.txt\") raw_text %\u003e% str_trunc(80) [1] \"第三十二回　情是何物\\r\\n\\r\\n　當黃蓉、一燈、郭芙等被困大廳之時，楊過和小龍女正在花前並肩共語。不久程英和陸無雙到來。小龍女見程英溫雅靦腆，甚是投緣，拉住...\" 無自訂詞庫 首先，我們可以看看沒有自訂詞庫的斷詞效果： library(jiebaR) stop_words \u003c- readr::read_table2(\"stop-zh-tw-withpunc\", col_names = F) %\u003e% rbind(\"\\n\", \"\\r\") %\u003e% set_names(\"word\") seg \u003c- worker(bylines = F, symbol = T) segment(raw_text, seg) %\u003e% as_data_frame() %\u003e% anti_join(stop_words, by=c(\"value\"=\"word\")) %\u003e% count(value) %\u003e% arrange(desc(n)) %\u003e% head() %\u003e% kable(\"markdown\", align=\"c\") value n 道 156 小龍女 115 楊 91 公孫止 86 楊過 76 黃 65 可以看到有些斷詞是正確的，如「公孫止」。但某些似乎常常斷錯，例如，「黃蓉」、「楊過」(某些似乎斷錯，導致有許多單獨的「楊」)。 使用自訂詞庫 在jiebaR::worker()中設定自訂詞庫的位置：user = \"sdxl_wordlist.csv\"，即可在斷詞系統中新增字典： seg \u003c- worker(bylines = F, symbol = T, user = \"sdxl_wordlist.csv\") segment(raw_text, seg) %\u003e% as_data_frame() %\u003e% anti_join(stop_words, by=c(\"value\"=\"word\")) %\u003e% count(value) %\u003e% arrange(desc(n)) %\u003e% head() %\u003e% kable(\"markdown\", align=\"c\") value n 楊過 187 道 150 小龍女 119 公孫止 104 黃蓉 95 李莫愁 59 可以看到使用自訂詞庫後，斷詞變得有意義多了。 本文目的僅在促進教育與學術，並無營利企圖。且本文僅顯示極少的小說內容，應屬合理使用。若有侵犯著作權的疑慮，麻煩透過 Email 與我聯絡。↩ Last updated: 2018-11-10 ","subtitle":"","title":"jieba 自訂詞庫斷詞","uri":"/2018/07/31/jieba-dict/"},{"content":" 流程 graph LR html(\"HTML\") html -.-\u003e|\"rvest\"| df0 subgraph 前處理 df1(\"斷詞 data_frame\") df0(\"data_frame\") df0 -.-\u003e|\" jiebaR (保留標點) \"| df1 df1 -.-\u003e|\"ropencc 簡轉繁\"| df1 end corp(\"Corpus\") token(\"Tokens\") subgraph quanteda df1 -.-\u003e|\"quanteda corpus()\"| corp corp -.-\u003e|\"quanteda tokenize()\"| token end html -.- bls(\" \") style bls fill:none,stroke:none style html fill:#ccbdb9 style df1 fill:#92ff7f linkStyle 5 stroke-width:0px,fill:none; 資料爬取 這邊使用 RStudio 軟體工程師 Yihui 的中文部落格文章作為練習素材。首先需要取得文章的網址，因此先到部落格的文章列表頁面(https://yihui.name/cn/)，使用瀏覽器的開發者工具(按Ctrl + Shift + I開啟)進行觀察。 接著使用rvest套件擷取網頁中所有文章的連結，並將文章網址儲存成list_of_post.txt： library(dplyr) library(rvest) list_of_posts \u003c- read_html(\"https://yihui.name/cn/\") %\u003e% html_nodes(\".archive\") %\u003e% # 列表在 div.archive 之下 html_nodes(\"p\") %\u003e% # 文章標題在 下之 html_nodes(\"a\") %\u003e% html_attr(\"href\") # 文章連結在 下之 readr::write_lines(list_of_posts, \"yihui/list_of_post.txt\") head(list_of_posts, 2) [1] \"/cn/2018/10/middle-school-teachers/\" [2] \"/cn/2018/10/potato-pancake/\" tail(list_of_posts, 2) [1] \"/cn/2005/01/rtx/\" \"/cn/2005/01/20-13-00/\" length(list_of_posts) [1] 1097 可以看到總共有 1097 篇文章，時間從 2005 年到今年七月都有發文的紀錄。 由於文章數量相當多，因此之後僅會下載部分文章，避免造成伺服器負擔過大。下載網頁時，可以在 R 中直接使用rvest(見下文資料前處理)，但我比較建議使用 Bash1的wget指令，才不會因為重複下載網頁造成伺服器負擔。 在下載前，需先決定目標文章的網址sub_list： library(stringr) set.seed(2018) # 設隨機種子 固定隨機函數的結果 idx \u003c- str_detect(list_of_posts, \"2018|2015|2010\") sub_list \u003c- list_of_posts[idx] sub_list \u003c- sub_list[sample(seq_along(sub_list), 20)] %\u003e% # 抽出 20 篇 str_replace_all(pattern = \"^/\", # 將站內連結改為完整 url replacement = \"https://yihui.name/\") %\u003e% str_replace_all(pattern = \"/$\", \"/index.html\") readr::write_lines(sub_list, \"yihui/sublist.txt\") # 給 Bash 用的 sub_list %\u003e% str_replace_all(\"https://yihui.name/cn/\", \"\") %\u003e% str_replace_all(\"/index.html\", \"\") %\u003e% str_replace_all(\"/\", \"-\") %\u003e% str_replace_all(\"-$\", \"\") %\u003e% readr::write_lines(\"yihui/sublist_name.txt\") Bash 指令下載網頁 無法使用 bash 指令者，可跳過此節 為了自動化下載網頁，我寫了一個簡單的 Bash script wget_list，用法如下: wget_list \u003c網址文字檔\u003e \u003c檔名文字檔\u003e2 \u003c網址文字檔\u003e： 每一列(row)由一個網址組成 \u003c檔名文字檔\u003e： 每一列由一個名稱組成，每個名稱與\u003c網址文字檔\u003e的網址對應 在這裡，執行下列指令即可下載網頁 cd yihui/html wget_list ../sublist.txt ../sublist_name.txt cd - wget_list: #!/bin/bash #\u003c\u003c\u003c wget_list: dowload webpages listed in a file \u003e\u003e\u003e# ### Argument 1 is the file of links, 1 url per row ### ### Argument 2 is the file of names, 1 name per row ### file1=$1 file2=$2 ## Get the number of lines in the link list num_lines=$(wc -l $file1 | egrep -o '^[0-9]*') ## loop over the lines in file1, dowload the the file \u0026 name them as listed in file2 for (( i=1; i\u003c=${num_lines}; ++i )); do wget \"$(sed -n ${i}p $file1)\" \\ -O \"$(sed -n ${i}p $file2)\" done 資料前處理 在清理資料之前，需先剖析網頁結構(就如同之前剖析文章列表頁面一樣)。 這邊觀察這篇文章，大致可以找出這些資訊： path \u003c- \"https://yihui.name/cn/2015/11/peer-review/\" all \u003c- read_html(path) %\u003e% html_nodes(\"article\") header \u003c- all %\u003e% html_nodes(\"header\") title \u003c- header %\u003e% # 文章標題 html_nodes(\"h1\") %\u003e% html_text() post_date \u003c- header %\u003e% # 發文日期 html_node(\"h3\") %\u003e% html_text() %\u003e% str_extract(\"201[0-9]-[0-9]{2}-[0-9]{2}\") article \u003c- all %\u003e% # 內文 html_nodes(\"p\") %\u003e% html_text() %\u003e% paste(collapse = \"\\n\") # 這裡將 chr vector collapse 至 1 個字串， # 簡化資料結構，並以分行符號保留段落資訊 num_sec \u003c- all %\u003e% # 內文段落數 html_nodes(\"p\") %\u003e% length links \u003c- all %\u003e% html_nodes(\"p\") %\u003e% # 內文連結 html_nodes(\"a\") %\u003e% html_attr(\"href\") link_text \u003c- all %\u003e% html_nodes(\"p\") %\u003e% # 內文連結標題 html_nodes(\"a\") %\u003e% html_text() library(tibble) df \u003c- data_frame(title = title, date = post_date, content = article, num_sec = num_sec, links = list(links), link_text = list(link_text) ) df %\u003e% mutate(title = str_trunc(title, 8), content = str_trunc(content, 8), links = str_trunc(links, 8), link_text = str_trunc(link_text, 8)) %\u003e% kable(\"markdown\", align = \"c\") title date content num_sec links link_text 同行评审 2015-11-11 看到这么一… 8 c(“ht… c(“一则… 我們可以將上面的程式碼改寫成函數post_data()，自動讀取文章並輸出 data frame： post_data \u003c- function (path) { all \u003c- read_html(path) %\u003e% html_nodes(\"article\") header \u003c- all %\u003e% html_nodes(\"header\") title \u003c- header %\u003e% # 文章標題 html_nodes(\"h1\") %\u003e% html_text() post_date \u003c- header %\u003e% # 發文日期 html_node(\"h3\") %\u003e% html_text() %\u003e% str_extract(\"201[0-9]-[0-9]{2}-[0-9]{2}\") article \u003c- all %\u003e% # 內文 html_nodes(\"p\") %\u003e% html_text() %\u003e% paste(collapse = \"\\n\") # 這裡將 chr vector collapse 至 1 個字串， # 簡化資料結構，並以分行符號保留段落資訊 num_sec \u003c- all %\u003e% # 內文段落數 html_nodes(\"p\") %\u003e% length links \u003c- all %\u003e% html_nodes(\"p\") %\u003e% # 內文連結 html_nodes(\"a\") %\u003e% html_attr(\"href\") link_text \u003c- all %\u003e% # 內文連結標題 html_nodes(\"p\") %\u003e% html_nodes(\"a\") %\u003e% html_text() df \u003c- tibble::data_frame(title = title, date = post_date, content = article, num_sec = num_sec, links = list(links), link_text = list(link_text) ) } 接著，將所有文章讀取至一個 data frame all_post： library(dplyr) library(tidyr) html_list \u003c- list.files(\"yihui/html/\") # 列出資料夾下的檔案 all_post \u003c- vector(\"list\", length(html_list)) for (i in seq_along(html_list)) { path \u003c- paste0(\"yihui/html/\", html_list[i]) all_post[[i]] \u003c- post_data(path) } all_post \u003c- bind_rows(all_post) %\u003e% arrange(desc(date)) head(all_post) %\u003e% mutate(title = str_trunc(title, 8), content = str_trunc(content, 8), links = str_trunc(links, 8), link_text = str_trunc(link_text, 8)) %\u003e% kable(\"markdown\", align = \"c\") title date content num_sec links link_text 修辞还是真实 2018-06-21 说两封让我… 12 chara… chara… 花椒香料 2018-05-31 古人似乎喜… 2 /cn/2… 去年的花椒 CSS 的… 2018-05-14 CSS 中… 15 c(“ht… c(“查阅… 毛姆的文学回忆录 2018-05-04 前段时间看… 14 c(“/c… c(“职业… 距离的组织 2018-05-03 前面《闲情… 5 /cn/2… 闲情赋 语言圣战的终结？ 2018-04-19 一直以来我… 3 c(“ht… c(“惊天… 直接從網路讀取 如果無法使用 Bash 指令下載網頁，可將上面程式碼的html_list改為讀取sublist.txt中的 url，並修改for迴圈中的path： html_list \u003c- read_lines(\"yihui/sublist.txt\") # 讀取 url all_post \u003c- vector(\"list\", length(html_list)) for (i in seq_along(html_list)) { path \u003c- html_list[i] all_post[[i]] \u003c- post_data(path) } all_post \u003c- bind_rows(all_post) %\u003e% arrange(desc(date)) 斷詞 在處理中文、日語等文本資料，需先經過斷詞處理，因為其不像英語等歐洲語言的文本，以空格表示字詞的界線。 我們將使用jiebaR套件的segment()進行斷詞。由?segment()查看其 documentation 可知segment()只吃文字檔或 一個句子，因此需先搞清楚all_post的結構才能進行斷詞： all_post: 20*5 的data_frame，每列(row)為一篇文章 - $title: 每列為 1 個值 - $date: 每列為 1 個值 - $content: 每列為 1 個值，段落資訊藏在字串中的\\n符號 - $links: 每列為 1 個 list - $link_text: 每列為 1 個 list all_post$content的結構相當簡單(一篇文章一個字串)，因此不須經過額外處理。其它變項不須斷詞處理，因此在此不加細談。 jiebaR::segment 因為all_post$content簡單的結構符合jiebaR套件的預設需求，但有時資料會比較複雜，因此記錄下來供未來參考。 前面提到jiebaR::segment只吃一個句子(一個字串)或文字檔，那如果丟一個 vector 給它會怎樣？答案是看worker()的設定： library(jiebaR) seg \u003c- worker(symbol = T, bylines = F) segment(c(\"妳很漂亮\", \"我不喜歡你\"), seg) [1] \"妳\" \"很漂亮\" \" \" \"我\" \"不\" \"喜歡\" \"你\" seg \u003c- worker(symbol = T, bylines = T) segment(c(\"妳很漂亮\", \"我不喜歡你\"), seg) [[1]] [1] \"妳\" \"很漂亮\" [[2]] [1] \"我\" \"不\" \"喜歡\" \"你\" bylines = F：回傳 1 個 chr vector，其每個元素為 1 個詞。 bylines = T：回傳 1 個 list，其長度(元素的數量)等於輸入之 vector 的長度，每個元素為一個 chr vector。 bylines = F的設定在此符合我們的需求，並且為配合quanteda套件的特性而將斷詞結果以一個字串(以空格分開字詞)而非一個 chr vector 的形式儲存。 以下對第一篇文章進行斷詞： library(jiebaR) all_post_seg \u003c- all_post seg \u003c- worker(symbol = T, bylines = F) all_post_seg$content[1] \u003c- all_post$content[1] %\u003e% segment(seg) %\u003e% paste(collapse = \" \") all_post$content[1] %\u003e% str_trunc(20) [1] \"说两封让我感到“我天，给亲友的书信...\" all_post_seg$content[1] %\u003e% str_trunc(30) [1] \"说 两封 让 我 感到 “ 我 天 ， 给 亲友 的 ...\" 要處理所有文章，僅需外包一個 for loop： all_post_seg \u003c- all_post seg \u003c- worker(symbol = T, bylines = F) idx \u003c- seq_along(all_post$content) for (i in idx){ all_post_seg$content[i] \u003c- all_post$content[i] %\u003e% segment(seg) %\u003e% paste(collapse = \" \") } head(all_post$content, 3) %\u003e% str_trunc(20) [1] \"说两封让我感到“我天，给亲友的书信...\" [2] \"古人似乎喜欢把花椒当香料用。在《古...\" [3] \"CSS 中的位置（position...\" head(all_post_seg$content, 3) %\u003e% str_trunc(30) [1] \"说 两封 让 我 感到 “ 我 天 ， 给 亲友 的 ...\" [2] \"古人 似乎 喜欢 把 花椒 当 香料 用 。 在 《 ...\" [3] \"CSS 中 的 位置 （ position ） 属...\" 簡轉繁 OpenCC 是一個簡體字與繁體字轉換的專案，非常優秀，因為其不僅是單純字轉字，甚至處理了地區性的用法(如「軟體」vs.「软件」)。因此，其簡繁轉換的選項有非常多： s2t.json Simplified Chinese to Traditional Chinese 簡體到繁體 t2s.json Traditional Chinese to Simplified Chinese 繁體到簡體 s2tw.json Simplified Chinese to Traditional Chinese (Taiwan Standard) 簡體到臺灣正體 tw2s.json Traditional Chinese (Taiwan Standard) to Simplified Chinese 臺灣正體到簡體 s2hk.json Simplified Chinese to Traditional Chinese (Hong Kong Standard) 簡體到香港繁體（香港小學學習字詞表標準） hk2s.json Traditional Chinese (Hong Kong Standard) to Simplified Chinese 香港繁體（香港小學學習字詞表標準）到簡體 s2twp.json Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom 簡體到繁體（臺灣正體標準）並轉換爲臺灣常用詞彙 tw2sp.json Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom 繁體（臺灣正體標準）到簡體並轉換爲中國大陸常用詞彙 t2tw.json Traditional Chinese (OpenCC Standard) to Taiwan Standard 繁體（OpenCC 標準）到臺灣正體 t2hk.json Traditional Chinese (OpenCC Standard) to Hong Kong Standard 繁體（OpenCC 標準）到香港繁體（香港小學學習字詞表標準） ropencc套件是 OpenCC 的 R 語言接口，其不在 CRAN 上，需以devtools從 GitHub 下載： devtools::install_github(\"qinwf/ropencc\") 使用上非常容易： library(ropencc) trans \u003c- converter(TW2SP) # 臺灣用法轉大陸用法 run_convert(trans, \"開放中文轉換軟體\") [1] \"开放中文转换软件\" trans \u003c- converter(T2S) # 單純繁轉簡 run_convert(trans, \"開放中文轉換軟體\") [1] \"开放中文转换软体\" trans \u003c- converter(S2TWP) # 簡轉臺灣用法 run_convert(trans, \"开放中文转换软件\") [1] \"開放中文轉換軟體\" 在此我使用S2TWP轉換$content；S2T轉換$title： library(ropencc) all_post_seg$content \u003c- run_convert(converter(S2TWP), all_post_seg$content) all_post_seg$title \u003c- run_convert(converter(S2T), all_post_seg$title) head(all_post_seg) %\u003e% mutate(title = str_trunc(title, 8), content = str_trunc(content, 8), links = str_trunc(links, 8), link_text = str_trunc(link_text, 8)) %\u003e% kable(\"markdown\", align = \"c\") title date content num_sec links link_text 修辭還是真實 2018-06-21 說 兩封 … 12 chara… chara… 花椒香料 2018-05-31 古人 似乎… 2 /cn/2… 去年的花椒 CSS 的… 2018-05-14 CSS … 15 c(“ht… c(“查阅… 毛姆的文學回憶錄 2018-05-04 前段時間 … 14 c(“/c… c(“职业… 距離的組織 2018-05-03 前面 《 … 5 /cn/2… 闲情赋 語言聖戰的終結？ 2018-04-19 一直 以來… 3 c(“ht… c(“惊天… quanteda 我們前面進行的資料前處理，已經將資料整理成符合quanteda::corpus()輸入的格式： A data frame consisting of a character vector for documents, and additional vectors for document-level variables 因此，依以下指令即可將all_post_seg轉換成corpus物件： library(quanteda) corp \u003c- corpus(all_post_seg, docid_field = \"title\", text_field = \"content\") corp %\u003e% summary() %\u003e% as_data_frame() %\u003e% head(3) %\u003e% mutate(links = str_trunc(links, 8), link_text = str_trunc(link_text, 8)) %\u003e% kable(\"markdown\", align = \"c\") Text Types Tokens Sentences date num_sec links link_text 修辭還是真實 217 375 15 2018-06-21 12 chara… chara… 花椒香料 149 246 9 2018-05-31 2 /cn/2… 去年的花椒 CSS 的位置屬性以及如何居中對齊超寬元素 347 805 23 2018-05-14 15 c(“ht… c(“查阅… 有了corpus的資料結構後，即進入了下圖quanteda的分析架構，也結束了資料前處理的階段，開始進入 EDA 的階段。 graph TD C(Corpus) token(Tokens) AP[\"Positional analysis\"] AN[\"Non-positional analysis\"] dfm(DFM) tidy(\"Tidy Text Format\") vis(\"Visualize\") C --\u003e token token --\u003e dfm token -.-\u003e AP dfm -.-\u003e AN tidy --\u003e|\"cast_dfm()\"| dfm dfm --\u003e|\"tidy()\"| tidy dfm -.- vis tidy -.-\u003e vis AP -.- vis style C stroke-width:0px,fill:#6bbcff style token stroke-width:0px,fill:#6bbcff style dfm stroke-width:0px,fill:#6bbcff style tidy stroke-width:0px,fill:orange linkStyle 6 stroke-width:0px,fill:none; linkStyle 8 stroke-width:0px,fill:none; quanteda 有相當完整的教學資源，且有很多有用的函數。同時，tidytext 套件也能輕易與 quanteda 配合，在 document-feature matrix 與tidytext所提倡的 tidy data frame(one-token-per-document-per-row) 兩種資料結構間自由轉換。tidy data frame 的格式與ggplot2相吻合，有助於資料視覺化的進行。 這裡選擇以quanteda而非tidytext作為主要架構的原因在於tidytext的架構僅容許 bag-of-words 的架構，但quanteda除了 bag-of-words 之外，還保有 Positional analysis 的潛力。 由於篇幅有限，這裡不多加細談quanteda套件3。關於quanteda的使用，可以參考 quanteda tutorial，內容非常詳盡。 Reproduce 這篇文章的原始碼在我的 GitHub，歡迎下載至自己的電腦執行。 參考資料 Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. 1st ed. O’Reilly Media, Inc. Watanabe, Kohei, and Stefan Müller. 2018. “Quanteda Tutorials.” Quanteda Tutorials. https://tutorials.quanteda.io/. Mac 和 Linux 內建有 Bash，但 Windows 沒有。↩ 要能直接執行wget_list需先給予其執行的權限，因此需設置chmod 755 ，並且將wget_list置於 shell 會自動搜尋程式的地方(如/usr/bin/)。 另一個方法是不設置權限，直接執行wget_list： bash ↩ 未來可能會發一篇續作。↩ Last updated: 2018-11-10 ","subtitle":"中文、R 與 quanteda","title":"Text Mining 前處理","uri":"/2018/07/28/quanteda-tutorial/"},{"content":" I’ve used R Markdown to do lots of work, such as writing a book, doing homework on statistics and ecology, and making web pages. The above mentioned are all HTML output formats from R Markdown, but PDF output formats are also possible, such as this Beamer Slide and an article about R Markdown.\nBecause of doing so much work with R Markdown, I write down some of the most common and useful features of R Markdown so I can look it up when I need to1. I also make it available to everyone by putting the document on my blog.\nOther Resources of R Markdown R Markdown: The Definitive Guide\nBookdown\nR Markdown Cheat sheet\nSince it is intended for quick lookup, the document is very compact. That means you need to have some basic knowledge on R Markdown to be able to understand all of it. For more detailed documents for beginners, see the last section of this post.↩\nVisit R-bloggers\nLast updated: 2018-12-01 ","subtitle":"","title":"My Notes on R Markdown","uri":"/2018/07/23/rmd_intro/"},{"content":" google 表單大幅降低蒐集問卷資料的難度；此外，表單將回應自動彙整成試算表更使分析資料變得非常容易。然而，google 表單缺乏一項重要的功能：即時將結果回饋給填寫者1。 讓問卷填寫者能馬上知道結果，可以增加其填寫意願，同時也是負責的態度(在回饋不會造成負面影響的前提下)。當然，這在 google 表單本身的限制下無法達成。以下將介紹如何結合 google 試算表 以及 DataCamp Light，讓任何人都能製作出一個在靜態網頁上運行的平台，使填寫者能在此填寫問卷、查詢結果。 實際操作 繼續閱讀下去前，可先至回饋功能示範平台操作看看，比較容易理解下文內容。文章中的說明即是依據此回饋功能示範平台。 概觀: 運作邏輯 上圖的每個方塊(除了左圖的試算表)名稱，皆對應到回饋功能示範平台背後運作的檔案。圖需要分左、右來看： 左側以使用者觀點為中心，顯示使用者填寫問卷(送出資料)到獲得回饋之間，資料流動的路程。 右側的流程圖，實際上是左圖試算表(表單、DataCamp之間)那格的完整路程，意即資料在 google 試算表間的流動及運算。使用者獲得的回饋即是由這些試算表的運算產生。 回饋功能運作的邏輯其實非常簡單：在問卷送出後，透過 google 試算表處理資料(運算、整理)；接著透過 DataCamp light 執行預先寫入的 R 語言，讀取經 google 試算表處理後的資料；最後使用者輸入查詢金鑰(於問卷中填寫)，篩選出那筆自己填寫的資料。 以下，說明如何設置問卷回饋系統的各個成分，並使用此資料夾中的檔案說明。檔案間的關係完全對應至上文概觀中的概念圖，亦即問卷回饋平台背後蒐集及運算資料所使用到的檔案。下方的說明，單純閱讀文字會難以理解。若有打算實作，可實際打開資料夾中的檔案以配合閱讀，或甚至完全自己重複文中的步驟。 表單、試算表 設置 這節將設置問卷回饋平台的資料蒐集與運算功能，包含 1 個 google 表單(表單)及 3 個 google 試算表(表單回應, 運算分析, 結果查找)。 連結表單至試算表 這項功能使用過 google 表單的人都知道，可參考 google 說明，以下簡單說明： 從雲端硬碟進入到表單後，即會顯示下圖的頁面(需具編輯權限)。注意需於中間白色方塊點選「回覆」，畫面才會如下圖(預設畫面是「問題」)。 接著點選白色方塊右上方的綠色 icon，即會出現 2 個選項： 建立新試算表，並命名。 (預設名稱為「無標題表單 (回應)」) 選取現有的試算表 選擇建立新的試算表。 點選建立後，即會在與表單相同的資料夾中建立試算表，我將其命名為表單回應(即概觀中右圖的表單回應)。 此後，每當有人填完問卷，表單回應即會自動新增一列(row)資料。 試算表間的連結: IMPORTRANGE 千萬不能編輯表單回應，這可能會破壞收集到的問卷資料。google 試算表有一個很實用的函數IMPORTRANGE，能夠選取一試算表中特定的範圍，將其連結至另一獨立的試算表中(獨立檔案)。因此，每當原先的試算表更新，透過IMPORTRANGE連結的新試算表也會跟著更新。如此，即可在不更動表單回應下，對表單回應的內容進行運算。 若此文關於IMPORTRANGE有描述不清的地方，可參考這篇寫得相當清楚的文章。 {: .info} IMPORTRANGE(\"\", \"\u003c工作表名稱\u003e!\u003c儲存格範圍\u003e\") : 所欲匯入資料之試算表的網址，在此為表單回應之URL \u003c工作表名稱\u003e: 表單回應只有一個工作表，將其名稱填入這裡。 \u003c儲存格範圍\u003e: 儲存格範圍視問卷的題數與筆數而定，其格式為：A1:F9999。大寫字母代表欄位，一個欄位即為問卷上的一題；字母後面的數字是列數，一筆資料(一份問卷)佔有一列(row)。 **運算分析**試算表 匯入 在運算分析中的儲存格A1，我輸入了以下公式： =IMPORTRANGE(\"https://docs.google.com/spreadsheets/d/1-eOAbpOZ1aeuNUHo3b0olLTrheq-T-pe2BsRXK-P-mM/edit#gid=579070166\", \"表單回應 1!A1:E9999\") 以匯入表單回應的 A 至 E 欄2。 運算公式 我在 G 欄設定公式計算 Q1, Q2, Q3 的分數總合，其中 Q3 是反向計分。 時間戳記 由於之後會透過 DataCamp Light 讀取 google 試算表，但其並不支援英文以外的文字，因此需將試算表的格式改為英文： 選擇試算表 檔案 \u003e 試算表設定 \u003e 一般: 語言代碼: 美國 時區: (GMT+08:00) Taipei3 更改完試算表語言後，需更改時間戳記的格式4： 選擇時間戳記那欄(在此為 A 欄) 格式 \u003e 數值 \u003e 日期時間 **結果查找**試算表 運算分析設置完成之後，需要選擇希望使用者查詢時，能看到的項目: 時間戳記: A 欄 Token: E 欄 score: G 欄 因此，需將結果查找中的 A、B、C 欄分別對應到運算分析中的 A、E、G 欄。在結果查找的儲存格A1、B1、C1，分別使用IMPORTRANGE： 儲存格A1 =IMPORTRANGE(\"https://docs.google.com/spreadsheets/d/1znFpdD_Kt1Jk274l0yD1dGZZyhsh7m1Xji9IYZUigEU/edit#gid=0\", \"工作表1!A1:A9999\") 儲存格B1 =IMPORTRANGE(\"https://docs.google.com/spreadsheets/d/1znFpdD_Kt1Jk274l0yD1dGZZyhsh7m1Xji9IYZUigEU/edit#gid=0\", \"工作表1!E1:E9999\") 儲存格C1 =IMPORTRANGE(\"https://docs.google.com/spreadsheets/d/1znFpdD_Kt1Jk274l0yD1dGZZyhsh7m1Xji9IYZUigEU/edit#gid=0\", \"工作表1!G1:G9999\") DataCamp Light 設置 DataCamp 是一個學習資料科學程式語言的線上教學網站，有 R 和 Python 的教學。DataCamp Light 是一個互動式的程式語言輔助教學工具。其能夠鑲嵌在網頁上，讓使用者直接透過網頁學習 R 或 Python。 這裡即透過 DataCamp 執行預先寫入的 R Script，讀取儲存在雲端的結果查找。使用者在 DataCamp Light 輸入的Token是用來篩選資料，以回傳使用者填寫的那筆問卷。 取得試算表權限 DataCamp Light 讀取的是結果查找的內容，因此需將結果查找需將結果查找透過連結分享5至網路: 開啟結果查找，選取右上角藍色按鍵共用即會開啟下圖中的小視窗： 接著， 選取「知道連結的人均可以檢視」(注意不要選到可以編輯) 複製連結 按下方「完成」 完整程式碼 以下是回饋功能示範平台的 DataCamp Light 程式碼(html)： 1 2 3 4 library(googlesheets) 5 data \u003c- gs_read(gs_url(\"https://docs.google.com/spreadsheets/d/1ufuzTL9VCxdvX1QeFQcMGxYbEMq1ZEWVht3CEDpXBmc/edit?usp=sharing\")) 6 7 data \u003c- as.data.frame(data) 8 colnames(data) \u003c- c(\"DateTime\", \"Token\", \"Score\") 9 data \u003c- data[which(!is.na(data$DateTime)),] 10 score \u003c- function(token) { 11 i \u003c- which(data$Token == token) 12 data[i,] 13 } 14 15 16 # Put Token in \"\". Ex: score(\"abcde123\") 17 score(\"Enter_your_Token\") 18 19 下面將分別說明這些程式碼的意義。 注意 DataCamp Light 僅能正常顯示英文，因此需確定 R Script 以及使用者填入的 Token 皆沒有多位元組字(例如，中文)。 {: .error} 預先執行程式碼 下面為...之間的程式碼 此段程式碼是使用者看不到，但會預先執行的 R Script，其中可分成 3 個部分：讀取資料、資料刪減、查找函數。 1library(googlesheets) 2data \u003c- gs_read(gs_url(\"https://docs.google.com/spreadsheets/d/1ufuzTL9VCxdvX1QeFQcMGxYbEMq1ZEWVht3CEDpXBmc/edit?usp=sharing\")) 3 4data \u003c- as.data.frame(data) 5colnames(data) \u003c- c(\"DateTime\", \"Token\", \"Score\") 6data \u003c- data[which(!is.na(data$DateTime)),] 7 8score \u003c- function(token) { 9 i \u003c- which(data$Token == token) 10 data[i,] 11} 讀取資料 1library(googlesheets) 2data \u003c- gs_read(gs_url(\"https://docs.google.com/spreadsheets/...\")) 上面這段程式碼透過雲端讀取結果查找，並將其儲存於變項data。函數內的連結即是上面在設置結果查找的讀取權限時的共用分享連結。 資料刪減 1data \u003c- as.data.frame(data) 2colnames(data) \u003c- c(\"DateTime\", \"Token\", \"Score\") 3data \u003c- data[which(!is.na(data$DateTime)),] 上面的程式碼做了 3 件事(1 行 1 件事)： 將data轉變成 base R 的data.frame。由readr::read_csv讀進來的data.frame會是tibble，而tibble在 DataCamp Light 的 console 印出時，會顯示出幾項對使用者沒用的訊息(這是 R 給資料分析者看的)，但傳統的 base R data.frame不會。 第二行則是將資料中每一個變項的名稱，更改為 DateTime(時間戳記)、Token(Token)、Score(Score)。括號內的名稱是結果查找內的變項名稱。 第三行是用以刪減多餘的資料。由於在運算結果套用公式時，勢必要為未來的儲存格著想：預先套入公式，才能計算未來產生的資料，因此，讀入的資料大多數都是空白的，僅有Score那欄為 66。 查找函數 1score \u003c- function(token) { 2 i \u003c- which(data$Token == token) 3 data[i,] 4} 這是 DataCamp Light 在此最關鍵的功能。score()是一函數，讓填寫者透過當初於問卷填寫的 Token ，查詢自己的問卷回饋7。該函數的功能，即是在data中的Token變項，尋找符合使用者輸入的值，並將符合的資料印在 console 上。 顯示程式碼 下面為...之間的程式碼，是使用者看的到的 R Script： 1# Put Token in \"\". Ex: score(\"abcde123\") 2score(\"Enter_your_Token\") 第一行是註解(同一行中，#之後的內容不會執行)，可用來說明。 第二行預先印出score()函數，讓使用者僅需輸入 Token 而不需自行打出函數。 靜態網頁 設置 對於完全沒有概念的人，設置靜態網頁可能會是比較困難的部分，因為多數人對此相當陌生。靜態網頁在此的目的是為了讓 DataCamp Light 的程式碼(即一段 HTML)能夠運行，因此若讀者使用的部落格平台允許自由變更網頁的 html 並且能自由匯入 JS8，則可以忽略此節內容。 GitHub Pages 架設靜態網頁9並非難事，難的是做出漂亮的靜態網頁。然而，網頁越漂亮，其結構通常也更加複雜。如何(短時間)打造美觀的靜態網頁以及基礎 HTML, CSS 的概念並非此文的目的。對於有這些需求的讀者，我推薦 Yihui Xie 的 blogdown。 以下提供一個最精簡的例子，由註冊 GitHub 帳號到架設網頁，過程中僅需使用到瀏覽器(GUI)，不需用到 Git。 註冊與建立 Repo 至 https://github.com/ ，填寫註冊資訊(一個 email 僅能註冊一次)，並記得去信箱認證。Username 即為之後網站的網址，以下圖為例，minimalghpage.github.io。 信箱認證後，將自動跳回 GitHub 頁面。之後，基本上不需更動出現之畫面的設定，只要按下一步。最後應會出現下圖，按右上角圖示並選取 Your Profile。 account info 按下網頁中上方的 Repositories 後應會出現下圖，接著再按下右上方的綠色按鈕 New。 出現下圖後，在 Repository name 輸入.github.io(一定要與當初註冊時填入的 Username 一模一樣)，並勾選下方 Initialize this repository with a README。最後按 Create repository。 上傳網頁 下載 Minimal Web Page (下載後需解壓縮。) 至剛剛建立的 Repository (.github.io)，點擊 Upload files (圖中黃色螢光處)。 進入新畫面後，將index.html, search.html, .nojekyll拖曳上傳，並按下畫面最下方 Commit changes. 上傳完成後，即可看到下圖。.nojekyll不會顯示出來。 **完成！**過 1, 2 分鐘後，即可至.github.io檢視網頁。 之後若要修改檔案，將修改過後的檔案依相同步驟上傳即可。 Minimal Web Page Minimal Web Page 裡面有三個檔案：index.html, search.html, .nojekyll。 index.html: 這是網站的首頁，亦即瀏覽器進入https://.github.io/時所讀取的檔案。此檔案內含 HTML 必要結構，並且匯入 bootstrap 的 CSS 和 JS 以快速製作漂亮的 Button 和 Modal。 search.html：這份檔案即為上文 DataCamp light 的完整程式碼，加上一些 HTML 的必要結構以及重新整理頁面的按鈕(Reload)。若需修改其中的 R Script，需用文字編輯器開啟此檔案修改...裡面的內容。 .nojekyll: Jekyll 是 GitHub Pages 預設的靜態網頁產生器，能自動將 Markdown 生成.html，對於常寫文章的使用者很方便：不需每次發文都要自己將文章轉為 html 檔。.nojekyll在此的作用是告訴 GitHub Pages 不要使用 Jekyll 產生網頁，因為使用 Jekyll 產生網頁，repository 需符合特定的檔案格式與架構10。 R 使用者 會用 Rmarkdown 的人，可直接下載回饋功能示範平台製作網頁(需額外安裝一些 package)，不須使用上述資料夾內的檔案。這能省下許多製作網頁(index.html)的時間。 R markdown 是 Markdown 的擴充，其輸出的 HTML 格式已經過簡單的排版，同時也支援 Bootstrap (Minimal Web Page 裡的 HTML 也有匯入 Bootstrap)，因此能夠輕易地製作出美觀的網頁。Rmarkdown 可輸出許多格式，其中 html_document 最為簡單。Rmarkdown 的語法(Cheat Sheet)即為 Markdown 語法加上許多額外的功能(透過 R 實現)。 隱私問題 在此需特別提醒問卷填寫者隱私的問題。由於查詢個人的問卷回饋需透過 DataCamp Light，其由雲端讀取之試算表(結果查找)是公開的。縱使網頁表面看不見結果查找的網址，但只要檢視網頁的原始碼(透過瀏覽器的開發人員工具，或至 GitHub 直接下載search.html)，即可取得結果查找的網址，並下載整份資料11。 以這裡的例子說明，結果查找僅含有 3 欄：時間戳記、Token、分數。這 3 欄是任何人都能看見的內容，其中 Token 是由問卷填寫者直接填寫，因此 在設計問卷時，需於 Token 那題特別提醒填寫者：不能填寫能關聯到個人身份的內容，如學號、e-mail 等。 Last updated: Apr 27, 2018 其實 google 表單確實能即時回饋分數，但僅限測驗模式，有諸多限制，例如，題目僅能為「對」或「錯」，無法處理反向計分的問題，無法使用線性刻度 (linear scale) 計分等。 ↩︎ 若擔心填答人數超過 9998 人，可設個更大的數字，如E99999。 ↩︎ 你也可以設置時區，通常依據的是多數問卷填寫者所在位置的時區。這邊設為台北時間。 ↩︎ 這邊是為了方便之後 R parse 日期格式。 ↩︎ 這關係到隱私問題，詳見最後一節隱私問題 ↩︎ 總分(Score) = 空白(Q1) + 空白(Q2) + 6(6 - Q3)。Q3 是反向計分的五點量尺。 ↩︎ 若有兩筆以上的資料有相同的 Token，score()就會篩選出相同筆數的資料，並將這些資料印在 console 上。此時，可以透過 DateTime 那行來確定填寫時間，以找到自己填寫的那筆資料。 ↩︎ 我對目前的部落格平台功能相當不熟悉，但就我所知提供此功能的應該不多。DataCamp Light 有提供 WordPress(不是 WordPress.com) 外掛，詳見 DataCamp Light Wordpress Plugin。 ↩︎ 這裡的靜態網頁是架設在 GitHub 上，代表(1)可以任意修改網頁；(2)網頁的內容(檔案)是完全公開的。因此，相對於其他部落格平台，如 Blogger， 網站管理人的彈性相當大，而且網頁上不會出現廣告。然而，由於檔案是完全公開的，需注意隱私以及版權問題。 ↩︎ 這是自行在 GitHub Pages 上架設部落格最困難的地方：使用者需對 Jekyll 有一定程度的理解。這同時也是我推薦 blogdown 的原因，其讓使用者略過理解複雜的靜態網頁產生器，而能專心在網頁的內容上。 ↩︎ 然結果查找透過IMPORTRANGE匯入的試算表只要未開放共用連結，仍是安全的。這也是為何即使僅需 2 個(或甚至 1 個)試算表和 DataCamp Light 即可做到問卷回饋，但我仍使用了 3 個試算表。 (另一原因是考量 google 及 DataCamp Light 的運算資源及時間。縱使我較喜歡，也應該要用 R 語言處理資料，考量到 google 擁有較強大及穩定的運算資源，多數的運算因此交給 google 試算表，而 DataCamp Light 僅用來讀取資料。) ↩︎ ","subtitle":"","title":"google 表單即時回饋","uri":"/2018/04/20/gsheet_survey/"},{"content":"I started customizing my blog template soon after I forked it from kitian616. The downside of customizing is that once started, there’s no going back. I saw some new features added to the TeXt theme recently, some of which are quite appealing to me. Since I started custimizing my blog and since I’m a layman of web page design, I have to figure out how to implement these features by myself. The features I implemented: Alert Text \u0026 Circled Image: These two features are basically simple CSS styling. I added these two features a bit different from the original TeXt theme, since we have different file structures now. But the concept is essentially the same, and I copy-and-pasted most of the code from _article.content.extra.scss of the TeXt theme to _article.content.scss of my blog’s source. I couldn’t figure out what some variables in _article.content.extra.scss refered to, so I changed all of them to plain CSS without refering to other files or variables. mermaid: Implementing mermaid is much more easy than the CSS things above, since I had experience with how JavaScript works on static sites (MathJax Setup). But I still encountered some difficulties: I don’t know how Tian Qi (author of TeXt theme) implemented it by setting code chunck language to mermaid. Anyway, I dealt with it by using the traditional html way1: using ... directly in markdown (see section mermaid). I put the mermaid script in mathjax.html instead of creating a new mermaid.html. Alert Text Success Text. {: .success} Info Text. {: .info} Warning Text. {: .warning} Error Text. {: .error} Code Success Text. {: .success} Info Text. {: .info} Warning Text. {: .warning} Error Text. {: .error} kramdown Feature {: something} is a feature unique to kramdown syntax (the markdown syntax used by Jekyll). It’s very useful for making markdown more powerful. The code (e.g. {: .error} above) works by attaching the class, error to the paragraph right above it (e.g. the paragraph, Error Text., above {: .error}. For more information, take a look at this post, Markdown Kramdown Tips \u0026 Tricks{:target=“blank}. Circled Image {:height=“80px” width=“80px”} {:.circle} Code ![](path-to-image){:height=\"80px\" width=\"80px\"} {:.circle} mermaid mermaid is a script language for generating charts from simple text. Below is an example of drawing a flow chart using mermaid. Documentation for Mermaid{: target=\"_blank”}. graph TD A(text)--\u003eB((rounded)) A--\u003eC B--\u003eD C--\u003eD Code 1```mermaid 2graph TD; 3 A(text)--\u003eB(rounded); 4 A--\u003eC; 5 B--\u003eD; 6 C--\u003eD; 7``` Notes See the section, Simple usage on a web page, in Usage of mermaid documentation. ↩︎ ","subtitle":"","title":"Some New Features of TeXt Theme","uri":"/2018/02/24/new_md_features/"},{"content":"接觸 R 的時間大約五個月了，從原本對電腦、程式一竅不通到現在能有效率的 debug、寫出簡潔有條理的 R code、甚至用 R 與 Markdown 架站寫部落格。算一算，我每週通常至少 3 天會用到 R，不是督促自己熟悉 R，是因為它太有魅力了。\n學 R 語言很有趣1，但也相當耗時耗力。我在學習 R 上花了大量的時間，若不是運氣好找到對的方向，我不可能花費那麼多力氣去學 R，甚至可能直接放棄。雖然如此，我還是感到有點可惜，若能更早知道有效率的學習方式，就不必花費如此大量的時間 (及紙張) 在學習上。因此，我希望將學習 R 的心路歷程寫下，給想學 R 的人做為參考，或許可以減短學習初期耗費精神的時期。接下來，我將說明：\n我學習 R 的動力來源\nR 學習路徑：若從頭來過，我會如何學習 R、挑選哪些資源，讓自己更有效率地學習。\n為何我能持續學習 R 而未放棄？其實現在我比較頭大的問題是：要怎麼克制自己不要一直打開 Rstudio？正是因為我學 R 的過程如同打電動一樣歡樂，所以放棄這個問題根本不存在。但事實上，剛開始學 R 時並不如何歡樂，直到上了課才越來越喜歡 R。\n課程回顧 106學年上學期，我最喜歡、也意外收穫最多的課是謝舒凱老師開設的 R 語言與資料科學導論，或許會是我大學生涯最喜歡的課吧。\n起初是抱著學好 R 語言 (加上通識快修不完) 的心情去加簽這門課的，但課堂上 R 語言的語法卻教得不多。然正是這種不聚焦在程式語法的課程安排，讓我把 R 學得非常好。如果課程從頭到尾都在教 R 語言，到期末我一定會受不了越顯複雜的語法，最後便隨意敷衍了事。我在這門課收穫最大的，反而不是課堂學到的技巧，而是老師、助教們傳達的一些概念與想法，以及自己探索這些概念想法的樂趣與收穫：\nOpen Source2 以及 R 的生態圈3\n資料科學與 Story telling: Coding 於資料科學中的應用，不僅是傳統所強調的功能性，Coding 亦於美觀及呈現上扮演重要的角色。資料科學透過 Coding 處理、分析資料；同時也透過 Coding 作圖將資料視覺化，好將資料科學上的發現說成故事給其他人聽。\n文本分析: 我本來對這部分最沒有興趣，直到我意識到文本資料是研究人類行為、社會與文化最重要的資料來源之一4 5。\nReproducibility: R Markdown (.Rmd) 是我見過最強大的文件格式，可以輸出成網頁、投影片、PDF、Word、Markdown 等。這讓使用者可以在一份文件中做事情 (Rmd)，並依據所需輸出成各種文件，而不用剪剪貼貼 (從 Word 貼到 PPT)，也省下排版所花費的心力。例如，要報告可輸出成投影片 (.html 或 .pptx) 、要列印可輸出成 PDF、要放到網路上可輸出成網頁。R Markdown 讓使用者能以簡單有效率的方式工作，這是達到 Reproducibility 的前提與重要基礎。\nReproducible Research with R 6\n課程起手式 這門課與我個性相符，完全顯現在第一堂課實習課 (Lab Session) 的作業上。作業與 R 並無相關，而是要使用 Markdown 格式撰寫自我介紹。由於我對簡約風有點痴迷7 (受 Markdown 影響後更為痴迷)，在 Markdown 上花了一些時間研究。目前，透過 R 與 Pandoc Markdown 語法的整合 (即 R Markdown)，我可以快速簡潔地寫出含有上下標、Footnotes、超連結、citation 的精美文件8或文章 (例如，這篇部落格) 而完全不須使用 html 語法。這讓我在寫許多作業時，效率提高不少。\n對 R Markdown 的痴迷與熱愛可說是我R語言功力進步的關鍵。R Markdown 賦予每個人將想法傳達給其他人的能力9，因此，能做出美好的東西並與其他人分享成為我學習 R 最強烈的動機之一。例如，當初在做這門課的期末專案 時，發覺 R 可以畫出很多種互動式圖表 (不須懂 JavaScript)，於是我開始尋找最適合呈現資料的方式。由於不同種類的圖，常有不同資料格式的要求10，因此為了畫出最適合的圖，我反覆整理許多資料以符合格式。這使得我在 data wrangling 上，能力大幅提升。\nR 學習路徑 起步 我剛開始學 R 時不算順遂11。剛進到 R 的世界，一定會被為數眾多的 function12 (以及更多的學習資源) 搞得迷失方向，不知從何開始。事實上，這不是我們的錯，因為連 R 的社群內也有相關的爭論：Base R first vs. Tidyverse first (詳細可參考此文)。R 真的是一個使用門檻不低的軟體，要精通它不是一、兩年內的事情13，所以一定要對 R 有熱情，才可能持續支撐對於 R 的學習14。\nR 語言的基礎 ─ Base R 是剛開始學 R 的一大障礙，這也是為何會有 Base R first vs. Tidyverse first debate。由於學好 R (更精確地說，是學好 Base R) 需一段時間，我認為 Base R first 的學習方式很容易使初學者放棄。但學習 R 不可能不接觸 Base R ，因為它是 R 語言最重要的基礎。因此，我覺得最好的方式是交錯並進：\nBase R 學到一定的基礎後 (不必精通) 15，開始學習用tidyverse套件處理資料。之後隨著時間，自然而然就會熟悉這兩者。\n維持 學習一項新技能最困難的地方大概是要能持續穩定前進。我認為有幾種方法可以幫助自己：\n參考書：一本好的參考書，能作為一個參照指標，幫助自己安排進度，也讓自己知道學到了哪裡。參考書指出了一條方向，比較不會在學習的路途中迷失 (相比零散的網路資源)。\n修課：我非常建議在學校修一門 R 語言的課 (最好 3 學分以上)，但不建議線上課程16。修課的話，有老師和助教提供經驗，能省下許多摸索的時間；每週有固定進度，或多或少可迫使自己持續接觸程式；若有期末專案，這會是 R 功力突飛猛進的最佳機會 (效果比起純粹練習好太多)。\n讓自己喜歡 R：初學 R 時因為不知道這項的重要性，所以學得有點辛苦。這點真的很微妙，有時候太過專注於學習某項\"技能\"，反而會過於專注實用性而忽略了趣味性，於是只剩理性說服自己：學這很有用，你一定要堅持下去。通常開始這樣想，代表離放棄不遠了。這也是為何我很推 R 語言與資料科學導論 這門課的原因 ─ 老師真的很有智慧，不斷鼓勵學生們去想有趣又可用資料科學回答的問題，也常介紹一些 R 意想不到的有趣應用，但從未將焦點放在程式的硬實力上。我自己由 R 善於視覺呈現的特質切入 – 從 R Markdown 開始，擴展到資料視覺化，又到網頁設計 (縱使我html和css基礎很差)，每一項對我來說都非常有趣，R 學起來因而樂此不疲。\n實作：專案 看再多參考書、修再多課、做再多練習題都比不上實際去完成一個專案。在實作專案的過程中，會遇到一缸子自己不知如何解決的問題，因此過程中就是在不斷吸收新知識、學習如何問問題17，如此的學習效果是零散練習題的好幾倍。此外，做專案是自頭至尾走過一次資料科學的流程，從爬資料、整理、分析到呈現，將先前習得的零碎技巧組織在一起，會讓 R 的功力大增；同時，資料科學專案是在實際解決 (回答) 一個問題，這不僅對於自己，且對於社會有實質的意義與價值。\n對於專案的一些建議 我認為要從實作專案收穫那麼多，不可或缺的因素同樣是興趣與熱情。沒有這些一定不可能堅持完美，不能堅持完美，專案就會淪落為練習題 (反正只要跑出東西就好)。題目的選擇因而會是成敗的關鍵；與組員 (若為團體專案) 的溝通與共識更是關鍵中的關鍵，一定要在專案中找到最適合自己的角色，否則專案只是浪費時間。\n學習資源 起初為了學 R 所印的兩本書以及聽的線上課程效果頗糟，讓我覺得自己起步不算順遂。這是因為當時對 R 不太了解，也不知如何觸及其社群，因而並未慎選書籍及課程即步入學習歷程。\n學習歷程 若回到五個多月前，我會如此安排 R 的學習歷程：\n基礎參考書 (若有餘力的話 + swirl18 當練習題)\nHands-On Programming with R：這是一本寫得非常好且相當簡單的書，目標讀者是沒有程式經驗的初學者 (雖然我覺得縱使有程式經驗也可看這本書了解 R 的邏輯)。此書內容僅聚焦在非常基礎的 Base R，但重點是其將 R 的邏輯寫得非常清楚。我是在學 R 一陣子之後才看了這本書，對我有莫大的幫助。這本書相當短，可以很快看完，更多資訊請 google 書名。\nR for Data Science：這本書應該只會越來越紅。如同其名，這本書就在教 R 於資料科學上的應用。與上本書不同的是，這本書以tidyverse套件為基礎而非 Base R。雖然如此，此書並不需要 Base R 的基礎 (有的話會更好，所以我才推薦 Hands-On)，需要的基礎書中都有介紹。\n我建議可以先從 Hands-On Programming with R 開始，因為這本書很短可以快速看完。之後再按照章節慢慢看 R for Data Science (可配合學校上課進度，這本書短時間內看不完)，逐步累積自己的功力。\n上述兩本書都是英文的，或許有些人很抗拒英文，但中文書並非一個好選擇。當 R 學到某個程度後，更進階的資源幾乎全為英文；此外，google 問題解決方法時，用中文十之八九會找不到答案。看英文書可讓自己熟悉 R 的英文用語，而且這些書的英文都很簡單。\n若有志探索 R 的其它可能性，可以參考 bookdown.org 的書 (僅推薦 Star 100 以上，上面還是有一些雷書)。這裡的書都是用bookdown套件寫的，而且皆為 open source，可以在網頁上看或下載到電腦 (詳見bookdown套件)。 學校課程：如課程回顧所提及，一門 R 的課可以讓自己視野更加廣博。僅是自修閉門造車進步很慢、常常會落掉進度、而且頗為苦悶。有人帶領比較不會走偏方向，同時還能學到許多有趣的東西。\n將 R 當玩具：如果學 R 如同遊戲一般好玩，那就沒有堅不堅持的問題了，這也是我自認學 R 相當成功的地方。以下，我將介紹一些我知道的有趣資源，不見得直接與資料分析相關，但一定可與 R 結合。\nR Graph Gallery 、htmlwidgets for R : R 最為吸引人的特徵之一就是其強大的繪圖功能，加上其它套件的擴充，幾乎所有跟資料有關的圖都可以用 R 畫。沒事多多欣賞其他人用 R 畫出的圖，不僅療癒放鬆，同時對 R 的能力有個大概的想像、未來更有可能會有使用需求。\nR Markdown : 先前提過 R Markdown 很強大，可以點進 gallery 感受一下其生產力。我認為不會用 R Markdown 比起不會用 R 還要可惜。\nBlogdown : 我目前的網站 (一部份) 是透過 blogdown 套件在經營19的。Blogdown 大幅降低了架站的門檻，即使完全不懂 HTML/CSS 也能快速上手，同時配合 Git/GitHub 能讓發表 (及修改) 文章的過程非常有效率。此外，以 blogdown 架設的網頁是靜態的，不同於 Wordpress 等的動態網頁，靜態網頁的速度要快許多、不需要付費伺服器，而且容易搬遷。\n集大成：專案製作\n如實作：專案所述，專案是讓自己快速進步的最佳方案。同時，結合上述的學習資源：\n資料分析：R for Data Science\n視覺化：R Graph Gallery, htmlwidgets\n網頁製作：R Markdown, Blogdown\n這些都可以透過 R 完成，並且讓自己更有動機實作專案、讓專案看起來更完美。\n更上一層樓：Following and Followers Following and Followers 是我直接借用 GitHub 上的用詞：\nFollowing: 該用戶追蹤的其他用戶\nFollowers: 追蹤該用戶的其他用戶\n透過追蹤 R 社群是一個讓自己跟上快速成長的 R 的好方法，讓自己每日都長一些知識。例如，我透過臉書追蹤 R bloggers，裡面常出現優質好文，且內容通常清楚易讀。\n另一個讓 R 功力快速成長的方式，就是自己成為 R bloggers：我不是指寫的文章要登上 R bloggers，而是像那些作者一樣在部落格上寫文章。寫文章是一種很好的學習方式，能幫自己重新組織所學，同時也可檢視是否真正了解正在撰寫的主題。文章不見得是寫給別人看的，有時忘記一些東西，透過自己的文字重新學習相當方便。推薦兩篇簡短的優質文說明寫 blog 的好處：Yihui Xie 、David Robertson。\n小結 我真的很慶幸能在大三時接觸到 R。以往我都把時間花在讀書上，也不知何時才能用得上。R 讓我發現了自己的嗜好20，也讓我體驗到為了處理想解決的問題而學習的感覺。我也才慢慢體會到為了興趣所學與為了其它目的 (競爭力、考試、學分、跟風？) 所學，能帶來的可觀差異。當然，興趣與其它目的並非不能同時存在，只是有時專注在其它目的會抑制興趣，而最能讓學習持續的動力來源卻是興趣。\n這篇文章的目的，旨在提供有志學 R 卻不確定如何開始的人作參考。文章介紹的學習資源 (或是任何嘮叨的內容)，不只是 (我認為) 有用的，且能強烈引起我的興趣。希望讀過這篇文章的人，不僅僅獲益於文章介紹的學習資源，且能為了興趣而學 R ─ 這是此篇文章最想傳達的想法。\nR 語言本身不算有趣，而是 R 及其生態圈所能做到的事。幾乎任何想得到、可用電腦處理的事情，都可找到相關的 R 套件。 ↩︎\n我覺得 Open Source 是一個很強盛且很令人感動的文化。其展現了透過分享與合作，人們能夠創造出多少美好的事物。 ↩︎\n我常在想，不知何時 SAS 與 SPSS 會被 R 幹掉。由於 SAS 與 SPSS 是私人版權軟體，在網路上的社群資源相當稀少且日漸縮小 (問問題找不到答案)，而 R 社群巨大且快速成長，加上其開放的特性，功能的擴充與成長是 SAS 與 SPSS 等封閉軟體無法企及的。 ↩︎\n至少是資訊量最大的資料來源，比較看看社群網站、網路、書籍 (見 Google Ngram，體驗一下文本資料的強大) 累積的資料量與心理學實驗累積的資料量。 ↩︎\n語言學不僅可用於研究語言、還可拿來當作研究人類社會與文化的工具。 ↩︎\n圖片擷取自 British Ecological Society 編寫之指南 A Guide to Reproducible Code ↩︎\n我本來對美感、簡約、使用者經驗完全不在乎，但受我好朋友的影響，越來越在意這些東西 (看到乾淨舒服的版面就會很興奮，有點點不和諧就會想修改它)。 ↩︎\n見 R Markdown 了解其強大。 ↩︎\nMarkdown 的學習門檻很低，使大家能簡單地寫出排版整潔漂亮的文章。R Markdown 大幅擴充 Markdown 的功能，秉持著相同的精神，R Markdown 使大家能簡單地寫出排版整潔漂亮 (且含有R跑出之圖表) 的文章、投影片、書籍，甚至網站。 ↩︎\n例如欲繪製時間序列的資料ggplot2及dygraphs對資料就有不同的要求。 ↩︎\n我在開學前的暑假即開始自學 R，進步相當緩慢，而且語法時常忘記。最重要的是，我必須花心力督促自己才能繼續。開學後在課堂上教導的內容不多，僅做些簡單的介紹，剩下的要靠自己發掘。由於深深感受到 Markdown 的潛力 (見課程起手式，我開始「不務正業」，去學一些和 R (看似) 不怎麼相關的東西 (例如，研究如何用 GitHub Pages 架部落格)，也越來越喜歡 html 格式 (以前偏愛 PDF，但發現 html 能呈現的東西比 PDF 多太多，又比較美觀)。於是，R 成為我處理作業最常使用的程式之一。 ↩︎\nR 除了函數非常多，使用起來也非常彈性，對使用者的限制相當少：其它語言很容易跑出 error message，但在 R 卻相對不容易 (例如，輸入c (1, 2, \"c\") 不會跑出 error 但會將1, 2從數字轉換成字串)。這對初學者不見得是好事：初學者常因此難以釐清 R 運作的邏輯。 ↩︎\n精通 R 應該可算是一輩子的事吧，因為 R 是個不斷在成長的語言。R 的社群與生態圈成長非常迅速 (例如，目前 CRAN 有 12,081 個 package)，要精通 R 勢必要與它一起成長。即使只求在傳統的資料分析上熟悉 R，也需要花不少的時間，而且很難在短時間內大幅進步。 ↩︎\n這裡把 R 說得有點負面。當 R 融入你生活中，你喜歡它都來不及了 (不論是它讓你做事更有效率或它能做出很厲害的東西)，根本就沒有支撐學習的問題，只有時間不足的問題。 ↩︎\n一定的基礎：了解 R 的向量式運算邏輯、資料結構 (vector, list, matrix, dataframe) 、熟悉 Subsetting (object[index])。見學習資源了解更多。 ↩︎\n我覺得學習任何東西，如果有實體 (面對面) 的課程，都比線上課程還好。我自己很難集中注意力在線上課程，加上時間太自由容易怠惰，或有事情時就會將其順位往後挪，因此成效往往不彰。 ↩︎\n在網路上要輸對關鍵字才找得到答案。起初由於不清楚 R 的運作邏輯，找答案的效率會很低。但隨著功力提升，會越容易知道問題的癥結在哪，找答案 (或是確認問題能否被解決) 也會變得更有效率。 ↩︎\nswirl 是 R 的套件，可以讓使用者直接在 R 的環境中互動式學習 R。其提供許多課程可供使用者下載，我覺得學完最基礎的課程就夠了，可以熟悉 Base R 的環境。詳細的課程見 swirl course。 ↩︎\n我是在逛 bookdown.org 的書時，無意間發現 Blogdown 的書，可惜當時我早已用 Jekyll / GitHub Pages 架好網站。Jekyll 的使用門檻相當高，我也並未學會如何使用 (我直接用別人做好的模板，因此只要修改一些.html和.css)。反之，blogdown支援的 Hugo 並沒有 Jekyll 那麼多的限制。blogdown同時大幅降低了架站的難度，例如，blogdown支援的 Markdown 語法 (Pandoc Markdown) 很豐富且很適合寫部落格，因此用其寫部落格就不必理會 Jekyll 或 Hugo 等網頁產生器所支援的特定 Markdown 語法 (Jekyll 支援kramdown、Hugo 支援blackfriday)。 ↩︎\n大學以前，我很排斥程式 (詳見此文)。後來發覺程式的實用性而開始想學程式語言。學了 R 後發現自己其實有資工魂 (雖然 CS 領域似乎不常用 R ？)，其實很喜歡程式語言、電腦等東西。 ↩︎\n","subtitle":"","title":"我的 R 學習歷程","uri":"/2018/01/31/rlearningpath/"},{"content":"THIS POST IS OUTDATED MathJax is a JavaScript display engine for mathematics that works in all browsers. By including MathJax support on the website, LaTeX mathematical expressions are rendered as pretty mathematical equations. Add MathJax Support Add the code below to the ... region of every page (.html) to enable MathJax support. The code below allows supporting $ and $$ in Markdown. 1 6 Equation autoNumber Add the code below to header if you want equation auto-numbering. 1 Escape Numbering Add \\notag to the equations to escape auto-numbering, e.g. Without \\notag 1$$\\frac{d}{d\\,t} N_{i}= b_{i} N_{i}\\left[C S\\right]_{i} - d_{i} N_{i}$$ gives $$\\frac{d}{d\\,t} \\,N_{i}= b_{i} N_{i}\\left[C S\\right]_{i} - d_{i} N_{i}$$ With \\notag 1$$\\frac{d}{d\\,t} N_{i}= b_{i} N_{i}\\left[C S\\right]_{i} - d_{i} N_{i} \\notag$$ gives $$\\frac{d}{d\\,t} N_{i}= b_{i} N_{i}\\left[C S\\right]_{i} - d_{i} N_{i} \\notag$$ Equation Reference Add \\label{eq:name} to equation to give identifier. Use $\\eqref{eq:name}$ to reference the labled equation1. Example I added \\label{eq:N} at the end of the equation: 1$$ \\frac{d}{d\\,t} N_{1} =\\left[ b_{1} N_{1}\\,\\left( S_{2}\\,c_{12}+S_{1}\\,c_{11}\\right)- d_{1} N_{1}\\, \\right] \\label{eq:N}$$ $$\\frac{d}{d\\,t} N_{1} =\\left[ b_{1} N_{1}\\,\\left( S_{2}\\,c_{12}+S_{1}\\,c_{11}\\right)- d_{1} N_{1}\\, \\right] \\label{eq:N}$$ Now, adding $\\eqref{eq:N}$ in the text, e.g, See $\\eqref{eq:N}$ for details. , gives See $\\eqref{eq:N}$ for details. GitHub Page Support You can take a look at MathJax support of my page here, it’s in the file mathjax.html. mathjax.html contains two part. The first starts with {% raw %}{% if site.mathjax == true or page.mathjax == true %}{% endraw %}, and the code below it is the same as the code in the section, Add MathJax Support. The second starts with {% raw %}{% if page.mathjax2 == true %}{% endraw %}, and the code below it is the same as the code in the section, Equation autoNumber. Note the variables mathjax and mathjax22 in the liquid tags. You can set them globally3 in _config.yml, or individually in the yaml header of each post/page. For example, I set the code below in _config.yml. 1mathjax: true 2mathjax2: false This allows mathjax support without equation auto-numbering to be the default setting of my site. When I want auto-numbering for some post (or page), I can set mathjax2: true in the yaml header of the post. Some Additional Warnings When I was typing this post, I found that GitHub Pages may have trouble rendering pages containing Latex. This is because GitHub Pages sometimes confuses several curly braces ({) written together as Jekyll liquid tags, which could cause page build failure. You can avoid this by not using too complicated Latex syntax, especially those with many { (or }) connected together. If you want to explicitely include liquid tags in your posts, look here for more details. Note that \\label \u0026 \\notag can’t be used together (You can’t label a numbering-escaped equation). ↩︎ Which is in site.mathjax, page.mathjax,and page.mathjax2, and are all set to true. ↩︎ The base template of my blog imports scripts from mathjax.html(line 18), so the whole site supports mathjax (as long as the page uses the template of my blog). ↩︎ ","subtitle":"","title":"MathJax Setup","uri":"/2018/01/27/mathjax/"},{"content":"I have been using the package dplyr to handle with data for a while, and I thought I can use it with ease until I was stuck with my homework on contructing a life table. I found spreadsheets (either Excel or Google Spreadsheets) easy for handling this task, but had a hard time dealing with it in R. I think it was due to my unfamiliarity with the built-in functions and insufficient practice in R. So, I wrote this post as a review and practice of my data-wrangling skills in R. I will illustrate how I constructed a life table with R, and you’ll find out how easy it is (and wonder how could I stumble on it). Load Packages I used these packages to construct a Life table. 1library(readr) 2library(dplyr) 3library(knitr) Load data Load the csv file to life_table. The raw data contains 3 columns: Age, Survivorship at Age x ($l_x$), and Fecundity at Age x ($m_x$). I added options(scipen=999) to disable scientific notations. 1options(scipen=999) # Disable Scientific Notation 2life_table \u003c- read_csv(\"life_table.csv\") 3life_table # A tibble: 10 x 3 Age lx mx 1 0 1.0000000 0 2 1 0.0000620 4600 3 2 0.0000340 8700 4 3 0.0000200 11600 5 4 0.0000155 12700 6 5 0.0000110 12700 7 6 0.0000065 12700 8 7 0.0000020 12700 9 8 0.0000020 12700 10 9 0.0000000 0 Variables to Construct Here are the variables that need to be calculated. Statistic Notation Calculation Formula $l_x m_x$ $x l_x m_x$ $l_x m_x e^{-rx}$ Average survivorship (age class) $L_x$ $L_x = (l_x + l_{x+1})/2$ Life expectancy $e_x$ $e_x = (L_x + L_{x+1} + … + L_{max})/l_x$ Reproductive value $V_x$ $\\displaystyle \\frac{\\sum_{y=x}^{max\\hspace{0.3mm}x} e^{-ry} l_y m_y}{e^{-rx} l_x}$ Net reproductive rate $R_0$ $\\sum_{all \\hspace{0.3mm} x} l_x m_x$ Generation time $G$ $\\frac{\\sum_{all \\hspace{0.3mm} x} x l_x m_x}{R_0}$ Intrinsic rate of increase Approximate $r$ $r \\approx \\frac{ln(R_0)}{G}$ Intrinsic rate of increase (True) $r$ $\\displaystyle \\sum_{all \\hspace{0.3mm} x} e^{-rx}l_x m_x = 1$ $l_xm_xe^{-rx}$ and $V_x$ will be calculated twice for the approximate and the true $r$. Constructing Variables mutate: Creating new columns Using the pipe %\u003e% and the function mutate in package dplyr, I first constructed 7 new variables. Note the dependencies of the variables, so that I couldn’t construct the life tables at once. 1life_table \u003c- life_table %\u003e% 2 mutate(\"lx*mx\"=lx*mx, 3 \"x*lx*mx\"=Age*lx*mx, 4 \"Lx\"=(lx+lead(lx))/2, 5 \"Lx\"=replace(Lx, 10, 0), 6 \"ex\"=rev(cumsum(rev(Lx)))/lx, 7 \"R0\"=sum(lx*mx), 8 \"G\"=sum(Age*lx*mx)/R0, 9 \"approx.r\"=log(R0)/G 10 ) Two things worth noting in the mutate function: The code \"Lx\"=(lx+lead(lx))/2, \"Lx\"=replace(Lx, 10, 0) lead(lx) shifts the whole column of $l_x$ to its next value, i.e. the column $l_x$ becomes $l_{(x+1)}$. Due to lead(lx), the last entry of the new column $L_x$ must be a NA, so I have to assign 0 to it (otherwise all calculations based on it will become NAs). The code \"ex\"=rev(cumsum(rev(Lx)))/lx (This is where I was stuck) The numerator of ex is calculated by summing over $L_x$ to $L_{max}$, the maximum age of $L_x$. This is not so intuitive when working with R. rev(Lx) reverse the order of $L_x$, and cumsum() is for cummulative sum. cumsum(rev(Lx)) then is equivalent to summing $L_x$ backwards (i.e. from $L_{max}$ to $L_x$). But since $L_x$ is reversed in the first place, cumsum(rev(Lx)) is also in reverse order. Reversing cumsum(rev(Lx)) with rev() then gives what I want. Calculating $r$ by while loop By the Eular-Lotka equation, I can calculate $r$. $$\\displaystyle \\sum_{all \\hspace{0.3mm} x} e^{-rx}l_x m_x = 1$$ Using while loop and Approximate $r$ calculated earlier as the starting value for r, $r$ is calculated as below. 1df \u003c- as.data.frame(life_table) 2r \u003c- 0.0812198 3x \u003c- sum(exp(-r*df$Age)*df$`lx*mx`) 4while (abs(x-1) \u003e= 0.000001) { 5 if (x-1\u003e0){ 6 r \u003c- r+0.00000001 7 } 8 else{ 9 r \u003c- r-0.00000001 10 } 11 x \u003c- sum(exp(-r*df$Age)*df$`lx*mx`) 12 r 13} The rest is simple, and the logic applied is the same. 1life_table \u003c- life_table %\u003e% 2 mutate( 3 \"approx.r\"=log(R0)/G, 4 \"r\"=r, 5 \"Vx\"=rev(cumsum(rev(exp(-r*Age)*lx*mx)))/exp(-r*Age)*lx, 6 \"lx*mx*e^-rx\"= lx*mx*exp(-r*Age), 7 \"approx.Vx\"=rev(cumsum(rev(exp(-approx.r*Age)*lx*mx)))/exp(-approx.r*Age)*lx, 8 \"approx.lx*mx*e^-rx\"= lx*mx*exp(-approx.r*Age) 9 ) Now, I’m done constructing a life table. Printing pretty Life Table 1kable(life_table, format=\"markdown\", align=\"c\") $Age$ $l_x$ $m_x$ $l_x m_x$ $x l_x m_x$ $L_x$ $e_x$ $R_0$ $G$ $Approximate$ $r$ $r$ $V_x$ $l_x m_x e^{-rx}$ $Approximate$ $V_x$ $Approximate$ $l_x m_x e^{-rx}$ 0 1.0000000 0 0.00000 0.0000 0.5000310 0.500153 1.2829 3.06727 0.0812198 0.0847117 1.0000010 0.0000000 1.0099103 0.0000000 1 0.0000620 4600 0.28520 0.2852 0.0000480 1.967742 1.2829 3.06727 0.0812198 0.0847117 0.0000675 0.2620352 0.0000679 0.2629518 2 0.0000340 8700 0.29580 0.5916 0.0000270 2.176471 1.2829 3.06727 0.0812198 0.0847117 0.0000297 0.2497000 0.0000299 0.2514499 3 0.0000200 11600 0.23200 0.6960 0.0000178 2.350000 1.2829 3.06727 0.0812198 0.0847117 0.0000126 0.1799362 0.0000126 0.1818310 4 0.0000155 12700 0.19685 0.7874 0.0000132 1.887097 1.2829 3.06727 0.0812198 0.0847117 0.0000067 0.1402737 0.0000067 0.1422467 5 0.0000110 12700 0.13970 0.6985 0.0000087 1.454546 1.2829 3.06727 0.0812198 0.0847117 0.0000028 0.0914634 0.0000028 0.0930743 6 0.0000065 12700 0.08255 0.4953 0.0000042 1.115385 1.2829 3.06727 0.0812198 0.0847117 0.0000008 0.0496567 0.0000008 0.0507081 7 0.0000020 12700 0.02540 0.1778 0.0000020 1.500000 1.2829 3.06727 0.0812198 0.0847117 0.0000001 0.0140380 0.0000001 0.0143853 8 0.0000020 12700 0.02540 0.2032 0.0000010 0.500000 1.2829 3.06727 0.0812198 0.0847117 0.0000001 0.0128978 0.0000001 0.0132632 9 0.0000000 0 0.00000 0.0000 0.0000000 NaN 1.2829 3.06727 0.0812198 0.0847117 0.0000000 0.0000000 0.0000000 0.0000000 Note that I edited the column names of the table in a text editor to make it display in LaTeX style. There is no simple knitr function (at least I don’t know) that print out pretty displayed style text in a table generated from a data frame ","subtitle":"","title":"Constructing Life Tables with R","uri":"/2017/12/11/life_tables/"},{"content":"There are many reasons to start a blog. I was inspired by this post and my interest in psychology.\nStarting a Blog for Others I have been reluctant to share my ideas with others (partly because I’m shy), but my interest in human evolution led me to realize the importance of learning and cultural transmission in our species1: A hard-working but not-so-smart student can do much better than a genius who never learn.\nI’m a not-so-smart student and have spent a large amount of time figuring out many things that turned out to be really simple while learning. I could have learned much more efficiently if someone had shared their experience with me (So that I don’t have to start by, say, proving $1+1=2$). Now, I think I have some experiences worth sharing, and people might benefit from them.\nStarting a Blog for Myself Probably, few or no people will read any post on my blog. But as noted in the post mentioned above, writing posts is a great chance to practice skills and communicating about them. By sharing what I’ve learned through writing a post, I’m actually reviewing it. And by communicating about what I’ve learned, I can know how well I understand the topic (thinking that you understand is different from understanding itself).\nI think we cannot fully understand human evolution without considering the effect of culture. ↩︎\n","subtitle":"","title":"Why start a Blog?","uri":"/2017/11/26/why-start-a-blog/"},{"content":"我從小腦袋就不太靈光，在周圍都是精英的環境下長大，更容易發現這點。尤其在數學上，別人那種迅捷的思維使我意識到那不光是努力可以達到的境界，因此一直以來，我認為數學不好與資質不佳是自己最大的限制。\n有印象以來，我就很怕與數學有關的科目。從小到大面對數學時所遇到的挫折，造成我一想到數學，就會有負面感受。久而久之，便認為自己就是笨就是數學不好，認為這是自己最大的限制。這想法逐漸成為我的一部份，但我從未發覺，直至最近才意識到它的影響竟如此巨大。 原來我一直以來所做的許多判斷與選擇，都強烈被這想法左右 (判斷:原因):\n讀心理系還不錯: 用到的數學不多，頂多牙根咬緊紮實讀完統計學就好(但我對研究人類的興趣是無庸置疑的) 修微乙就好: 微積分只要會操作就好，不用學太深 修普化好了(與普物二擇一): 普物應該都是數學，我不會喜歡。 我自認最合適的判斷，原來只是建立在一個長久以來的直覺感受。原來我最大的限制並非不聰明也非數學不好，而是深信這些都是真的，進而 fit in 自己與其他人對數學不好的期待。用術語來講，就是自己的行為不知不覺中符合了這種數學不好的 schema。\nSchema很可怕，因為它來去無蹤，在給自己冷靜判斷喜歡、適合什麼的機會之前，它就會迅速代替我們做好判斷(這領域好多數學→我數學不好→我不適合也不會喜歡這領域)，而我們甚至還以為這是最好的判斷。\n於是就一輩子與真心喜歡(但尚未發現)的東西擦肩而過。\n大一結束時，對心統這科目的不滿救了我。我很生氣花了一學年的時間讀了統計學，卻搞不清楚統計學是什麼，所以大二時，花了一年的時間去修數理統計。過程中有一點點辛苦，但比想像中的容易，更重要的是，這一年來學習的內容，使我具備能力與自信(習慣抽象的數學操作)去探索喜歡的東西。\n當發現自己的興趣無法被歸類於任何傳統心理學領域時，我不再受限於以往的數學障礙，而敢跨出心理學領域去接觸其它學界的想法(當然並非所有領域都用到數學，只是我有興趣的問題，數學很可能可以提供一些想法)。雖然我很幸運地跨出以往對自己的限制，但我仍因過去對數學的排斥，每天為這後果付出代價:\n當初上大學選擇修簡單的微乙。現在我很想修一些課，但數學基礎太差無法修。 高三放棄物理，導致自學微分方程時，常卡在物理的例子 高一開始學C++前，就已認為自己學不會。現在想自學 1 種 general-purpose language 都擠不出時間。 我覺得一定有很多人和我一樣，因為認為自己很笨或數學很差而對某些事物望之卻步，因而錯失了接觸自己喜歡的東西的機會。\n數學不應該成為一塊絆腳石，但大家的想法似乎完全相反:\n我們總認為一般人不需要了解數學比較抽象的部分，這是天才的專利\n但其實不正是因為我們不夠聰明，才要依靠數學將事情抽象化，進而看到事物間許多隱藏起來的關聯，而這些關聯才是我們最在意的東西。\n只有當我們認同了數學是天才的專利，而放棄掌握數學的機會， 數學才會成為我們追求所愛的絆腳石\n","subtitle":"","title":"數學與我","uri":"/2017/11/26/mathematics/"},{"content":" Language This is where I share my notes and personal experiences on learning. Some of the posts were written in Chinese, and some in English. I write posts in English when I want to reach a wider audience.\nLicense All the posts on my blog are licensed under CC-BY-NC-4.0. Other parts of this website, such as HTML/CSS code for constructing the web page, are licensed under the MIT license.\nCredits \u0026 Acknowledgment Theme Template The template of this blog, TeXtLite, is inspired by jekyll TeXt theme, which is the template I used before. To get more control of my website and to make it more lightweight, I ported TeXt to Hugo, resulting in TeXtLite. TeXtLite is meant to be lightweight and minimalist, in which most of the fancy features of the original TeXt theme are pruned off.\nLanding Page Instead of adopting TeXt theme’s landing page, TeXtLite’s landing page was inspired by the Academic theme.\n","subtitle":"","title":"","uri":"/about/"},{"content":"\n","subtitle":"","title":"Search","uri":"/search/"}]